#+OPTIONS: ^:{}

* [2015-09-30 mer.]
:PROPERTIES:
:header-args: :results none
:END:
Trying to come up with a fast equivalent to ~which-func-mode~ under Spacemacs.
The most naive implementation would be to lookup backward for the first heading.

#+BEGIN_SRC elisp
(spacemacs|define-mode-line-segment which-org-headline-segment
  (fmdkdd/org-current-headline)
  :when (eq major-mode 'org-mode))

(add-to-list 'spacemacs|define-mode-line-segment 'which-org-headline-segment t)

(defun fmdkdd/org-current-headline ()
  (save-excursion
    (re-search-backward org-complex-heading-regexp nil t)
    (match-string-no-properties 4)))
#+END_SRC

That does not give you the full current hierarchy (bread crumbs).
Actually, there is an ~org-get-heading~.

#+BEGIN_SRC elisp
(spacemacs|define-mode-line-segment which-org-headline-segment
  (org-get-heading)
  :when (eq major-mode 'org-mode))
#+END_SRC

It even gives us the font-lock properties.

There is also a ~org-get-outline-path~ that gives the rest of the crumbs.

#+BEGIN_SRC elisp
(spacemacs|define-mode-line-segment which-org-headline-segment
  (fmdkdd/org-current-headline)
  :when (eq major-mode 'org-mode))

(defun fmdkdd/org-current-headline ()
  (let ((path (append (org-get-outline-path)
                      (cons (org-get-heading t t) nil))))
    (org-format-outline-path path 40)))
#+END_SRC

Removing the text properties can be achieved by calling
~substring-no-properties~.  Though I rather like the effect as is.

Another, longer (but more proper?) way of removing them is the following:

#+BEGIN_SRC elisp
(defun fmdkdd/org-current-headline ()
  (let* ((path (append (org-get-outline-path)
                      (cons (org-get-heading t t) nil)))
        (formatted (org-format-outline-path path 40)))
    (set-text-properties 0 (length formatted) nil formatted)
    formatted))
#+END_SRC

* [2015-10-01 jeu.]
I would like to be able to use Emacs for collaborative editing.  I have light
requirements:

- over local network would suffice.  I just want to be able to share a buffer
  with someone next to me, each with their own computer.  For pair programming
  or teaching.
- I don’t care much about security: I trust the other person since she is right
  next to me.  When we finish, the computer should not be left in a vulnerable
  state however.
- I prefer to stay with my own Spacemacs config, rather than having to use the
  config of the peer.
- it should be painless to setup, and stable.

This [[http://stackoverflow.com/questions/12546722/using-emacs-server-and-emacsclient-on-other-machines-as-other-users][SO thread]] is a good start.

The scenario is as follows.  Host is where the files to be modified reside.
Host has an Emacs session and buffer on file A.  Client wants to drop in Host
and take control of Emacs from his machine, and edit the same buffer.  He can
also split windows, switch buffers, etc.

** Using tramp and ssh
Client needs ssh access to Host.  Client can browse to file A from his Emacs.

However, Host will not see the changes until Client saves.  This is
insufficient.

** Using ~make-frame-on-display~
Emacs can spawn a frame on another X display.

The requirements:

- Allow X to listen to TCP connections.

  Under Ubuntu, X is spawned by lightdm, so, in =/etc/lightdm/lightdm.conf=
  : xserver-allow-tcp=true

  and restart lightdm.

- After that, allow the Host to access the X server with xhost
  : xhost +host

  In Ubuntu, my LAN machines can be accessed via =host.local=.

  One can also use ~xauth~ here, as described in the SO thread.  Deauthorize the
  Host with ~xhost -host~.

- Finally, the Host can spawn a frame from its Emacs on the client display
  server with
  : make-frame-on-display client.local:0

Now, Client can write in the buffer, and Host can see the changes.  Both can
even edit at the same time.

Seems stable.  There is the issue that if any of the peer starts a modal action
(helm lookup, M-x minibuffer spawn), the other cannot type anymore.  When the
modal action is over, the input will be sent to the frame however.

This is a distinct frame, so Client cannot control the Emacs frame on Host.
Splitting windows should be done on both machines.  Client can browse the Host
files.

Host only has one command to spawn.  But Client needs to restart X before
pairing, because tcplisten seems like a fun backdoor to leave open.

Alas, Client is stuck with the Emacs configuration from Host.  This cannot be
avoided, since there is only one Emacs process.

** Using tmux
As suggested [[http://www.emacswiki.org/emacs/tmux_for_collaborative_editing][there]].  However, I could not make the socket sharing work.

Rather, sharing the same tmux session is simple:

- Host does ~tmux new -s pp~ to create a new session named ‘pp’.
- Client does (connected on Host) ~tmux attach -t pp~ to join the session.

With tmux, Client can connect to Host using ssh, and join a tmux session.  Both
share the same cursor.

Since tmux is terminal-based, Emacs runs in tty mode.  Functionality is the
same, but can be unfamiliar for Host.  Using frames would be possible through
ssh X forwarding, but that would not give us more than the previous solution.

Client has to use Host Emacs config, again.  The setup is also slightly more
involved with Host.

But, sharing through tmux is useful beyond Emacs.  So there is that.  And this
solution should work well over the network (if you can ssh to Host).

There is even a wrapper around tmux called [[https://github.com/zolrath/wemux/][wemux]] which simplifies the setup and
provides relevant options for multiple peers.

** Using floobits
A proprietary web service.  Use a Github account, create a workspace (?) and
share files.  Other users you have authorized can then access the workspace, and
you can see the changes in realtime in your editor if you are viewing the same
file.

Rather nice is that every peer is using his own machine and editor.

However, it goes through the Floobits server, thus it’s pretty slow compared to
the previous solutions.

And there is the requirement of going through a workspace.  It might make sense
for collaborative realtime editing of a project, though I’d rather use Git then.
But it’s cumbersome to setup when playing on a throwaway file.

The nail in the coffin is of course having to go through a third-party.  If the
server software was at least available as open source, I could run a local
instance and that would be a pretty good solution.  Alas, that does not appear
to be the case.

** Using rudel
[[http://rudel.sourceforge.net/][Rudel]] is an Emacs package which share functionality with Floobits.

One Emacs must host a rudel session.  Others can join.  The host does not take
part in collaborative editing.  The host passes editing data from one peer to
another using an open protocol.  Other clients can join.

Users in a session can publish a buffer, and others can subscribe to it.  When
you subscribe to a buffer, Rudel opens a new window with the buffer text
inside.  You can then edit the text with your own cursor, and editor.  Changes
are highlighted with the color of each user (that can be disabled through the
menu option, thankfully).

Rudel is intended to work with menu-bar-mode on, it seems.

I don’t know what data Rudel sends, but from the project website, it seems it
can break the functionality of some modes like EShell.  This behavior can be
troubling.

I’m not sure what exactly is the buffer a client edits: does it have a local
copy?  Does it exist only temporarily?

Speed is alright, but slower than tmux and xhost.

Also, the setup is a bit more involved, and the package is in dire need of
maintenance.
* [2015-10-21 mer.]
** Explanations
In web apps, I find it would be useful to be able to ask why a value is 0, or
NaN.  E.g., why a DOM element has its ‘left’ property to ‘12px’.  I would like
to find the culprit code immediately.  Alas, there are no ‘conditional
breakpoints’ in Firefox or Chrome.

Wait, there are!  You can break on attribute modification by right-clicking a
DOM in the Elements panel in Chrome.  In FF, you can conditionally /stop/ a
breakpoint, but not break conditionally.

Anyway, jumping into the debugger when a value is modified is only one part of
the workflow.  That gives you the place where the value is set, but not how the
right-hand side was computed.  You have to backtrack through the call stack to
get this information.

Instead, if a value contains its history, the explanation is always available.
See [[file:javascript/explanation.js][explanation.js]] for a minimal proof of concept.

** Interactive value inspector in s3c
Trying to add interactive value inspectors into s3c.

*** Rationale
Instead of plain text, the editor should put an HTML element that represents the
full object, like in Firefox or Chrome consoles.  Each property can be
inspected.

- Why do you need that?  The current behavior of displaying serialized objects
  is good enough for small programs.  At least you have all the properties on
  display at once.  With an “interactive” object, you have to click to view
  further properties...

- The current behavior is nice and simple, true.  But for larger objects, it
  is unwieldy.  Also, an interactive value inspector opens the door for
  interactive “explanations” of values: backtrack through the code that created
  some value in order to understand why it’s a NaN, or 0, or ...

- Do you really need explanations?  I mean, in a full application it could be
  nice (provided a good signal-to-noise ratio), but s3c is for simple JavaScript
  code for beginners.  To find out why a value is NaN, just add more //: to
  track the flow.

*** Implementation
CodeMirror provides two functions: ~addWidget~ and ~addLineWidget~.  ~addWidget~
puts an HTML element on a line with absolute positioning.  So I can create HTML
to inspect an object, and put it after the delimiter.  It does not matter if the
element is larger than the line: with a positive z-index, it will appear as if
floating over the text.

To do that, in ~write~, instead of replacing, I can call:

: editor.addWidget({line: l}, p, false, "above")

The last argument is undocumented, but it puts the element /on/ the given line
rather than below (the default).

However, the element is absolutely positioned.  It does not move when the line
does, which breaks the illusion that it gives a view of the value to the left of
the delimiter.

To sync the widget, I would need to listen on changes on the document, and move
all markers that are potentially affected.  It is not sufficient to listen to
the ~change~ event of a line, as when a line is moved as a side-effect of
inserting a new line above, no change event is fired.

The ~addLineWidget~ is quite different, as it inserts the element below the line
and appears to be inset /in/ the text.  The lines it takes are not numbered, and
are skipped by the cursor.  It behaves correctly when inserting new lines.  Bit
of a space hog currently, as it eats vertical space rather than making use of
the usually empty space at the right of the screen.

Hacking the DOM created by CodeMirror sounds like a bad idea, if only for
forward compatibility.

* [2015-12-02 mer.]
** Free monad for interpreters
Reading up on free monads.  Again.  And discussing them with Ronan.

Beyond [[http://programmers.stackexchange.com/questions/242795/what-is-the-free-monad-interpreter-pattern][this blog post]], [[http://programmers.stackexchange.com/questions/242795/what-is-the-free-monad-interpreter-pattern][this SO answer]] is particularly helpful.

On a related note, even setting up a Free monad can be seen as boilerplate.
[[http://okmij.org/ftp/Computation/free-monad.html][Okmij shows]] how to eliminate the noise.

* [2015-12-09 mer.]
** GameBoy Sound player
The sound component of Boyo is a mess.  It sort-of works, but there are weird
artifacts coming out after a while.  And it’s eating at least 20% CPU.  And it
doesn’t even pass blargg’s tests!

I want to start from a clean state, and understand how the damn thing works.
Maybe writing a player for GBS files would be a more appropriate target?  I’m
curious as to what these files store anyway.  Can’t be samples, or they would
directly be in a sound format.  So they must be instructions directly from the
ROM, but probably only the instructions relevant to the audio?

Found a [[http://ocremix.org/info/GBS_Format_Specification][spec]] for GBS files.  At that point, eww does not seem capable of
downloading a sample GBS from Zophar.

Got some GBS.  They are indeed smaller than the ROM file from which they are
extracted.  Pokemon Red ROM is 376K while the GBS is 48K for instance.

Looking at the source for gbsplay, it seems indeed that playing the files means
emulating the CPU and the audio unit.

Maybe what would be nice is if we could compile the output from a GBS into audio
instructions only.  To get an output similar to what MOD file looks like for
trackers.  GBS to MOD converter.

* [2015-12-11 ven.]
** GameBoy Sound player
Will try to go with rust-lang.  Why not make it harder on myself?  At least if I
don’t complete the project, I’ll have learned the basics of a new language.

Someone already did a library for [[https://github.com/emu-rs/spc][reading spc]] in Rust!  This will help.

* [2015-12-12 sam.]
** Learning Rust
The proof of concept code I wrote yesterday worked, but some pieces went over my
head.  Today I went over the [[https://doc.rust-lang.org/stable/book/][Rust book]] to RTFM.

Now, I know how I should use result types to avoid deconstructing with match so
much.  And also how to put my utility functions in a module for better
organization in the long term.

* [2015-12-19 sam.]
** Filling instruction is boooring
Revamped the instructions macros a bit.  Leaner, and now matching the order of
[[https://code.google.com/p/game-music-emu/source/browse/trunk/gme/Gb_Cpu.cpp?r=40&spec=svn40][Blargg’s emulator]].  Though I don’t really know if there is a performance payoff
for that, since it could be optimized by the compiler as a jump table anyway.

Not sure what I want to do with flags tests after operations.  Seems like lot of
duplicate code.  Unless I use a ~test_flags~ macro...

* [2015-12-20 dim.]
** Overflow are safe in Rust
Which means ... that 0xFF + 1 triggers a panic!  But only in debug builds, since
these checks are removed on release builds.  However, the right way to go about
that is to use ~wrapping_add~ instead to /explicitly/ signal overflow is
intended.

* [2015-12-23 mer.]
** Improving s3c
Was looking to improve the error feedback of s3c.  But I realized that I could
fix the O(n^2) complexity of code evaluation.

Since we have only one worker when evaluating the whole file, and since the
worker evaluates all its code in the global context, we don’t need to
re-evaluate the previous blocks.  We can just send each block of code to the
worker by resetting the current code string.

So, evaluation is back O(n) with a one-line change.  D’oh.  And this also fixes
the multiple console.log calls!

But, it also changes the behavior of error output.  Previously, the first error
encountered in the evaluation would propagate as the result to all the following
evaluation markers.

: throw 1 //: 1
: 1 + 1   //: 1

Now, even a syntax error will affect only the next marker.

: throw 1 //: 1
: 1 + 1   //: 2

Is this ... better?  I’m not sure.  On the one hand, errors don’t propagate
anymore.  So you can go on with your code and still get feedback, even if a
previous definition triggers an error.

On the other hand, it’s now easy to miss an error up in the file and continue
working, and then wonder why something doesn’t work down the road.  Syntax
errors are signaled by the linter.  But other errors, like:

: fn f(a) { return a.b(a) }
: f(12) //: TypeError: a.b is not a function

are not.

For beginners, it might be a good idea to make runtime error more noticeable.

Okay, marked the lines in inverted red.  Can’t miss them now.

-----

Also added visual feedback for triggering evaluation.  Just erase the text after
//: at the time of sending the code to the worker is enough to /see/ that the
editor is doing something even when the results are the same.

-----

Made console calls to not trigger any linting error or warning, since they can
be used to step through a block.

-----

Maybe using a forEach on each /block/ rather than line would be faster than the
current way.  Another time.

* [2016-01-06 mer.]
** Decoding opcodes in GBS
The decoding opcode part of GBS is a bit redundant:

#+BEGIN_SRC
0x41 => ld!(b, c),
0x42 => ld!(b, d),
0x43 => ld!(b, e),
0x44 => ld!(b, h),
0x45 => ld!(b, l),
0x47 => ld!(b, a),
#+END_SRC

There is a way to factor that by just looking at how the opcode is composed.
For the ‘ld’ instruction, there is a pattern:

: ld r,q = 01rrrqqq
: ld r,n = 00rrr110 nnnnnnnn

With r and q being one of:

| Register | Code |
|----------+------|
| B        |  000 |
| C        |  001 |
| D        |  010 |
| E        |  011 |
| H        |  100 |
| L        |  101 |
| HL       |  110 |
| A        |  111 |

So, we already have the register information from the opcodes.  No need to spell
it out.  But this means additional work at runtime (decoding the opcode), and
decreased legibility of source code.  As of now, the code is very
straightforward, save for the organization of the opcodes.

We could decode the opcode at compile time using a macro, but I’m not sure we
would gain in legibility.

And unfortunately, the pattern breaks down for other opcodes:

: ld A,BC = 0000 1010

At least the tedious way to spell it out is homogeneous.

* [2016-01-07 jeu.]
** Dragging boxes around
For a prototype visualizer of the JS heap.  I need to move boxes, representing
objects, around, and link them with arcs.

Started with a simple div box absolutely positioned and a homebrew drag’n’drop.
Works.

** Cloning SVG in a template tag
But for arcs, I need to switch to SVG.  First suprise: using HTML templates to
clone SVG elements needs namespacing.  So I wrap the elements (like ~rect~) in a
~svg~ tag with explicit namespacing.  Works!

** Slow drag in Firefox
Chrome is perfectly happy using the CSS transform property for dragging the
SVG boxes around.  Firefox is choppy.

[[https://jakearchibald.com/2013/solving-rendering-perf-puzzles/][This post]] is helpful on the subject.  Changing the x and y attribute of the rect
is definitely worse.  Using the transform property of the SVG (rather than CSS
transform) seems okay.  Certainly not as fast as Chrome, but looking at the
numerous bug report on SVG performance on Bugzilla, I’m gonna assume that SVG
animations in Firefox are just slower.

Hmm, closing the DevTools /is/ a definite improvement however.  Good thing to
keep in mind.

* [2016-01-08 ven.]
** Switching to d3
Managing SVG and interactivity is tedious.  D3 seems a good fit for what I want
to do.  I get browser compatibility, selectors, the join model of handling data,
and even animations.

Drag and drop is built-in, and I might need things like force layouts.

Also, it’s one of the most-used JS library, which means it probably won’t
disappear for at least a few years.

** Heisendrag
I was curious as to why the drag and drop example of D3 in Firefox was fluid,
while mine was choppy.  Turns out, dragging the browser tab in the other window
fixed the slowness ಠ_ಠ

* [2016-01-12 mar.]
** Mastering D3 and event propagation
In order to better understand how event propagation works in the DOM, and to
experiment with D3 animations, I made a simple visualization based on [[http://www.quirksmode.org/js/events_order.html][this
helpful page]], and using [[http://bl.ocks.org/mbostock/3943967][this block]] as a model for chaining transitions, and [[http://bl.ocks.org/mbostock/9631744][this
block]] for the visual language.

* [2016-01-15 ven.]
** Mouseenter event fired only when going to the right in FF
At least I thought that, maybe it was a bug in Firefox.  The behavior puzzled me
and then I noticed that the SVG rect I was hovering my mouse onto was /not/ the
only element around: the temporary line I drew on top of everything was there
too!

So, #notabug.  Standard PEBKAC.  The line should not be interact with the cursor
in this case, and that is what the CSS property ~pointer-events: none~ is for.

And hey!  As a bonus, it fixed the behavior I was seeing in Chrome: since the
cursor was just above the line, whenever I clicked on it to validate, I was
clicking on the line, which had only the SVG container as a parent, and thus the
SVG registered the click while the node did not.  In Firefox, for some reason,
the cursor always clicked the node below the line.  Maybe the calculations were
off a pixel...

** The self-perpetuating task of explaining code with code
I want to visualize JS code to better understand it, and be able to explain it.
For that, I build a program.  I write more code, /different/ code, code that the
visualization might not suffice to explain.  The visualization if for heap
objects, but for that I’m writing an automaton, and we already have a good
visual language for those.  But!  If I want this automaton visualization to be
interactive, I again need to write more code.

Either at some point I have visualizations for the first kind of code, and also
for the code of the visualizations, etc.—I converge—or I just throw up my arms
in the air and leave some code unexplained, or self-evident.

Will only know if I try.

** Declarative automaton for linking nodes interactively
The linking nodes code is /clearly/ an automaton, and /clearly/ is spaghetti
code at the moment.  Dealing with listeners that should only exist on one state
is especially nasty, since we have to register them, then toggle them off, and
this is a repeating pattern that surely could be taken care of by a declarative
automaton.

As it stands, here is the description of the functionality needed to make the
linking:

#+BEGIN_EXAMPLE
Complete (functional description of) automaton

ready --click on circle--> select-dst
       |
       +- create temp line from circle to mouse

select-dst --move mouse-> select-dst
            |
            +- set end point of temp line to mouse position

select-dst --click on a free node-> ready
            |
            +- remove temp line
            +- add link between src and dst to model
            +- add link to view (update view)

select-dst --click elsewhere-> ready
            |
            + remove temp line

Animations and highlights:

ready --enter circle-> ready
       |
       +- grow circle

ready --leave circle-> ready
       |
       +- reduce circle to original size

select-dst --enter node-> select-dst
            |
            + stroke node in green

select-dst --leave node-> select-dst
            |
            + stroke node in default color (black)
#+END_EXAMPLE

I’m pretty sure there is a fluent API there that can take care of the
administrative details of entering a state, and setting up/destroying events
listeners as it goes through a transition.  Anything that need to be done on a
transition can be passed as a function.

Transitions, for my case, are always events happening on some element.  Then 4
things happen, in order:
1. We execute whatever needs to be done when leaving the state (cleaning up
   event listeners)
2. We execute the transition function
3. We change the state, internally
4. We execute whatever needs to be done when entering the new state (setting up
   new listeners)

If the transition is a loop (to and from the state), then only step 2 is needed.

That’s it!  Initially I don’t think any more control is needed for my use case.

Here is how I would sketch the API:

#+BEGIN_SRC js
var link_automaton = automaton()

var ready = link_automaton.state('ready')
      .on('circle.mouseenter', grow)
      .on('circle.mouseleave', shrink)
      .to('select-dst', 'circle.click', create_tmp_link)

link_automaton.init('ready')
#+END_SRC

Need to prototype that to know if it works in practice, and make sure it is
composable (can add states and transitions in multiple steps, not just one
monolithic call).

* [2016-01-19 mar.]
** Declarative automaton API choices
Nearly done.  The code is much clearer using the automaton.  For now I’m just
declaring state objects and adding callbacks to their transition/enter/leave
events, and not using a fluent interface at all.

However, the fluent interface can come on top of that, to alleviate two problems
with the lower-level interface:
1. All the states must be declared beforehand.  If A refers to state B (in a
   transition, say), then B must be declared.

   Using a fluent interface, we can just give the name of the state rather than
   a reference to it, and let the interface build the actual state objects for
   us.

2. Adding a callback to a transition is done with ‘on’, but a callback to an
   enter/leave event of a state uses ‘addListener’.  The fluent interface can
   merge the two calls based on the arguments.

There remains a problem with the automaton that I would like solved before
moving forward: how to deal with state that is local to the automaton.  The link
automaton needs to keep a reference to the first element selected, in the ready
state, for use in the select-dst state.

I elected to add an empty ‘data’ object to the automaton.  It’s basically the
same as closing over a variable, but at least it’s namespaced.  And in the
future, maybe I can provide a way to get a ref to the automaton from callback
calls.



An issue I encountered in this version is that I can’t add multiple callbacks to
one transition.  Or even add a callback after creating the transition without
any, at first.

To solve that, transitions should be first class, either through giving them a
name, or returning a new object.

As an added nicety, I think I know how to settle the dilemma of having to choose
whether transition callbacks happen after or before we leave the current state:
let the user choose.  Callbacks can be added either at the ‘debut’ of the
transition (before leaving the old state), or at the ‘end’ (after entering the
new state).  Maybe the ‘middle’ (after leaving the old state, but before
entering the new one) can also be useful.

* [2016-01-22 ven.]
** More design decisions
I’ve pondered whether using the automaton as a pure event emitter.  When
entering a state, when a transition is made (3 stages), just emit custom events
and define the behavior only in the listeners to these custom events.

This is better for decoupling the code.  But the cost is that you lose track of
the control flow.  Some animation bugs are subtle, and require you to know
precisely what happens and in which order.  Animation is part of the
interaction, and the code should not be declared separately.

* [2016-01-29 ven.]
** Event binding troubles
So, I was on the fence about binding listeners to elements themselves, rather
than on the containing SVG, fearing performance issues.  Since boxes can be
added/removed, and we add several listeners to different element of each box,
AND we add/remove listeners depending on the current state of the automaton.

The upside is code that is free of ~if~ statements, since the dispatching is
taken care of by the event dispatcher.

However, it has come to bite me back.  If I define the automaton only once (as I
should have from the start), then when a new box is created, no listeners are
bound to it.  Can’t be dragged.

Of course I could add the drag behavior to newly created boxes.  But, it might
not be correct if we are not in the ready state.  What we should do is add the
listeners for boxes (and sub-components, cell and circle) valid /in the current
state/.  That seems like it’s easy to forget, and it is.  Also, it seems
a bit wasteful, because I would select all boxes again, and reassign the
listeners for all.

Another solution is to catch all events at the container level, let them bubble
up and identify the original target.  But now the problem is that sometimes I
don’t want to just know the original target, but I need the path in the DOM that
the event took.  So now I need to walk up the tree, duplicating the bubbling
phase.

And, ultimately, the drag behavior from d3 need to be called on a selection, not
on the container.

The more pragmatic solution is just to call drag_box when a new box is created.
Since I know the user is in the ready state.  Even though it’s not correct, I
might find a better way to organize this stuff later down the road if need be.

* [2016-03-22 mar.]
** Comparing approaches to deal with state
Ronan has been using RxJS for an application that presents a GUI in the
browser.  I was wondering how the reactive programming approach would handle my
situation, for which I found that a state automaton was the best approximation.

But at the same time, it seems odd that I have to resort to an explicit state
automaton to handle my elementary interaction.  So, how do others deal with it?

Looking at RxJS docs, it seems that it is a complete algebra of events, meaning
I could use the basic operators to build richer ones, and eventually create
streams of predicates that would give me exactly the same information that a
state automaton gives.

But, would the complex operator be as clear, or clearer than the description of
an automaton?  And what about the performance of the thing, as this is always a
worrying concern when techniques from functional programming are naively ported
to JavaScript.

I need to find out:
1) the way ‘traditional’ GUI systems deal with this kind of interaction (Swing,
   GTK, Qt, Cocoa?)
2) if there is a ‘canonical’ way to handle this kind of interaction using RxJS
   (or in reactive programming)
3) if there is a standard, or well-known technique to bind listeners to DOM
   elements ‘lazily’, that is, whenever an element matches the given selector, it
   should trigger the listener.

For point 3, if I set up a single listener at the root of the document, I can
capture any click and match the given selector against event.target.  But what
if I want to match against a /parent/ of the target?  Knowing that clicks
bubble, I could walk up the DOM and test the selector against each element,
until I hit the root.

Except now I’m duplicating logic done by the browser, and it’s incompatible with
stuff like ~event.stopPropagation()~.



Okay, on 3, there is an [[https://developer.mozilla.org/en-US/docs/Web/API/element/matches][~Element.matches~]] predicate to know if the element would
have matched the given CSS selector.  Better than having to check the ~tagName~
and ~classList~.  But doesn’t solve the need to look up the parent.

The name of the technique is “event delegation”.  [[https://api.jquery.com/on/][Jquery]] has an argument for
that, but for some reason, it doesn’t work on SVG.  And indeed, it walks the
tree:

#+BEGIN_QUOTE
jQuery bubbles the event from the event target up to the element where the
handler is attached (i.e., innermost to outermost element) and runs the handler
for any elements along that path matching the selector.
#+END_QUOTE

On point 1, there are certainly a number of hits for “GUI state machine”, and
the pattern seems recognized.

* [2016-03-28 Mon]
** Trying out a Sparkets rust server
Since server is in need of a rewrite, to be faster, cleaner and more robust.

Since we already compiled Coffeescript, that does not change the compilation
time much.

** Choosing a library
I’ve got a fast and simple [[https://github.com/housleyjk/ws-rs][websocket library]].

Now, I know I will want to benchmark binary messages vs. text messages.  So I
should design around this choice by presenting a common interface.

** Testing input latency
I want to test how the game feels with a moderately high latency (~50 to 100ms
roundtrip).  I thought Chromium was able to do that, but it seems the throttling
option of the network panel only works for initiating the connection, and is not
applied to all subsequent frames when the websocket is established.

But, there is an option to add latency directly on the loopback interface
through [[https://daniel.haxx.se/blog/2010/12/14/add-latency-to-localhost/][netem]]:

: tc qdisc add dev lo root handle 1:0 netem delay 50ms

this sets 50ms of delay.  It does affect ~ping~, and it visibly affects
websocket frames on my machine.

To reset:
: tc qdisc del dev lo root

It seems you need to reset before applying a different delay.

** Multi-threaded server or asynchronous?
Building up a small prototype.  Not familiar at all with how to build a game
server in Rust.  And I have to deal with memory management explicitly.

The nodejs server was asynchronous, because nodejs.  One event loop where input
was collected, and one setTimeout to deal with game updates.

In Rust, I guess I could also do that, but I have to look up how.  Meanwhile, I
could also use a multi-threaded approach.  One thread per client might be
simpler to code, and since we are not expecting thousands of players, the
performance scaling of thread is not an issue.

In any case, I need to brush up on coding concurrency in Rust.

Been reading:

- [[http://fabiensanglard.net/quakeSource/quakeSourceNetWork.php][Network code review of Quake]]

  Yes, I know it uses UDP, and WebSocket is on TCP.  But I want to know how
  clients are handled.

  Well, it’s not clear from that article.

** What’s the ideal solution to input latency anyway?
I’ve always wondered if the treat input/update logic/render loop was optimal.
I’ve been doing that for ages.  I remember it bit me because updates were tied
to graphical frames, and lagging on frames made the game slow.

But this was an issue of handling time in the updates.  If the game updates by
doing ~player.x++~ each frame /and/ you assume the game runs at 60fps, then when
an old machine churns out 30fps, the game plays in slow motion.  Because what
you really wanted to say is ‘x increases by one each 16.66ms’; the simulation is
tied to continuous time.

A game is a simulation.  The simulation, to feel good, needs to be as responsive
as possible.  If I act on the real world, I expect an immediate feedback.  The
simulation, to feel real, must do the same.  It means that a player must be able
to react on input, and see his impact on the simulation in /realtime/.  Of
course, the computer cannot do realtime, only discrete.  But, the computer can
compute the simulation and redraw it much faster than the brain can notice.

25fps is good enough for our brain to believe that movies are real.  But when
you add interaction, you usually need to be a bit faster than that.  25fps means
40ms between two frames.

Let’s say it takes 10 ms to update the simulation, and another 10ms to draw the
scene and refresh the display.  Out of 40ms, the CPU is only busy for 20ms,
which is good.

#+BEGIN_EXAMPLE
   late input                            early input
   |                                     |
--UUUUUUUUUURRRRRRRRRR--------------------UUUUUUUUUURRRRRRRRRR----------------
  |  compute s        |                   |  compute s+1      |
 screen shows s-1     |  screen shows s                       | screen shows s+1
#+END_EXAMPLE

Already something is troubling.  The simulation should render things as they are
/right now/.  But as it /takes time/ doing so, the display is already outdated
as it is shown on the screen!

It’s like when I give you the time, by the time you hear it and process it, it’s
already false.  Now, luckily, the time is still useful to you because I only go
to the minutes.  Seconds are trickier.  Milliseconds are already hopeless.

Same thing for the simulation.  It’s in some state ‘s’, then at the scheduled
time (every 40ms), it starts updating to state s+1.  When the screen is
refreshed, we are already 20ms in.  What time does the simulation reflect?

If it reflects the time of the world at the /beginning/ of the update, then the
image on the screen is already 20ms outdated when it comes up.

That means that if a user action is made just before the update comes along, we
will see the result 20ms at the earliest.  Worst case, the input is made just
after the update component reads them, then we have to wait for the current
frame to draw, then the next: 60ms before our action impacts the world we see.

So, for any random button press, the screen might display the changed world
after a delay that is anywhere between 20ms and 60ms.

If that delay is long enough for the brain to have time to think “did I press
that button?”, for the brain to /notice/, then the simulation is not fluid, and
the illusion breaks.

The question is then, how long can this delay be before the brain starts to
notice?



Running some tests...

Typing a key (down key event) paints a square on the screen.  The square
alternate between pink and green colors to distinguish each key stroke.

Delays are chosen randomly, I just type to see if it feels responsive.
Delays are just lower bound on the actual perceived delay: the screen might take
some milliseconds longer to refresh.

I’ve noticed that typing just one key is vastly different than stringing a few
keys together.  If I type once, and wait to see if I notice the delay before the
square is painted, 100ms feels immediate.  But string 3 keys rapidly, and it
does not feel instant anymore.

400ms is definitely noticeable, and feels sluggish at all.

A delay of 200ms is noticeable, but can still feel responsive for one key.  Not
for 3 keys.

100ms feels immediate.  But I can feel the delay when stringing keys.

50ms feels immediate.  Stringing keys also.

10ms feels a bit faster than 50ms, but not really much.



Another test, on input speed this time.  Measuring time between key downs.

Double-tapping the same key: I can hit 87ms minimum reliably, but with effort.
Effortless is more 150ms.

Stringing two different keys: now there is an issue with measurement.  Tapping
two or more keys /at the same time/, I can never get below 8ms.

Since each key down is a separate call to the listener, I suspect that the time
is spent dispatching and cleaning up.  So, 8ms is the effective resolution of
the browser in this setup.  Sometimes I get a 3, or even 0.5, but quite
randomly.

Now, stringing two different keys: I can do 8ms (same time for the browser) and
16ms reliably (the earliest to distinguish between two key down), without
effort.

With 3 fingers, I can do <100ms for each successive tap, effortlessly.



What does this mean?  Well, if I am able to hit two keys with 20ms between them,
I can also hit them with a 60ms interval.  If I can feel the difference in
my fingers, the game should also reflect this difference.

But, if I sample the input every 40ms (by polling the keys at the beginning of
the update loop), keys hit with an interval <40ms are counted as being hit at
the same time.

It’s basic sampling.  The signal is 1 when the key is down, and 0 when the key
is up.

#+BEGIN_EXAMPLE
----------|----------|----------|----------
0000000111110000000000011111000000011110000
#+END_EXAMPLE

As long as the key is held down for longer than the polling interval, we are
sure to get every key.

And if we want to distinguish between two successive key pressed, we just have
to use a reasonably low polling.

On my browser, the lightest tap I can muster holds the key for 32ms.  Meaning
that if the polling was 40ms, I could miss that key down from time to time,
depending on how it falls with respect to the update.

In this case, 30ms would suffice.  Poll interval of 30ms, or you start losing
keys.



So I guess the morale of the story is: faster feedback is always better.  But
below 50ms of visual feedback, the gains are negligible.

Polling keys at the start of a monolithic update loop is okay, as long as the
polling interval is less than the time a key can be held down.  Should check on
target hardware how low the resolution can be (browser + keyboard is certainly
not the optimal setup).

** Carmack on movement prediction
To alleviate server latency in QuakeWorld, Carmack tried to use prediction.  The
player movement is duplicated on the client, starting from the last known good
state received from the server.

The server works by directly answering to received packets: update only the
world around the player, and send the state back.  There is no global time
anymore.  But the player does not have to wait for the fixed update.

Carmack notes that simulating 300ms of player movement on the client is
hopeless.  But, for <100ms delays, client prediction helps smooth out the
movements.  Because server updates may not always arrive on time, we can keep
the framerate constant on the client with prediction.

* [2016-04-02 Sat]
** Setting the MTU on Archlinux
I had issues connecting to wiki.archlinux.org, but other websites were fine.

Apparently, that was caused by a misconfigured MTU.  Under Windows, the MTU was
1480 for ipv6, and 1500 for ipv4, but in Linux it was 1500.

To find out the correct MTU, I used ping:

: $ ping -4 -l 1452 -M do www.dslreports.com

‘-M do’ tells ping to look for MTU discovery packets.  The host has to be
configured to send these packets back, which few of those I tested (8.8.8.8,
google.com, free.fr) did.

Setting the MTU temporarily:

: # ip link set eth0 mtu 1480

(replace ‘eth0’ by interface name)

Then wiki.archlinux.org loaded correctly.

To set the MTU permanently, the wiki advised to use an udev rule, but I could
not get it to match the interface name for some reason.  Too lazy to RTFM, turns
out there is an MTUBytes option for systemd-networkd.service.  In
/etc/systemd/network/my.network:

: [Link]
: MTUBytes=1480

Voilà.

** Mounting a WDTV Live Hub
Did not want to install/configure Samba.

But luckily, only ~cifs-utils~ is required:

: # mount -t cifs //SERVER_IP/WDTVLiveHub/ /mnt/wdtv -o uid=USER,gid=USER

To find what shares are up on the network:

: $ smbclient -L //SERVER_NAME

To find the IP of the server:

: $ nmblookup SERVER_NAME

* [2016-04-03 Sun]
** Making progress
Spent a few hours trying to find a way to emulate a setInterval on the server.
Well, good old thread::sleep is still the state of the art, apparently.  It was
used by a [[https://github.com/mvdnes/rboy/blob/master/src/main.rs][gameboy emulator]], and measurements show it as accurate enough.

I ought to make a [[http://gafferongames.com/game-physics/fix-your-timestep/][“right” timestep]] this time around though.

And I’m sure I’ll run into all kind of ownership fun when I start accessing the
game state from the logic thread as well as from the websocket handler.

One thing I haven’t settled, is whether to send game updates to clients when we
receive a message, or broadcast in the logic thread.  I recall reading Carmack
switching to the former for Quake3.  Cuts time between updates for the client,
but every client will have a slightly different state (although the interval are
so small, it should not be noticeable).

Serialization was another issue.  I found a library, [[http://tyoverby.com/bincode/bincode/][bincode]], so I don’t have to
write a struct to [u8] function.  But on the JS side, I still need to write a
deserializer.  So I might end up writing the serializer by hand, to have more
control over endianness.  And for diffing snapshots to send updates.

And while I’m experimenting, maybe find a way to use the unreliable WebRTC data
channel, rather than websocket.  Should be quite faster especially out of the
LAN.

- http://www.html5rocks.com/en/tutorials/webrtc/datachannels/
- https://hacks.mozilla.org/2013/03/webrtc-data-channels-for-great-multiplayer/

But on the Rust side, it’s rather bleak:

- https://github.com/phsym/sctp-sys

** SCTP experiments
Tried to use the rust-sctp library.  For some reason it always returns an error
when I try to accept a connection.

Tried to bind the socket in C.  It gets past the accept and blocks.

So, I guess if I can chat with a JS web page over RTCDataChannel, it might be
worth to try to see how to integrate the C code into Rust?

* [2016-04-09 Sat]
** Understanding WebRTC
Found a pretty [[http://chimera.labs.oreilly.com/books/1230000000545/index.html][comprehensive book]] on WebRTC and browser networking.

Managed to build a minimal example of a client page using WebRTC to setup an
unreliable data channel to itself.

Now, the sad part of that is that setting up a WebRTC connection is /much more/
than just creating a socket.  You need an SDP, to setup ICE candidates, and then
let the browser establish the SCTP connection over DTLS over UDP under the hood.

I only found an SCTP library for Rust for now, so I’m missing a few components
to make a Rust binary talk to a WebRTC JS client.

Nodejs can talk to a WebRTC browser, right?  The goto library on npm seems to be
[[https://github.com/feross/simple-peer][simple-peer]].  To use it in node, they point to [[https://www.npmjs.com/package/wrtc][wrtc]].  Seems /they/ mostly wrap
around the WRTC implementation of Chromium, and export that to node bindings.

So using that with Rust seems... not fun, at all.

On the other hand, I /could/ use simple-peer and wrtc in Sparkets directly, and
have an UDP protocol for messages.  Less work, more benefits.

* [2016-04-19 Tue]
** One fat listener
I like simple approaches.  Watching [[http://mollyrocket.com/861][Immediate-mode GUIs]], I want to try writing
a catch-all listener that will handle all the logic in one place.

I suspect that he had in mind to repaint the components in the single update
function.  I don’t need to do that here, as I deal with SVG elements inserted
into the DOM.

The single update function works.  But it made me realize I really ought to
decompose the ‘box’ functionality into independent behaviors, or traits:
- a movable behavior that adds a moveTo command for manual positioning
- a draggable behavior for mouse dragging
- the box is just a container, doesn’t need to know what’s inside to draw itself
- a snappable behavior for snapping to a grid

Properties are distinct components also.  And links too.

* [2016-04-29 ven.]
** Diving into V8 optimizations
Trying to find out if, in a simple ~for~ loop:

#+BEGIN_SRC js
var a = []
for (var i=0; i < a.length; ++i) {}
#+END_SRC

the ~i < a.length~ check is optimized as:

#+BEGIN_SRC js
var a = []
for (var i=0, l=a.length; i < l; ++i) {}
#+END_SRC

or not.

Via nodejs, we can pass a bunch of flags to V8 in order to obtain more
information about the optimization, GC calls, intermediate representations, and
generated code.

After putting the loop in a function that's called 10000 times, the function is
/hot/ and will be compiled and optimized.  We can see that with the --trace-opt
option.

#+BEGIN_EXAMPLE
[compiling method 0xad4d20bd9c1 <JS Function f (SharedFunctionInfo 0x187ace9573f1)> using Crankshaft OSR]
[optimizing 0xad4d20bd9c1 <JS Function f (SharedFunctionInfo 0x187ace9573f1)> - took 0.061, 0.151, 0.038 ms]
#+END_EXAMPLE

To find out the generated code, we can use --print-opt-code:

#+BEGIN_EXAMPLE
--- Optimized code ---
optimization_id = 1
source_position = 72
kind = OPTIMIZED_FUNCTION
name = f
stack_slots = 10
compiler = crankshaft
Instructions (size = 696)
0x2335adc58220     0  55             push rbp
0x2335adc58221     1  4889e5         REX.W movq rbp,rsp
0x2335adc58224     4  56             push rsi
0x2335adc58225     5  57             push rdi
0x2335adc58226     6  4883ec30       REX.W subq rsp,0x30
0x2335adc5822a    10  488b45f8       REX.W movq rax,[rbp-0x8]
0x2335adc5822e    14  488945d8       REX.W movq [rbp-0x28],rax
0x2335adc58232    18  488bf0         REX.W movq rsi,rax
...
#+END_EXAMPLE

Now, unfortunately, that's a bit low level.

I tried to generate the same code for the hand-optimized for loop, and diff the
outputs.  But there many random addresses that gets in the way of seeing if
instructions differ.  One thing that's easy to spot though is the Instructions
(size) line.

My guess is it's the size of the compiled function.  But the hand-optimized
version has size=812, which seems counter-intuitive.

Or maybe, the hand-optimized version actually /defeats/ optimization made on the
more common idiom by the compiler.

We can get a look at some of the optimization phases made on the high-level
representation (HIR) through the --trace-hydrogen flag.  My guess is Hydrogen is
responsible for high-level representation.

The file contains multiple control flow graph, with the helpful name of the pass
that generates it.

When ~f~ is optimized, it triggers a full compilation phase.  The graph is full
of "blocks" of code:

#+BEGIN_EXAMPLE
                               +----------+
                               v          |
B0 -> B1 -> B2 -> B4 -> B5 -> B6 -> B7 -> B8
             |          ^        -> B9 -> B10 (return)
             +--> B3 ---+
#+END_EXAMPLE

Clearly, the loop is B6 -> B7 -> B8, and B9 is the exit path.

If we look at B6, we can see our length check:

#+BEGIN_EXAMPLE
      0 0 v48 BlockEntry  type:Tagged <|@
      0 0 t52 CheckHeapObject t39 <|@
      0 1 t53 CheckMaps t39 [0x2a5dde306c51] <|@
      0 1 i54 LoadNamedField t39.%length@24 t53 type:Smi <|@
      0 0 i55 CompareNumericAndBranch LT i44 i54 goto (B7, B9) type:Tagged <|@
#+END_EXAMPLE

So, at this point, we are checking the ~length~ field of the array.

But, after the "H_Global value numbering" phase, all that's left of this block
is just the comparison:

#+BEGIN_EXAMPLE
      0 0 v48 BlockEntry  type:Tagged <|@
      0 0 i55 CompareNumericAndBranch LT i44 i54 goto (B7, B9) type:Tagged <|@
#+END_EXAMPLE

i54, the integer that holds the length value, has moved to block B5, which is
not part of the loop:

#+BEGIN_EXAMPLE
      0 0 v45 BlockEntry  type:Tagged <|@
      0 0 v46 Simulate id=30 type:Tagged <|@
      0 0 t52 CheckHeapObject t39 <|@
      0 3 t53 CheckMaps t39 [0x2a5dde306c51] <|@
      0 2 i54 LoadNamedField t39.%length@24 t53 type:Smi <|@
      0 2 t70 Constant 0x2ffe54fafc79 <JS Array[0]> [map 0x2a5dde306b49]  <|@
      0 0 t71 CheckMaps t70 [0x2a5dde306b49](stability-check) <|@
      0 2 t72 Constant 0x2ffe54facc81 <an Object with map 0x2a5dde306519> [map 0x2a5dde306519]  <|@
      0 0 t73 CheckMaps t72 [0x2a5dde306519](stability-check) <|@
      0 4 t74 LoadNamedField t53.%elements@16 type:Tagged <|@
      0 0 t75 CheckMaps t74 [0x2a5dde304209] <|@
      0 0 v47 Goto B6 type:Tagged <|@
#+END_EXAMPLE

So, it seems that the length check is indeed optimized by V8.  And that is done
in the "Global value numbering" phase on the HIR.

* [2016-05-02 lun.]
** Someone who actually knows V8 optimizations
already [[http://mrale.ph/blog/2014/12/24/array-length-caching.html][covered]] the ~array.length~ case in depth.

He also built a [[http://mrale.ph/irhydra/2/][tool]] to visualize V8 HIR, contron flow graph, and
deoptimizations output.  Much better than recreating the graph by hand.

He mentions that manually caching the ~array.length~ may actually be worse,
because it creates an additional variable that is assigned to a register.

The morale here is, again, to measure before optimizing.

The compiler does a good job a optimizing common idioms.  And it actually
produces less-efficient code if you are trying to optimize things yourself.

This was [[http://www.infoq.com/presentations/chrome-v8-optimization][reiterated]] by V8 engineer Ben Titzer for heap optimizations.  Someone
asked if using an object pool is a good idea when you have allocations
problems.  The answer: probably not, because V8 /assumes/ a usage pattern of
creating objects and throwing them away.  An object pool is an uncommon pattern,
and it might defeat optimizations.

Measure first, understand how the runtime works, formulate a strategy, implement
and measure again.

* [2016-05-06 ven.]
** Testing the GB CPU emulator
The [[http://blargg.8bitalley.com/parodius/gb-tests/][Blargg test suite]] is a good start.  But there is a slight bootstrapping
issue, as it needs a mostly-working CPU to actually start running the tests.

And the output requires minimal screen emulation, which I don't really wanted to
cover.

And the GB rom files are not the same format as GBS files... again, I don't want
to parse those.  On that front, since the assembly source is provided, I can
actually recompile the tests for GBS.

In shell.inc, you find:

#+BEGIN_EXAMPLE
; GBS music file
.ifdef BUILD_GBS
     .include "build_gbs.s"
.endif
#+END_EXAMPLE

The readme mentions that 'wla-dx' was used to compile and link those assembly
files.  The project is [[https://github.com/vhelin/wla-dx][still alive]], and also in [[https://aur.archlinux.org/packages/wla_dx/][AUR]] (gotta love Arch).

To compile a GBS file from an individual test file, you just need to define
~BUILD_GBS~ like so:

: wla-gb -o -DBUILD_GBS FILE test.o
: wlalink linkfile test.gbs

Two issues for the moment with that ROM.  The play address of the header is
0xC6D5, which is outside the 0x400--0x7FFF range of the GBS spec...  and if I
remove the checks there is an infinite loop (maybe because I haven't implemented
all flags for instructions yet).

Maybe a basic test harness in Rust is a better idea.

** Testing single instructions
Created a ~step~ function that goes through one instruction and returns the
number of cycles.  More useful for unit testing than ~run~.

Using macros for testing, since I have lot of repetitive code for each register.
But now, running into a strange SIGSEGV error when I have too many macro
calls... strange.

#+BEGIN_EXAMPLE
error: Process didn't exit successfully: `gbs-4725f7ba8db983e2`
(signal: 11, SIGSEGV: invalid memory reference)
#+END_EXAMPLE

Trying to debug by finding out what is generated after macro expansion.  Need an
(undocumented, of course!) option:

: rustc --test --pretty=expanded -Z unstable-options src/cpu.rs

~--test~ means compile the test module, I suppose.  And ~--pretty~ is the option
to output pretty printed code after macro expansion.

Ok, I have code like this:

#+BEGIN_SRC rust
#[test]
fn test() {
  ld!(b, c);
  ld!(b, d);
  ...
}
#+END_SRC

and the macro creates a new ~Cpu~ each time:

#+BEGIN_SRC rust
macro_rules ld! {
  ($r:ident, $r2:ident) => ({
    let mut cpu = Cpu::new();
    ...
    assert!(..)
  });
}
#+END_SRC

In the generated code, ~test~ contains as many blocks as there are ~ld!~ macro
calls.  I suppose that the code generator doesn't like code that has too many
blocks... Maybe I should split those into functions?

Ok, changed the tests to generate one function for each test case.  Only
slightly more verbose, but greatly increases my number of tests!

** Wait, was that a compiler bug?
The SIGSEGV with too many macros... no unsafe code, but still an invalid memory
reference?  How come?

Building a minimal example now.

: rustc --test main.rs; and ./main
: fish: “and ./main” terminated by signal SIGSEGV (Address boundary error)

Ok, just a single test function that calls 32 ~Cpu::new~ does it, but 31 calls
does not SIGSEGV.  I emptied the ~Cpu~ struct to contain only the ~ram~ field,
which has 65536 u8, hence each Cpu eats 64K.

Let's see, 32*64K = 2048K = 2M.

That's a suspiciously round number.  <2M, no SIGSEGV, >=2M, SIGSEGV.

According to [[https://play.rust-lang.org/][play]], happens on stable, beta and nightly.  But only in debug mode
(release optimizes everything away probably).

Aaaand there we have it: [[https://github.com/rust-lang/rust/issues/31748][#31748]].  Rust has a default stack size of 2M, so we
overflow that.  But there should be a stack overflow message that's skipped for
some reasons, and the devs are aware of it.

* [2016-05-07 sam.]
** Fixing flycheck-rust
flycheck-rust is confused when you have both a lib.rs and a main.rs in the same
folder.  Because cargo needs to know what target to build: the lib, or the
binary?

flycheck-rust does not specify the target, and spouts an error, and fails to
check the buffer (and any buffer in the project).  This has been [[https://github.com/flycheck/flycheck-rust/issues/23][reported]], but
not yet fixed.

Now, we can get the all targets from cargo itself, thanks to the ~read-manifest~
command:

: cargo read-manifest

returns a JSON with all targets.

Now, which one to chose?  I suppose the 'lib' target will start with the
'lib.rs' file, and compile all files that are included in it, recursively.  And
the 'main' target is the same, but starting from the 'main.rs' file.

Flycheck works per-buffer, so we should chose the target that will end up
compiling the current file.  Ideally we would compile only the current file, but
in larger projects, there are dependencies to keep track of.

So, which target to chose?  I don't think there is a way to get that information
directly from cargo right now, that is:

: cargo which-target src/a.rs

which would return the target name.

In my use case, the project is a library, that also contains a binary as an
example.  So, we should always build the 'lib' target (there's only one of
those), and build the 'bin' target only when looking at the 'main.rs' file.

If the current buffer is a match for the src file of any target, then chose the
according target.  Otherwise, chose 'lib' by default.

That seems to work locally.  Now, onto the PR!

** Making the pull request
Forked flycheck.

Made the changes.  Tried to run the tests... fail!  Ah.

: make specs test

fails because it asks me for passphrase during the tests.  What?

Looking around the source, the passphrase is "spam with eggs".  Now it passes:
: Ran 71 out of 105 specs, 0 failed, in 10.0 seconds.

Some tests are canceled because they need Emacs 25.

Apply back my changes, there is a documentation failure.

I ~ag~ for the option above mine, to look where it appears in the source.  There
is a documentation entry in 'languages.rst'.  I document the new variable, test
passes.

Now, onto the integration tests:

: make LANGUAGE=rust integ

Okay, two tests fail: warning and multiline-error.  Actually, the second failed
without my patch.  Probably a change in the compiler output.  Fixed the test.

The first fails because there is no value for my new variable.  The test project
is a crate named "flycheck".  Put that, all tests pass.

Done.  Now, flycheck-rust!

** Finding the right build target
Had to change the approach a little, because we cannot default to "lib" crate
type in a crate that contains only a 'main.rs'.  So instead of guessing, I just
look the targets up in ~cargo read-manifest~.  First one is the default, and if
we are looking at a file that is specified by the targets, this is the target we
pick.

Simple cases: only one target (lib or bin), that is the one chosen.  Works with
"simple" setups.

Multiple targets: lib, main.rs bin and multiples source files in src/bin.  If
looking at 'main.rs', or any of the 'src/bin' files, those are targets, so they
are chosen.  Any other file will default to the first target.

It's not ideal.  I think it might miss cases like:

: src/a.rs src/b.rs src/lib.rs src/main.rs
: src/lib.rs depends on 'a.rs'
: src/main.rs depends on 'b.rs'

If the default target is 'lib', then Looking at 'b.rs' will pick lib, even
though it's a dependency for the binary.  Converse is true for 'b' and a default
target of 'bin'.

Haven't encountered the issue, because I only have the case where 'main.rs'
depends solely on the lib, and every other file is part of the lib, and the lib
is the default target.

Anyway, unless there is a way to find the target for a file, this will do.  This
can always be overridden by setting the `flycheck-rust-binary-name` manually.

Reviewed the code and added a docstring.  No test suite this time (though it
would not be a bad idea to ensure we don't break any convoluted setups).

* [2016-05-10 mar.]
** Checking the state of Rust tool support
Error output seems to have changed in nightly: [[https://github.com/rust-lang/rust/pull/32756][PR#32756]].

That means Flycheck will soon break in parsing them.  Luckily, there is also a
new [[https://internals.rust-lang.org/t/rustcs-json-output-format/3446][unstable option for JSON output]].  The JSON format should hopefully stabilize
soon.

Speaking of which, using ~-Z no-trans~ for faster compilation is an unstable
flag, and currently outputs a warning.  [[https://github.com/rust-lang/rust/issues/31847][This]] is the issue to follow if we want
this flag to stabilize.

On the horizon, there is also the [[https://github.com/rust-lang/rust/issues/31548][Rust Language Server]], which aims to be a
direct interface for IDEs, providing error checking, completion candidates, find
definition, etc.  But this is only a RFC, awaiting for incremental compilation
progress in rustc.

A good place to check for news on all of this is the [[https://internals.rust-lang.org/c/tools-and-infrastructure][tools and infrastructure]]
forum.

* [2016-05-11 mer.]
** Checking that flycheck-rust works right for everyone's use case
I've tested the basic layouts of src/lib, src/main and src/bin/.  But cargo
allows for some fancy overrides, and I don't even have dependencies in my
projects for now.

[[https://github.com/flycheck/flycheck-rust/issues/7][I see]] that the cargo project itself is a corner case, and indeed it doesn't work
as intended when looking at the src/bin/cargo.rs file.

The cargo.toml of cargo sets the library path directly rather than relying on
the project layout:

: [lib]
: name = "cargo"
: path = "src/cargo/lib.rs"

Note that the path is relative.  And it still is in ~cargo read-manifest~:

#+BEGIN_EXAMPLE
  {
    "kind": [
      "lib"
    ],
    "name": "cargo",
    "src_path": "src/cargo/lib.rs"
  },
#+END_EXAMPLE

But it's an absolute path when ~path~ is not set in the TOML.  Which isn't
really helping as a machine-readable output.  The issue was raised in the [[https://github.com/rust-lang/cargo/pull/1434#issuecomment-94117884][original]]
[[https://github.com/rust-lang/cargo/pull/2196#issuecomment-171411921][pull requests]], but not picked upon.

Solution?  I guess either ensure that the ~src_path~ is always relative to the
crate root, or always absolute.  Leaning towards the latter, as it should be
easier to debug.

However, even if it does check the correct file, it takes several seconds for a
project as large as cargo.  Not sure if that's a good use case of flycheck.

*** metadata replaces read-manifest
In the future, it [[https://github.com/rust-lang/cargo/issues/2356][looks like]] ~read-manifest~ might be replaced by ~metadata~,
which gives much more information, especially on the dependencies.  For the
moment though, the targets section looks identical.

On surprising effect of the ~cargo metadata~ command is that it fetches
dependencies on first invocation before returning the JSON.  Which means that
the first invocation is slow, and the stdout is not a correct JSON, since you
have lines like:

: Updating registry

Though that can be skipped with the ~--no-deps~ flag.

~jq~ can be useful to wade through the metadata dump:

: cargo metadata | jq '.packages | .[] | select(.name == "cargo")'

*** subcrates
A use case of subcrates is the [[https://github.com/rust-lang-nursery/regex][regex crate]], which has regexp-syntax has a
"subcrate": a dependency crate hosted inside the same repository.

In this case, ~cargo read-manifest~ will report the targets for the current
crate.  So if we are in the main crate, or in the subcrate, it picks the right
target.

*** cargo declares mod at compile time
Using macros, which means that files that are part of the binary target are not
picked up by flycheck.

But even without macros, I don't think we would pick it up:

~src/bin/read-manifest.rs~ is a ~pub mod~ (via macro) in ~src/bin/cargo.rs~.
But there's no target corresponding to read-manifest, so how do we know that's
part of the ~cargo~ binary target?

* [2016-05-20 ven.]
** Toying with JITs
Always wondered how you build one.  Another pretext to use more Rust.

Found a [[http://www.hydrocodedesign.com/2014/01/17/jit-just-in-time-compiler-rust/][couple]] [[http://www.jonathanturner.org/2015/12/building-a-simple-jit-in-rust.html][tutos]].  They showed how to create a memory region, mark it as
executable, write a few opcodes, and the magic ingredient: cast the memory
region as a function.  Then, invoke the function, and boum.

Technically, that's just injecting binary code at runtime.  A kind of "metal
eval"... meteval?  meval?

Anyway.

I wanted to know the order of magnitude difference between JITed code and
emulated code.

I wanted to JIT the GB emu.  But that's not done yet.  So, I thought about a
Chip8 emu.  But I didn't have that.  I do have a JS Chip8 emu.

If I code a Chip8 pure interpreter in Rust, then code a JIT interpreter in Rust,
I could compare the performance of each, and see how much a JIT would gain.

I'm also curious as to whether I can compile most of the ROM code directly to
native binary, without inspecting "hot loops" first.  So, technically, AOT.

Started converting that Chip8 emu by following the JS code and looking up how to
deal with slices, or build up an SDL screen as I went.

Works, although there is a strange display bug at the moment.  But didn't have
time for the JIT version tonight.

So I thought, if I want to compare JIT performance to pure interp, and I already
have a JIT for a fixed piece of x86 binary, why not quickly whip up a hackish
x86 pure interp, and see how /that/ fare?

My test program is a loop that counts down from 0xFF000000.  This takes 1.24
seconds to execute JITed.

The pure interpreter is hackish, but does minimal work on top of decoding and
executing opcodes.  It takes 120.96 seconds in debug mode, and 23 seconds in
release.

So, this preliminary test shows a 20 times improvement in performance for the
JITed version.  Quite impressive.

That's enough to entice me to try that on a real emulator!

* [2016-05-25 mer.]
** Gameboy JIT opportunities
Making a note here of the fact that, due to hardware quirks, the following
snippet is the recommended way to access the state of all the buttons in the
Gameboy:

#+BEGIN_SRC asm
LD A,$20       ; bit 5 = $20
LD ($FF00),A   ; select P14 by setting it low
LD A,($FF00)
LD A,($FF00)   ; wait a few cycles
CPL            ; complement A
AND $0F        ; get only first 4 bits
SWAP A         ; swap it
LD B,A         ; store A in B
LD A,$10
LD ($FF00),A   ; select P15 by setting it low
LD A,($FF00)
LD A,($FF00)
LD A,($FF00)
LD A,($FF00)
LD A,($FF00)
LD A,($FF00)   ; Wait a few MORE cycles
CPL            ; complement (invert)
AND $0F        ; get first 4 bits
OR B           ; put A and B together
#+END_SRC

Cycles are wasted with repeated instructions (/debouncing/), because the polling
is not instantaneous.

In an emulator, we don't have that hardware quirk.  So we could coalesce all
these ~LD A~ into one (but still add the cycles of all the ~LD~ calls).

In fact, if this whole sequence is frequent in ROMs, we could just emit binary
that constructs the full byte of button states directly.

Another hint of optimizations is to look for redundant operations, like the ~LD~
above, and systematically coalesce them into one.  These optimizations would be
useful for any piece of code, not just this snippet.

* [2016-06-07 mar.]
** The fastest Chip8 emulator
So, I ported my Chip8 emu to Rust.  To have a smaller code base to test a JIT
with.

I have two ways to recompile a rom. It might be possible to compile the rom when
loading it (AOT): just create a function that does as much as possible in native
code, and jumps back to Rust code for things I don't know how to code in
assembly (e.g., drawing).

I don't yet know how I would jump back to a Rust function.  Is calling the
pointer address enough?

Otherwise, I can watch the code for hot loops, and try to compile those.  So I
need to visualize hot paths, in order to understand what patterns I need to
match.  Which brings me to the second point.

** GUIs in Rust
Been looking for a nice and minimalist way to view the rom disassembly that
updates in real time as the interpreter goes through each opcode.

There's nothing provided by SDL.  Nor OpenGL.  Even writing text in those is a
PITA, and I don't want to be writing code to align two lines of text, to detect
mouse clicks, etc.

There are Rust bindings for GTK, but that does not strike me as friendly nor
minimalist.  And I'm not sure about the portability.

Luckily, I found ImGui which seems to fit the bill.  It renders to vertex
buffers, which can be plugged into an OpenGL renderer, so it's as portable as
OpenGL.  It's certainly minimalist, but it's good enough to have been used in
games and emulators for... debuggers and disassemblers!

Now, the only trouble is: the Rust bindings are light on the documentation
(read: there is none).  The only code example uses Glium as a renderer.  But I
already have an SDL window.  I could launch two windows: one with a SDL backend,
and one with Glutin (the backend of Glium).  But do I have to use threads?  That
could degenerate quickly, and seems opposed to the way ImGui is supposed to be
used.

Maybe I can just keep one loop that polls SDL, draws the frame, then does the
same for the Glium window.

Otherwise, I could switch my SDL rendering to Glium, or any other GL binding,
replace the drawing code with OpenGL calls, then draw the ImGui on top of that.

[later]

Tried only one loop to handle the two windows: one SDL, one Glium.  The ImGui in
Glium works fine, but the SDL windows does not update anymore.  Console is full
of debug errors caught by /Glium/, but the backtrace indicates that the error
originate in SDL2 calls.  Craziest thing.

I can only guess that SDL2 uses a GL context under the hood for accelerated 2D
rendering and, /somehow/, Glium takes hold of that GL context, and that,
/somehow/, they do not like sharing.

The errors caught by Glium are things like "~glVertex2f~ or ~glEnd~ is
deprecated".  Maybe SDL2 uses the old OpenGL API, whereas Glium is only
compatible with 3.0+?  Who knows.

In any case, that means doing the right thing: sticking to OpenGL for drawing
the emu AND ImGui.

* [2016-06-20 lun.]
** Switching chipers to OpenGL
Went full glium/glutin.  Glium is the library for high-level OpenGL bindings.
As I understand, it takes care of allocating GL objects and disposing them for
you.  It also help avoid the unsafeness of the GL API.

Glutin deals with the display manager of your OS to give you keyboard and mouse
events, to create a window, etc.  SDL handled both.  As I understand, Glium is
not tied to Glutin, but both are from the same author, so...

Anyway, using Glium/Glutin is not the hard part.  The hard part is understanding
how to draw things in OpenGL, especially with shaders.

** Drawing colored squares with triangles
With SDL I was just drawing a "point" for each pixel of the Chip8 screen (cixel
henceforth).  And since I only knew how to draw triangles in OpenGL, I thought:
"Hey, let's draw a quad for each cixel!"

And that was a few hours, just to get something on the screen.  Because I had to
allocate a vertex buffer and modify it each frame, figure out how to pass my
vertices to this VBO, how to setup shaders just to get something, how to use a
projection matrix in the vertex buffer so that cixel coordinates would translate
to screen coordinates...

After a copious amount of ddging ([[https://tomaka.github.io/glium/book/tuto-01-getting-started.html][helpful tutorial]] from Glium dev
notwithstanding), I managed to get a Chip8 screen back.  Albeit clipping when
resizing.  And ... with horrible FPS performance after a few seconds.  What?

** Switching to drawing on a texture
I figured that, since I didn't know what I was doing in OpenGL, I must have done
something wrong there.

The SDL version was smooth in frame time (constant 16.666ms).  Since I hadn't
touched that in the conversion, my GL-fu was to blame.

Maybe I was allocating a new VBO needlessly every frame?  Surely that would cost
me.  I don't know how Glium is implemented, but that looked like a potential
inefficiency right there.

So I started to question my rendering solution.  I knew that drawing triangles
was not the only way to draw the Chip8 screen in OpenGL.  It was the only way I
knew /how/.  But what solution did other choose?

Turns out, there are at least 30 chip8 interpreters written in Rust on Github.
And a dozen that use glium for rendering.  As far as I can tell, /every one of
them/ elected to draw the screen to a 2D texture.

The texture is then drawn to a single quad that spans the entire output screen.
No VBO allocation after initialization.  Not even a new texture allocation.

That... seemed alright.  And maybe even simpler that my approach, considering.

Some re-create a new texture for the quad each frame.  I read somewhere on the
Glium API that rewriting the texture contents can cause a CPU/GPU
synchronization, which I guess is bad for performance.  Have not tried to
compare the two approaches in frame time.  I just followed the [[https://github.com/Gekkio/mooneye-gb/blob/master/src/frontend/renderer.rs][guy who wrote a
GB emulator in Rust]].  Good enough for GB, good enough for Chip8, right?

Anyway, I was thrilled to see that the texture approach solved the clipping
issue that drawing quads had.

But, the horrible performance drop after a few seconds was still present.

** Did I enable VSync?
Lots of fumbling around, trying things with timing and what not.

In the end, I /though/ I'd found the issue.  My Nvidia driver had "force VSync"
enabled.  It's weird, because Glutin has a vsync option, which was disabled by
default.  And based on the fact that, in the SDL version, disabling VSync
actually worked, I figured it would be the same for Glutin.  Apparently not.

Disabling this option made the performance drop disappear.. for a while.

But I did encounter it a few times after that.  I guess it's a timing issue,
like not meeting frame time and still going after it.  Then there must be a sort
of mad race of the CPU trying to catch up to a shorter and shorter frame
time...

Probably should fix the main loop next.

** Anyway, ImGui is great
Once rendering to Glium was done, integrating ImGui was a breeze.

Could had a FPS counter, a memory view, and register info.

The only downside of using the Rust binding imgui-rs, is that porting C++ ImGui
examples is not straightforward.

The [[https://github.com/ocornut/imgui/wiki/memory_editor_example][memory editor example]] has nice features, like editing.  But you cannot just
"port" its code imgui-rs, because the API is not at the same level.  ImGui has
~begin~ and ~end~ blocks, while imgui-rs has closures.  Inside Rust closures,
there are mutability issues: you cannot borrow ~self~ mutably more than once for
instance.  I might find a way around it, or I might implement the memory view
using imgui-sys, the low-level binding.

** And GLSL can be great, too!
After battling with GLSL just to get a single color on the screen, I at least
put them to good use.

In my JS version, I wanted a CRT-like effect, since straight big quads on LCD
screen were boring.  Unfortunately, scared of OpenGL ES, I was rendering on
canvas, which meant that the CRT effect was done in software.  JavaScript +
software rendering effects = 10 FPS fullscreen for an emulated 64*32 screen.
Rather sad.

So I was delighted to see that fullscreen CRT + phosphor trail effects were
easily achievable on my machine.  And since I was using GLSL, I figured
/someone/ had battled the language long enough to produce a nice-looking CRT
effect that I could re-use.

Turns out, there are dozens of CRT shaders (especially for retro emulation).
Some of them are in a defunct shader language for NVIDIA hardware, Cg.  Some of
them target the D3D shader language, HLSL.  Some of them use various versions of
GLSL (compatible with OpenGL 2 to 4.. with mystifying shader language versions).

Anyway.  I took one that was convincing enough, banged on it until it worked for
my setup, and voilà.  Convincing effect.

Though I also tested it on my work box (integrated intel chipset from '07), and
it is unbearably slow.  Will add a flag, and might look into optimizations later
on...

* [2016-07-11 lun.]
** Thinking about perspective in 2D games
For a moonshot project.  I was envisioning a side-scrolling view, but I knew
from games I'd played that a top-down view lent more to exploration.  It got me
thinking of perspective choices in classic games.

Zelda 1 is top down.  Top-down gives you two axes of freedom.  It's much more
"open" than a side-scroller like Mario.   In Mario, it is evident you have to go
to the right.  There's no choice.  The difficulty is in getting there.   The
contrast with Zelda is evident: as you start, there are already four choices of
directions: up, left, right, and a cave.  Most of the screens have two exits or
more.  This choice helps convey a real sense of an open world, left to explore.
There's no pressure to the player, even though there is an implicit progression
path.

Contrast that with Zelda 2.  Zelda 2 has top-down overworld, but side-scrolling
dungeons, towns, and encounters.  The towns feel empty and repetitive, even
though they have people moving around.  You are just passing by.  Contrast to
Kokoriko village in Zelda 3: the structures there hamper your movements, they
are real.

But the overworld of Zelda 2 is rather limited as well: there are obvious
paths you should take.  The map is too much gated: you cannot go there yet,
cannot go there yet, etc.

The dungeons in Zelda 2 mostly feel like corridors.  The side-scrolling make
combat harder than it should be.  There are strong Castlevania vibes, except
with a puny dagger instead of a satisfying flail.

Castlevania, Megaman, Duck Tales... the side-scrolling lends itself more to
action than exploration.

But Metroid shows you can still pull off exploration in a side-scroller.

Roguelikes have been predominently top-down.  Rogue, Nethack, and the like.
Although this might have been motivated by technical limitations, the choice has
been deliberate in modern variations: Isaac and Necrodancer.  Though Isaac was
clearly inspired by Zelda 1, and Necrodancer rhythm component might have left
only the top-down option.  Risk of Rain chose a side-scrolling view, and it
makes the level much less interesting.  But again, that might just be because
levels are mostly empty, rather than caused by the perspective choice.

One thing is certain: in a side-scroller, the character usually obeys gravity.
Jumping becomes the basic way to use the second axis of freedom.  Otherwise you
have the clunky stairs of Castlevania.  Now, a game with jumping will lend
itself more to platforming than pure exploration.  This opens opportunities for
combat design: the fights in Zelda 2 are more involved than in Zelda 1.  But
Isaac shows that a top-down perspective can also have deep combat: it's mostly
about constraining the space the player can move to.

* [2016-07-26 mar.]
** About DSLs
So when you build any application, at some point you realize that you want a DSL
for maximum expressivity.

But there are various needs for a DSL, and various ways to build them.

For instance, in JS, there's a common idiom called a fluent API:

#+BEGIN_SRC js
$('#a')
  .css('color', 'blue')
  .toggle()
  .on('click', ...)
#+END_SRC

jQuery and D3 make heavy use of it.  I like to think of it as a DSL: it really
is a different language than plain JS, with different composition rules.  When
you begin an expression with ~$()~, you mentally switch into jQuery mode, to
know what you can follow.

The jQuery language is actually rather simple, the usual pattern is:

: $(selector)
:     .more_selection()
:     .manipulation()

First you target the elements you want to manipulate, then you manipulate them.
Pretty simple.

It happens to like the builder pattern used in Rust to build objects:

#+BEGIN_SRC rust
let display = glium::glutin::WindowBuilder::new()
  .with_title("Chipers")
  .with_dimensions((screen::SCREEN_WIDTH * zoom) as u32,
                   (screen::SCREEN_HEIGHT * zoom) as u32)
  .build_glium()
#+END_SRC

Here we are just building a configuration object.  The grammar is also rather
simple:

: FrobinatorBuilder::new()
:          .with_a()
:          .with_b()
:          ...
:          .build()

Bonus: there are actual types to these functions so the compiler can complain if
you mess up the grammar, like ~build~ before ~new~, or two ~build~ in a row.

D3 also has a fluent API.  There, the grammar can be a little more complex, with
the select/join mechanism, and things like ~enter~.

A simple language is one that builds an AST, you just compose functions:

: seq(assign(var(x), plus(num(1), num(2))), print(deref(x)))

The grammar is simply:

: expr: seq | assign | var | plus | num | print | deref

I'm wondering what happens when you take object algebras, but you only really
need one interpreter, not many?

#+BEGIN_SRC js
/* eslint-disable */

var e1 = m => { with(m) {
  return plus(num(1), num(2))
}}

e1 //: function

var interp = {
  plus(a, b) { return a + b },
  num(n) { return n },
}

e1(interp) //: 3

// Might as well

var plus = (a, b) => a + b
var num = n => n

var e2 = plus(num(1), num(2))

e2 //: 3

// thunk it

var e2t = _ => plus(num(1), num(2))

e2t //: function
e2t() //: 3

// How about partial evaluation?

// Here is a program

var e3 = m => { with(m) {
  _def('rec', _ =>
       _if(_less(0, 2),
           _ => 1,
           _ => _call('rec')))
  return _call('rec')
}}

e3 //: function

var _eval = {
    _v(n) { return  },
    _def(f, b) {
      this[f] = b()
    },
    _if(c, t, e) {
      if (c) { return t() } else { return e() }
    },
    _less(a, b) { return a < b },
    _call(f, a) {
      return this[f]
    }
  }

e3(_eval) //: 1

// Well, that's not very interesting

// Thunk everything?

var ast = {
  plus(a, b) { return {
    eval() { return a.eval() + b.eval() }
  }},
  num(n) { return {
    eval() { return n }
  }},
}

e1(ast).eval() //: 3
#+END_SRC

Okay, that was crap.  Time to forget.

* [2016-07-29 ven.]
** Revisiting the Game Loop
All [[https://www.youtube.com/watch?v=fdAOPHgW7qM][these]] [[https://www.youtube.com/watch?v=jTzIDmjkLQo][talks]] helped me understand how a game loop should work.  But it also
applies to any simulation, including emulation.

As usual, I prefer to go from most straightforward solution, and understand
/why/ it's wrong, and /why/ the correct solution is not the first that pops into
my mind.

So, the first game loop I remember writing was an OpenGL Pong.

I lifted code from NeHe's OpenGL tutorials, and hacked it until I had a game
working.  The tutorial code already took care of pushing a triangle to the
screen.  It used OpenGL direct mode, which was easy to pick up, so I just
changed it to have two rectangles at the edges of the screen.

Then came input.  Here again, the tutorial had code for grabbing input from
Win32.  I just had to find the right place, the correct keycode, and move the
rectangles by a reasonable amount.  The code looked like so:

#+BEGIN_SRC c++
void handle_input(...) {
  ...

  if (is_keydown(VK_UP)) {
    player1 += 0.12f;
  }
  if (is_keydown(VK_DOWN)) {
    player1 -= 0.12f;
  }
  if (is_keydown(VK_A)) {
    player2 += 0.12f;
  }
  if (is_keydown(VK_X)) {
    player2 -= 0.12f;
  }

  ...
}
#+END_SRC

Now I had moving rectangles!  Then I moved to collision detection, which as I
remember was solved with a bunch of ~if~.  Anyway, it worked great!  Surely I
had to tweak the move values above until if felt right–not too slow, but not too
fast either.  At this point I was rather proud.

So I copied the game onto a floppy, and brought it into school.  It so happens
that we had a computer room, to which I had access at any time between classes
because I helped set it up, along with other students.  So I put the floppy in,
launch the executable and behold!

Oh wait, it's all going /much too fast/.  Even the slightest input will move the
paddle half a screen worth; it's barely playable.  And the ball just passed
right through the right paddle without hitting it!  What happened?  It was
working right on my machine.  Needless to say, my friends were only mildly
impressed.

Of course, now I understand perfectly why it happened, and why I made that
mistake.  The computers at my school were simply faster than the one I had at
home.  I don't remember if there was any syncing to a fixed framerate or to the
monitor refresh rate in the NeHe code.  If there was, it might be that I was not
hitting that framerate at home, but I doubt it as it was /OpenGL/ for rendering
two rectangles paddles and a square ball, not software rendering.  So maybe
there wasn't any framerate limit in place, and the computer at the school just
went as fast as possible.

Now, having only written a handful of programs, this was my first simulation.  I
had written interactive text-based games, but these were turn-based.  You print
something to the screen, wait for user input, then print something else.
Running it on different computers would get you the same results.  For other
programs that sort numbers or print something to the screen even without
interaction, you usually /want/ them to run faster on beefier computers.  So I
did not even think twice at how that would play out for a simulation.

In a game like Pong, you want it to behave the same from one computer to
another, regardless of the specific hardware that supports it.  If you think
that the ball moves at 1 pixel per frame, then the game will feel faster at a
lower resolution, or at a higher frame rate.  OpenGL already frees you from the
actual display resolution, by giving you a continuous space for positioning
objects: the paddle moves by ~0.12f~ each frame, not 1 pixel.  You have to think
of time as being continuous as well: say, the ball at ~0.3f~ each 60th of a
second.

Then you understand why you cannot write the game with a ~while(true)~ loop that
just simulates and renders as fast as possible.

** Sampling player input
One thing that I might have missed from my [[*What’s the ideal solution to input latency anyway?][previous discussion on input latency]].

Consider sampling a simple button press:

: __________----------__________
:  10ms        10ms      10ms

This is continuous from the player point of view.  But if we sample, say, every
20ms, depending on where the sampling begins, we might miss the button press
altogether:

: __________----------__________  signal
:    |   20ms            |        samples
: ______________________________  reconstructed

Because the game has to reconstruct the signal from the sampled points, and the
two samples are 0 (button up), the game never sees that the player has pressed
the button:

Now if have a 6ms sample rate:

: __________----------__________
:   |     |     |     |     |
: _________------____________
: _________-----------_______
: ______________------_______
: ____________-----__________

If the signal changes between two sample points, there is an issue.  You don't
know exactly when the signal changed, so you have many ways to interpret it.

Usually in the code I write, I just look if the button is pressed, then simulate
as if it was pressed for the duration of the frame.  So you end up with:

: __________----------__________
:   |     |     |     |     |
: ______________------_______

we see that are already losing information.

Now, if we are sampling every 4ms:

: __________----------__________
:   |   |   |   |   |   |    |
: __________------------_____

it does not matter if we are below the Nyquist frequency for sampling, because
the function is not continuous I guess?

Anyway, if we sample at a high enough rate, hopefully the user won't notice the
discrepancy between their input and the input synthesized at the screen.

* [2016-07-30 sam.]
** Revamping S3C for evaluation inside blocks
See [[https://github.com/fmdkdd/s3c/issues/4][issue 4]].

Managed to make it work using esprima + estraverse + escodegen.

First: using those on the browser is kind of a shitty situation without modules.
I can install esprima with bower and use that directly.  Fine.  Then, estraverse
is also on bower, so I install that.  But the file is not browser compatible,
you have to use browserify.  Ok fine, I install browserify and run it, then get
something I can import in my HTML and it works.  Finally I need escodegen.
Surprise, the bower package does not work.  I try npm, I see that one can build
a browser version from that (not using browserify mind you, but another tool,
cjsify).  Does not build.  Ok, there's an issue and even a pull request for
that.  You can't build the browser version from the npm package; you have to
clone.  I clone, build, and now I have a browser build!

Three related modules, three ways to get the browser version.

So at the moment I have the basic functionality of evaluation markers working,
even in blocks.  There are changes from the previous evaluation model though.

Previously, we split the evaluation of the code everywhere there was a marker.
So if an expression evaluated to an error (even a syntax error), we would
evaluate the rest of the program without the error impacting us.

Now, we evaluate the whole program at once, and collect the values of the
expressions that have markers.  If there is an error at the start of the file,
it's less resilient.  Also, esprima will fail to produce an AST if there is a
single syntax error.

I don't have errors working yet, but we are already losing functionality I'm not
sure I can get back.

Cleaning up the logic.  I can't use the backlog method because now because
markers in blocks may receive multiple results.

Cleaning up more, I have errors and timeouts working again.  For the moment one
error stops evaluation for the whole program.  Maybe I can capture them by
wrapping the expression statements in a try/catch...

Speaking of which, I tried to put a marker inside a try/catch and it did not
work.  Must investigate later.

Now I'm trying to see if the code I have from my PhD manuscript works with the
new logic.  And... SYNTAX ERROR AT LINE 2.  Esprima fails to parse ES6
syntax... sigh.  Wait, the README says it /does/ support ES6.  Latest version is
2.7.2, and ... the heck.  I have 2.0.0.  Well, thanks bower.  Guess I'll just
grab the latest version and manage it by hand then.

Ah, now that's funny.  Because ESLint also uses esprima, but an obsolete
version that's bundled inside the file.  That's at least three different parser
for the same project.

Anyway, updated Esprima, and the example works!  Except I have to try/catch the
one deliberate error.

Oookay.  Fixed evaluation markers in IIFEs.

Problem was twofold: first could have multiple markers associated with the same
evaluation comment.  But only one them would receive a result back from the
worker.  So ~undefined~.

I fixed this by using a map to keep track of comments already seen and the
marker we constructed for it.

The we did not associate evaluation comments to the nearest parent expression
statement, but to all expression statements above.  Thus, in an IIFE like:

(function(x) {
  x //:
})(1)

there would be two ExpressionStatements: ~x~, and the IIFE.  Both would be
associated to the one evaluation comment, and receive a result from the worker.
And the second result would overwrite the first, so ~undefined~.

I fixed that by doing a first traversal of the AST to find evaluation comments
and associate them with the nearest parent expression statement.

IIFEs work.  Try/catch works.  Loops work.  ~with~ works.  Useless braces blocks
work.

I have slightly changed the semantics of the evaluation marker though.  Before,
it would give the result of the /last expression/.  Now it gives the result of
the nearest parent expression.

Okay, wrapping the expression in a try/catch allows me to prevent errors from
polluting the rest of the results.  I added an alternative syntax for this
behavior though, as it can be unexpected inside a try/catch.

* [2016-07-31 dim.]
** Updating ESLint
So I want to update ESLint because the parser is out of date.  And the
browserified file is disgustingly huge (671K).  I get the latest version, well
they are still using browserify.  The output is now 2.7M.

Okay, been looking around.  It's a bit ridiculous to charge that 2.7M, but there
might not be an easier way to get an up to date version of ESLint.

I've noticed that ESLInt is using a fork of Esprima, espree, so I can't factor
that out.  It might make sense to use espree as well, or even Acorn.  Shouldn't
be too much a bother since the interface seems compatible with Esprima's.

Maybe I'll just try to uglify ESLint and see how that goes.

Making a note here that there's a way to get back the parsed AST from ESLint.
Should I want to reuse it.  But I'm not sure it would make a difference.

Using Uglifyjs compression and mangling slims down ESLint to 808K.  An
acceptable size bloat for the gained functionality.  Okay, let's minimize
everything while I'm at it.

aaand updated CodeMirror to latest version.

Done & uploaded.

* [2016-08-01 lun.]
** Performance issues
It didn't feel like the new version of s3c was any slower than the previous
one.  On my home machine.  On my work machine there is perceptible delay.
Around 500ms I would say, but can't say exactly since profiling does not even
work under Firefox.

So on my machine a full eval cycle + rewrites takes 75ms:
- triggering the eval takes 45ms with 35ms spent in ~reval~ (15ms parsing, 8ms
  clearing the markers on the page) and 10ms lost in ~endOperation~.
- the remaining 30ms are spent in ~write~ calls.  Each write averages 1.5ms.

And that is /after/ doing a first optimization, which is fixing the size of the
editor.  Previously the editor had ~height: auto~.  But that meant that any
change to its content would be written back to the DOM, even if that content was
outside of view.  CodeMirror does not do a hit test to check if it's in view.
Instead, you should let CodeMirror handle the scrolling.  Doing that shaved 30ms
off.

Also of note is the time to evaluate the JS: 277ms, and 47ms to finish ~init~.

Reusing the AST from ESLint is a big improvement.  But, it's not equivalent.
Linting happens sporadically (debounce + 500ms), so Ctrl+enter just after an
edit will have an outdated AST.  Linting takes 121ms on the same buffer: 50ms
parsing and the rest applying rules and update the DOM.  121ms is the first
time, after I get around or below 50ms.  Maybe JIT optimizations kicking in?
Might be worthwhile to reduce the linting delay and have linting always happen
before we have time to trigger evaluation.  Then we reuse the AST.

Was trying to reuse the ESLint AST in this fashion, but hit a weird behavior
where after a first eval, the subsequent evals did not refresh the markers.
The markers are empty the second time around.  Not sure why.  But it negates the
visual feedback of clearing the markers.  Maybe I can get the visual feedback by
flashing the Run button instead?

Reusing the AST shaves 15ms off, but is not quite correct yet, since we have to
detect if the text has changed since before the last lint, otherwise triggering
eval reuses the obsolete AST and it does nothing.  I have to think through the
whole pipeline as:

user changed text -> debounce to 250ms -> reparse (ideally, with a parser that
does not start from scratch) -> give AST to linter

But if reval is triggered and we don't have a fresh AST, then reparse, eval, and
save the AST for linting afterwards.

In the meantime, I've got it down to spending only 10ms to reval and 10ms to
rewrite.

But, only now I finally find that the worker takes 25ms to actually eval the
code.  And from hitting Ctrl+Enter to seeing the eval results, it's around
350ms, mostly of waiting around for debouncing.

* [2016-08-02 mar.]
** Links on incremental parsing
Not sure it would be worth it for the scale of the code that s3c deals with, but
here are some resources on incremental parsing would I want to pursue it (or
just out of curiosity):

- [[http://harmonia.cs.berkeley.edu/papers/twagner-parsing.pdf][this paper]] from 1998 seems to cover the theory, and even provides the Java
  code for its algorithms for incremental parsing based on LR grammars.
- [[https://github.com/Eliah-Lakhin/papa-carlo][this project]] is an incremental parser in Scala using PEG grammars.

Intuitively, we might get good mileage out of a few heuristics like looking at
blocks: if I change a character inside function ~f~, then at worst we only need
to reparse the node for this function.  Given a change, walk up the tree to the
first block, throw the node, reparse and replace.  Now, 1) I don't know how
sound that actually is, and 2) now sure how it holds with larger changes (a find
and replace, or an undo).  The pathological example would be: erase everything.
Now parsing from scratch the empty string should be faster than walking the tree
checking if every node is still there.

The problem can also be entirely side-stepped with an editor that would only
allow actions that modify the AST without ever creating an invalid one.  Rather
than editing at the character level, you edit at the AST node level.  But I
don't know how practical that can be in the end.

Anyway, all of that might not even matter for speeding up s3c, since parsing
might not even be the biggest bottleneck.

* [2016-08-13 sam.]
** Using the JSON error format of rust for flycheck
*** Restoring functionality
Previous message parser was rather straightforward: error appeared as errors,
warnings as warnings, and note or help lines appeared as info squiggles.

In the JSON output, we have multiple spans that corresponds to squiggles.  One
span is the primary (the root cause or main line of the error), and the others
seem to correspond to notes in the compiler human readable output.

*** Passing tests
The JSON output is the same format for stable and nightly, but the exact output
can change from version to version.

*** Changing flags triggers a rebuild?
There was a mention on a thread somewhere that using RUSTFLAGS to ask for
~--error-format~ in IDE can trigger a full rebuild of cargo.  Can't reproduce in
our setting; maybe because we don't use RUSTFLAGS but call ~cargo rustc~?

*** Flycheck does not use line or column end points
Squiggles only overlap the symbol at the given line/column, but rustc will
output the start and end position already.  Flycheck does extra calculation for
nothing, and it's less accurate than rustc's info.

Sebastian outlined the steps for accepting column pairs in flycheck ([[https://github.com/flycheck/flycheck/issues/89][issue 89]]),
but that might be outdated.

*** Looking up explanations from Emacs
rustc provides explanations, but I don't think that's flycheck's job to show
them to us.  I could write a function ~explain-rust-error~ that looks at the
code of the error under the cursor (when flycheck is loaded) and opens a
temporary help buffer with the explanation.  Without flycheck, it asks for an
error code interactively.

* [2016-08-15 lun.]
** Using column end points for rustc in flycheck
I started by using cons cell for columns instead of a number.  Then flycheck
complained the checker returned an error.  But since it caught the error, I
could not use the debugger to trace it.

There are multiple places where columns are used.  I managed to hack my way
through them until it worked.

I had assumed that just reusing the column value of rustc for the overlays would
work... but overlays only use a single coordinate for their start and end
points.  I had to convert the (line column) information to a single character by
piggy-backing on ~flycheck-error-column-region~.

And it works!

But it is at odds with the notion of flycheck highlighting modes.  I think the
behavior we want is: try to use the line/column info returned by the checker,
otherwise fallback on the selected mode: lines, columns, symbols, sexps.

We want to fallback because not all tools might give column end information.

* [2016-08-16 mar.]
** Imaginary property
This morning when coming over to work, I was having an internal debate about one
of my pet peeves: copyright.  Or, how I prefer to call it, /imaginary property/.

Note that I have no claim of originality on this moniker.  To the extent that
someone /can/ claim precedent on a juxtaposition of two words.  But after all,
since many companies do hold rights to such juxtapositions in the form of
slogans, brands, or product names, you never know.  I can however cast any doubt
that I thought of it first, as I encountered it years ago on the news site
Slashdot, where a user went by the asserting handle
"I_do_not_believe_in_imaginary_property".

I was having this internal debate.  Oh, an internal debate is basically what it
says on the cover: me having an argument in my head, with at least two voices
making their points in order.  These debates tend to play like a mix of chess
and golf.  Each side is carefully considering their next move to find the best
play.  They want to corner the opponent, and not leave him options to escape.
At the same time, I, as the observer, want to find arguments that have the most
weight, that raise the most interesting questions.  I try to take each argument
charitably, as the purpose is not so to that one side wins, but to better
understand each side's point of view.

Thus, this morning debate's was about imaginary property.  Now, I like this term
because it is not neutral at all; it's a moral statement.  Not unlike the word
"copyright" itself: the "rights of copying" is not an innocent denomination.  If
you accept the word, you accept its moral premise: that copying should be
regulated by rights.  The same happens In the french terminology, where our
copyright law is an "intellectual property law".  If you silently accept the
name, you tacitly agree that there is such a thing as an intellectual property.
The assumption here is that coining terms such as "intellectual property" is a
weasely way to conjoin your mental representations of both concepts.  With this
connection unconsciously made in your brain, you are eased into taking this
chimeric concept as a fact.  The choice of words here is truly Orwellian.

One of my mental orator disagrees with this premise, and counters with a loaded
term of his own: you speak of intellectual property, but I say it is imaginary.
The whole premise is refuted, so that any further arguments on the specifics of
copyright is moot.  It steers the debate to whether it is even /possible/ to own
thoughts in the first place.  It is powerful opening move.

The answer may seem obvious.  /Cogito ergo/ dibs.  Who is doing the thinking?
/I/ do, therefore the thoughts are mine.  Consequently, any product of these
thoughts is also mine.  Well, that may be tautological for some, but I do not
see how it follows.  We could again argue the premises: that there is an "I",
that there are thoughts to be had, that our experience of individuality is not
just an illusion, a side effect to the working of our brains.  However, at this
point in the debate most interlocutors would question my sanity and leave the
room (through my ears).

Fine, we'll take another route.  We can concede the reality of consciousness and
individuality.  These are convenient concepts after all—allowing me to use "I"
all along this text without eliciting existential conundrums.  But we can still
question the ownership of thoughts.  For me, ownership by the thinker is not
obvious.  Here's how I /think/ it works: thoughts are not created; they are not
elaborated by the sheer power of our will, they are merely witnessed as they
happen in the brain.  The brain is made of neurons; neurons stimulate each other
by chemistry and electricity; so much we know for a fact.  Now, to the best of
my knowledge there are no sound explanation of what a thought is in term of
neurons.  It might forever be an ill-defined notion, even if we someday crack
the brain's secrets.  I will make the reasonable assumption that if we have any
thoughts, they are caused by neuronal activity.  I like to picture the neurons
as a large and dense graph.  Millions of nodes, billions of edges.  A thought is
then a collection of /bounded walks/ along the graph.  Some neurons are excited,
they light up, thinking happens.  Due to the size of the graph, the number of
different walks in just one brain is practically infinite.  In this view, one
can have infinitely many different, unique thoughts.  But two exact same walks
would produce the exact same two thoughts.  All our thoughts thus depend on two
factors: the brain configuration, the way it is wired up; and the initial
stimuli, the start of the neuronal promenade.

If we accept this model, we must see that in order to claim ownership over these
thoughts, we must be in total control of these two factors.  But how can it be
the case?  The initial stimuli is clearly not entirely in our control.  We have
no way of forcing thoughts through some neuronal pathways.  It can /feel/ like
we are steering the boat, but there clearly is something happening at the
unconscious level that is doing the heavy lifting.  And the other factor is
mostly genetic and/or environmental, depending on your stance.  If you do not
believe in free will, then trivially you cannot say to be in control of your
thoughts.  If you do believe in free will then you can think your daily actions
may have an impact on your brain configuration.  But this impact is at best
indirect.  Your thoughts are what they are because you where brought up in
/that/ city, in /that/ neighborhood, in /that/ country, on /that/ planet, and
you grew up with /that/ family, /these/ friends, and you read /these/ books, and
listened to /that/ music, and visited /that/ place where you had all /these/
memories...  All of that shaped who you are, and what you think, and it
continually keeps doing so.  And you cannot reasonably claim ownership of all of
these factors.

-----

Most proponents of copyright conflate two topics: the regulation of copying
intellectual works, and the remuneration of the authors of said works.  A
simplified, but too common argument goes like this: "Well, copyright is a good
thing because that's how artists get paid."  To which one would answer: "No,
copyright is /wrong/, because it goes against our instincts to share".  Stop!
You are arguing different things!  One is for the remuneration of artists, while
the other is against the criminalization of sharing!

How I can conceive that we have these views:

- making your mark in imaginary space, obtaining a plot of intellectual land

That's by analogy to physical property.  But maybe this analogy is not
appropriate?

Intellectual colonists?  Who go and appropriate themselves a plot of
intellectual land.  We do seem to treat intellectual property as we do physical
property.  We can cede these lands: rights of exploitation.  We even have a word
for intellectual property trespassing: plagiarism.


The view of an untainted vision, the lone genius:

- seeing your work as optimal?  Then any deviation would invariably lessen it.

I stumble upon an indie gameboy color game.  The sources are given in a CC-NC
license, but the music strictly forbids /derivatives/.  How can anyone sustain
this position?

Here are the musicians in question:

#+BEGIN_QUOTE
The discussion and copyrights are mostly to protect the original score and its
original vision. I can't really give you much more info than that, mostly
because I wasn't the one who was negotiating all of this.

I had a lot of fun converting and worked really hard with the music translating
it to GBC though. It'd be a shame to hear it modified. So I believe the
negotiations were meant to protect our involvement as well.
#+END_QUOTE

#+BEGIN_QUOTE
The status of the game as of now is open source with special rights, music
cannot be used elsewhere/modified without my consent (Eric E. Hache) and no
commercial endeavours. For the rest of the licensing, please check Affinix’s
Github license file.
#+END_QUOTE

* [2016-08-17 mer.]
** Chasing a failing build under emacs snapshot
Trying to reproduce the Travis errors on my machine.

~./autogen.sh~ suggest I run ~./autogen.sh git~ after it.  This is not done in
the makefile.

Trying to run the tests with emacs 25, cannot find ~dash~.  ~make clean~ and
~make init~ fixes it, and now I have an error because warnings (same thing as
the Travis build):

#+BEGIN_EXAMPLE
In toplevel form:
flycheck.el:5443:1:Warning: Unused lexical variable ‘\.label’
flycheck.el:5443:1:Warning: Unused lexical variable ‘\.column_start’
flycheck.el:5443:1:Warning: Unused lexical variable ‘\.line_start’
flycheck.el:5443:1:Warning: Unused lexical variable ‘\.file_name’
flycheck.el:5443:1:Warning: Unused lexical variable ‘\.is_primary’
#+END_EXAMPLE

So just having warnings trigger a non-zero exit in Emacs 25?

As I suspected, the warnings are caused by nested ~let-alist~ calls.  With just
one ~let-alist~ the compiler does not complain, but when they are nested all the
~.name~ inside the nested calls are considered free variables.

Unnesting these calls make the warnings disappear.

But why are warnings appearing in the first place?  My understanding is that
~let-alist~ is a macro that adds syntactic sugar for looking up the alist.  This
is expanded at compile-time, and thus all ~.name~ should disappear.  But the
byte compiler still sees those that are in nested calls to ~let-alist~, so the
macro expansion is not recursively done?

#+BEGIN_SRC emacs-lisp
(cl-prettyexpand
 '(let-alist '((a . 1) (b . 2)) .a))

(let ((alist '((a . 1) (b . 2))))
  (let ((\.a (cdr (assq 'a alist))))
    \.a))
#+END_SRC

#+RESULTS:
: 1

#+BEGIN_SRC emacs-lisp
(cl-prettyexpand
 '(let-alist '((a . 1) (b . 2))
    (let-alist '((c . 3) (d . 4))
      .c)))

(let ((alist '((a . 1) (b . 2))))
  (let ((\.c (cdr (assq 'c alist))))
    (let ((alist '((c . 3) (d . 4))))
      (let ((\.c (cdr (assq 'c alist))))
        \.c))))
#+END_SRC


Ah!  The first ~let~ line triggers the warning.  This is because ~let-alist~
thinks every ~.name~ under it should apply to it, but this is false when nesting
calls.

If the expansion happened from the innermost ~let-alist~ first, I guess this
would work.  So it's a bug in ~let-alist~.

The docstring of ~let-alist~ acknowledges that you can nest it, with the
downside that you cannot access the variables of the outer ~let-alist~.  This is
obviously because the inner ones shadow the ~alist~ variable.  This is a hygiene
issue.

So, two bugs for nesting.  Thanks, ~let-alist~!
