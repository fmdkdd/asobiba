#+OPTIONS: ^:{}

* [2015-09-30 mer.]
** which-func for org-mode                                            :emacs:
:PROPERTIES:
:header-args: :results none
:END:
Trying to come up with a fast equivalent to ~which-func-mode~ under Spacemacs.
The most naive implementation would be to lookup backward for the first heading.

#+BEGIN_SRC elisp
(spacemacs|define-mode-line-segment which-org-headline-segment
  (fmdkdd/org-current-headline)
  :when (eq major-mode 'org-mode))

(add-to-list 'spacemacs|define-mode-line-segment 'which-org-headline-segment t)

(defun fmdkdd/org-current-headline ()
  (save-excursion
    (re-search-backward org-complex-heading-regexp nil t)
    (match-string-no-properties 4)))
#+END_SRC

That does not give you the full current hierarchy (bread crumbs).
Actually, there is an ~org-get-heading~.

#+BEGIN_SRC elisp
(spacemacs|define-mode-line-segment which-org-headline-segment
  (org-get-heading)
  :when (eq major-mode 'org-mode))
#+END_SRC

It even gives us the font-lock properties.

There is also a ~org-get-outline-path~ that gives the rest of the crumbs.

#+BEGIN_SRC elisp
(spacemacs|define-mode-line-segment which-org-headline-segment
  (fmdkdd/org-current-headline)
  :when (eq major-mode 'org-mode))

(defun fmdkdd/org-current-headline ()
  (let ((path (append (org-get-outline-path)
                      (cons (org-get-heading t t) nil))))
    (org-format-outline-path path 40)))
#+END_SRC

Removing the text properties can be achieved by calling
~substring-no-properties~.  Though I rather like the effect as is.

Another, longer (but more proper?) way of removing them is the following:

#+BEGIN_SRC elisp
(defun fmdkdd/org-current-headline ()
  (let* ((path (append (org-get-outline-path)
                      (cons (org-get-heading t t) nil)))
        (formatted (org-format-outline-path path 40)))
    (set-text-properties 0 (length formatted) nil formatted)
    formatted))
#+END_SRC

* [2015-10-01 jeu.]
** Collaborative editing in Emacs                                     :emacs:
I would like to be able to use Emacs for collaborative editing.  I have light
requirements:

- over local network would suffice.  I just want to be able to share a buffer
  with someone next to me, each with their own computer.  For pair programming
  or teaching.
- I don’t care much about security: I trust the other person since she is right
  next to me.  When we finish, the computer should not be left in a vulnerable
  state however.
- I prefer to stay with my own Spacemacs config, rather than having to use the
  config of the peer.
- it should be painless to setup, and stable.

This [[http://stackoverflow.com/questions/12546722/using-emacs-server-and-emacsclient-on-other-machines-as-other-users][SO thread]] is a good start.

The scenario is as follows.  Host is where the files to be modified reside.
Host has an Emacs session and buffer on file A.  Client wants to drop in Host
and take control of Emacs from his machine, and edit the same buffer.  He can
also split windows, switch buffers, etc.

*** Using tramp and ssh
Client needs ssh access to Host.  Client can browse to file A from his Emacs.

However, Host will not see the changes until Client saves.  This is
insufficient.

*** Using ~make-frame-on-display~
Emacs can spawn a frame on another X display.

The requirements:

- Allow X to listen to TCP connections.

  Under Ubuntu, X is spawned by lightdm, so, in =/etc/lightdm/lightdm.conf=
  : xserver-allow-tcp=true

  and restart lightdm.

- After that, allow the Host to access the X server with xhost
  : xhost +host

  In Ubuntu, my LAN machines can be accessed via =host.local=.

  One can also use ~xauth~ here, as described in the SO thread.  Deauthorize the
  Host with ~xhost -host~.

- Finally, the Host can spawn a frame from its Emacs on the client display
  server with
  : make-frame-on-display client.local:0

Now, Client can write in the buffer, and Host can see the changes.  Both can
even edit at the same time.

Seems stable.  There is the issue that if any of the peer starts a modal action
(helm lookup, M-x minibuffer spawn), the other cannot type anymore.  When the
modal action is over, the input will be sent to the frame however.

This is a distinct frame, so Client cannot control the Emacs frame on Host.
Splitting windows should be done on both machines.  Client can browse the Host
files.

Host only has one command to spawn.  But Client needs to restart X before
pairing, because tcplisten seems like a fun backdoor to leave open.

Alas, Client is stuck with the Emacs configuration from Host.  This cannot be
avoided, since there is only one Emacs process.

*** Using tmux
As suggested [[http://www.emacswiki.org/emacs/tmux_for_collaborative_editing][there]].  However, I could not make the socket sharing work.

Rather, sharing the same tmux session is simple:

- Host does ~tmux new -s pp~ to create a new session named ‘pp’.
- Client does (connected on Host) ~tmux attach -t pp~ to join the session.

With tmux, Client can connect to Host using ssh, and join a tmux session.  Both
share the same cursor.

Since tmux is terminal-based, Emacs runs in tty mode.  Functionality is the
same, but can be unfamiliar for Host.  Using frames would be possible through
ssh X forwarding, but that would not give us more than the previous solution.

Client has to use Host Emacs config, again.  The setup is also slightly more
involved with Host.

But, sharing through tmux is useful beyond Emacs.  So there is that.  And this
solution should work well over the network (if you can ssh to Host).

There is even a wrapper around tmux called [[https://github.com/zolrath/wemux/][wemux]] which simplifies the setup and
provides relevant options for multiple peers.

*** Using floobits
A proprietary web service.  Use a Github account, create a workspace (?) and
share files.  Other users you have authorized can then access the workspace, and
you can see the changes in realtime in your editor if you are viewing the same
file.

Rather nice is that every peer is using his own machine and editor.

However, it goes through the Floobits server, thus it’s pretty slow compared to
the previous solutions.

And there is the requirement of going through a workspace.  It might make sense
for collaborative realtime editing of a project, though I’d rather use Git then.
But it’s cumbersome to setup when playing on a throwaway file.

The nail in the coffin is of course having to go through a third-party.  If the
server software was at least available as open source, I could run a local
instance and that would be a pretty good solution.  Alas, that does not appear
to be the case.

*** Using rudel
[[http://rudel.sourceforge.net/][Rudel]] is an Emacs package which share functionality with Floobits.

One Emacs must host a rudel session.  Others can join.  The host does not take
part in collaborative editing.  The host passes editing data from one peer to
another using an open protocol.  Other clients can join.

Users in a session can publish a buffer, and others can subscribe to it.  When
you subscribe to a buffer, Rudel opens a new window with the buffer text
inside.  You can then edit the text with your own cursor, and editor.  Changes
are highlighted with the color of each user (that can be disabled through the
menu option, thankfully).

Rudel is intended to work with menu-bar-mode on, it seems.

I don’t know what data Rudel sends, but from the project website, it seems it
can break the functionality of some modes like EShell.  This behavior can be
troubling.

I’m not sure what exactly is the buffer a client edits: does it have a local
copy?  Does it exist only temporarily?

Speed is alright, but slower than tmux and xhost.

Also, the setup is a bit more involved, and the package is in dire need of
maintenance.
* [2015-10-21 mer.]
** Explanations
In web apps, I find it would be useful to be able to ask why a value is 0, or
NaN.  E.g., why a DOM element has its ‘left’ property to ‘12px’.  I would like
to find the culprit code immediately.  Alas, there are no ‘conditional
breakpoints’ in Firefox or Chrome.

Wait, there are!  You can break on attribute modification by right-clicking a
DOM in the Elements panel in Chrome.  In FF, you can conditionally /stop/ a
breakpoint, but not break conditionally.

Anyway, jumping into the debugger when a value is modified is only one part of
the workflow.  That gives you the place where the value is set, but not how the
right-hand side was computed.  You have to backtrack through the call stack to
get this information.

Instead, if a value contains its history, the explanation is always available.
See [[file:javascript/explanation.js][explanation.js]] for a minimal proof of concept.

** Interactive value inspector in s3c                                   :s3c:
Trying to add interactive value inspectors into s3c.

*** Rationale
Instead of plain text, the editor should put an HTML element that represents the
full object, like in Firefox or Chrome consoles.  Each property can be
inspected.

- Why do you need that?  The current behavior of displaying serialized objects
  is good enough for small programs.  At least you have all the properties on
  display at once.  With an “interactive” object, you have to click to view
  further properties...

- The current behavior is nice and simple, true.  But for larger objects, it
  is unwieldy.  Also, an interactive value inspector opens the door for
  interactive “explanations” of values: backtrack through the code that created
  some value in order to understand why it’s a NaN, or 0, or ...

- Do you really need explanations?  I mean, in a full application it could be
  nice (provided a good signal-to-noise ratio), but s3c is for simple JavaScript
  code for beginners.  To find out why a value is NaN, just add more //: to
  track the flow.

*** Implementation
CodeMirror provides two functions: ~addWidget~ and ~addLineWidget~.  ~addWidget~
puts an HTML element on a line with absolute positioning.  So I can create HTML
to inspect an object, and put it after the delimiter.  It does not matter if the
element is larger than the line: with a positive z-index, it will appear as if
floating over the text.

To do that, in ~write~, instead of replacing, I can call:

: editor.addWidget({line: l}, p, false, "above")

The last argument is undocumented, but it puts the element /on/ the given line
rather than below (the default).

However, the element is absolutely positioned.  It does not move when the line
does, which breaks the illusion that it gives a view of the value to the left of
the delimiter.

To sync the widget, I would need to listen on changes on the document, and move
all markers that are potentially affected.  It is not sufficient to listen to
the ~change~ event of a line, as when a line is moved as a side-effect of
inserting a new line above, no change event is fired.

The ~addLineWidget~ is quite different, as it inserts the element below the line
and appears to be inset /in/ the text.  The lines it takes are not numbered, and
are skipped by the cursor.  It behaves correctly when inserting new lines.  Bit
of a space hog currently, as it eats vertical space rather than making use of
the usually empty space at the right of the screen.

Hacking the DOM created by CodeMirror sounds like a bad idea, if only for
forward compatibility.

* [2015-12-02 mer.]
** Free monad for interpreters
Reading up on free monads.  Again.  And discussing them with Ronan.

Beyond [[http://programmers.stackexchange.com/questions/242795/what-is-the-free-monad-interpreter-pattern][this blog post]], [[http://programmers.stackexchange.com/questions/242795/what-is-the-free-monad-interpreter-pattern][this SO answer]] is particularly helpful.

On a related note, even setting up a Free monad can be seen as boilerplate.
[[http://okmij.org/ftp/Computation/free-monad.html][Okmij shows]] how to eliminate the noise.

* [2015-12-09 mer.]
** GameBoy Sound player                                                 :gbs:
The sound component of Boyo is a mess.  It sort-of works, but there are weird
artifacts coming out after a while.  And it’s eating at least 20% CPU.  And it
doesn’t even pass blargg’s tests!

I want to start from a clean state, and understand how the damn thing works.
Maybe writing a player for GBS files would be a more appropriate target?  I’m
curious as to what these files store anyway.  Can’t be samples, or they would
directly be in a sound format.  So they must be instructions directly from the
ROM, but probably only the instructions relevant to the audio?

Found a [[http://ocremix.org/info/GBS_Format_Specification][spec]] for GBS files.  At that point, eww does not seem capable of
downloading a sample GBS from Zophar.

Got some GBS.  They are indeed smaller than the ROM file from which they are
extracted.  Pokemon Red ROM is 376K while the GBS is 48K for instance.

Looking at the source for gbsplay, it seems indeed that playing the files means
emulating the CPU and the audio unit.

Maybe what would be nice is if we could compile the output from a GBS into audio
instructions only.  To get an output similar to what MOD file looks like for
trackers.  GBS to MOD converter.

* [2015-12-11 ven.]
** GameBoy Sound player                                            :gbs:rust:
Will try to go with rust-lang.  Why not make it harder on myself?  At least if I
don’t complete the project, I’ll have learned the basics of a new language.

Someone already did a library for [[https://github.com/emu-rs/spc][reading spc]] in Rust!  This will help.

* [2015-12-12 sam.]
** Learning Rust                                                       :rust:
The proof of concept code I wrote yesterday worked, but some pieces went over my
head.  Today I went over the [[https://doc.rust-lang.org/stable/book/][Rust book]] to RTFM.

Now, I know how I should use result types to avoid deconstructing with match so
much.  And also how to put my utility functions in a module for better
organization in the long term.

* [2015-12-19 sam.]
** Filling instruction is boooring                                      :gbs:
Revamped the instructions macros a bit.  Leaner, and now matching the order of
[[https://code.google.com/p/game-music-emu/source/browse/trunk/gme/Gb_Cpu.cpp?r=40&spec=svn40][Blargg’s emulator]].  Though I don’t really know if there is a performance payoff
for that, since it could be optimized by the compiler as a jump table anyway.

Not sure what I want to do with flags tests after operations.  Seems like lot of
duplicate code.  Unless I use a ~test_flags~ macro...

* [2015-12-20 dim.]
** Overflow are safe in Rust                                       :gbs:rust:
Which means ... that 0xFF + 1 triggers a panic!  But only in debug builds, since
these checks are removed on release builds.  However, the right way to go about
that is to use ~wrapping_add~ instead to /explicitly/ signal overflow is
intended.

* [2015-12-23 mer.]
** Improving s3c                                                        :s3c:
Was looking to improve the error feedback of s3c.  But I realized that I could
fix the O(n^2) complexity of code evaluation.

Since we have only one worker when evaluating the whole file, and since the
worker evaluates all its code in the global context, we don’t need to
re-evaluate the previous blocks.  We can just send each block of code to the
worker by resetting the current code string.

So, evaluation is back O(n) with a one-line change.  D’oh.  And this also fixes
the multiple console.log calls!

But, it also changes the behavior of error output.  Previously, the first error
encountered in the evaluation would propagate as the result to all the following
evaluation markers.

: throw 1 //: 1
: 1 + 1   //: 1

Now, even a syntax error will affect only the next marker.

: throw 1 //: 1
: 1 + 1   //: 2

Is this ... better?  I’m not sure.  On the one hand, errors don’t propagate
anymore.  So you can go on with your code and still get feedback, even if a
previous definition triggers an error.

On the other hand, it’s now easy to miss an error up in the file and continue
working, and then wonder why something doesn’t work down the road.  Syntax
errors are signaled by the linter.  But other errors, like:

: fn f(a) { return a.b(a) }
: f(12) //: TypeError: a.b is not a function

are not.

For beginners, it might be a good idea to make runtime error more noticeable.

Okay, marked the lines in inverted red.  Can’t miss them now.

-----

Also added visual feedback for triggering evaluation.  Just erase the text after
//: at the time of sending the code to the worker is enough to /see/ that the
editor is doing something even when the results are the same.

-----

Made console calls to not trigger any linting error or warning, since they can
be used to step through a block.

-----

Maybe using a forEach on each /block/ rather than line would be faster than the
current way.  Another time.

* [2016-01-06 mer.]
** Decoding opcodes in GBS                                              :gbs:
The decoding opcode part of GBS is a bit redundant:

#+BEGIN_SRC
0x41 => ld!(b, c),
0x42 => ld!(b, d),
0x43 => ld!(b, e),
0x44 => ld!(b, h),
0x45 => ld!(b, l),
0x47 => ld!(b, a),
#+END_SRC

There is a way to factor that by just looking at how the opcode is composed.
For the ‘ld’ instruction, there is a pattern:

: ld r,q = 01rrrqqq
: ld r,n = 00rrr110 nnnnnnnn

With r and q being one of:

| Register | Code |
|----------+------|
| B        |  000 |
| C        |  001 |
| D        |  010 |
| E        |  011 |
| H        |  100 |
| L        |  101 |
| HL       |  110 |
| A        |  111 |

So, we already have the register information from the opcodes.  No need to spell
it out.  But this means additional work at runtime (decoding the opcode), and
decreased legibility of source code.  As of now, the code is very
straightforward, save for the organization of the opcodes.

We could decode the opcode at compile time using a macro, but I’m not sure we
would gain in legibility.

And unfortunately, the pattern breaks down for other opcodes:

: ld A,BC = 0000 1010

At least the tedious way to spell it out is homogeneous.

* [2016-01-07 jeu.]
** Dragging boxes around                                           :visualjs:
For a prototype visualizer of the JS heap.  I need to move boxes, representing
objects, around, and link them with arcs.

Started with a simple div box absolutely positioned and a homebrew drag’n’drop.
Works.

** Cloning SVG in a template tag                                   :visualjs:
But for arcs, I need to switch to SVG.  First suprise: using HTML templates to
clone SVG elements needs namespacing.  So I wrap the elements (like ~rect~) in a
~svg~ tag with explicit namespacing.  Works!

** Slow drag in Firefox                                            :visualjs:
Chrome is perfectly happy using the CSS transform property for dragging the
SVG boxes around.  Firefox is choppy.

[[https://jakearchibald.com/2013/solving-rendering-perf-puzzles/][This post]] is helpful on the subject.  Changing the x and y attribute of the rect
is definitely worse.  Using the transform property of the SVG (rather than CSS
transform) seems okay.  Certainly not as fast as Chrome, but looking at the
numerous bug report on SVG performance on Bugzilla, I’m gonna assume that SVG
animations in Firefox are just slower.

Hmm, closing the DevTools /is/ a definite improvement however.  Good thing to
keep in mind.

* [2016-01-08 ven.]
** Switching to d3                                                 :visualjs:
Managing SVG and interactivity is tedious.  D3 seems a good fit for what I want
to do.  I get browser compatibility, selectors, the join model of handling data,
and even animations.

Drag and drop is built-in, and I might need things like force layouts.

Also, it’s one of the most-used JS library, which means it probably won’t
disappear for at least a few years.

** Heisendrag                                                      :visualjs:
I was curious as to why the drag and drop example of D3 in Firefox was fluid,
while mine was choppy.  Turns out, dragging the browser tab in the other window
fixed the slowness ಠ_ಠ

* [2016-01-12 mar.]
** Mastering D3 and event propagation                              :visualjs:
In order to better understand how event propagation works in the DOM, and to
experiment with D3 animations, I made a simple visualization based on [[http://www.quirksmode.org/js/events_order.html][this
helpful page]], and using [[http://bl.ocks.org/mbostock/3943967][this block]] as a model for chaining transitions, and [[http://bl.ocks.org/mbostock/9631744][this
block]] for the visual language.

* [2016-01-15 ven.]
** Mouseenter event fired only when going to the right in FF       :visualjs:
At least I thought that, maybe it was a bug in Firefox.  The behavior puzzled me
and then I noticed that the SVG rect I was hovering my mouse onto was /not/ the
only element around: the temporary line I drew on top of everything was there
too!

So, #notabug.  Standard PEBKAC.  The line should not be interact with the cursor
in this case, and that is what the CSS property ~pointer-events: none~ is for.

And hey!  As a bonus, it fixed the behavior I was seeing in Chrome: since the
cursor was just above the line, whenever I clicked on it to validate, I was
clicking on the line, which had only the SVG container as a parent, and thus the
SVG registered the click while the node did not.  In Firefox, for some reason,
the cursor always clicked the node below the line.  Maybe the calculations were
off a pixel...

** The self-perpetuating task of explaining code with code         :visualjs:
I want to visualize JS code to better understand it, and be able to explain it.
For that, I build a program.  I write more code, /different/ code, code that the
visualization might not suffice to explain.  The visualization if for heap
objects, but for that I’m writing an automaton, and we already have a good
visual language for those.  But!  If I want this automaton visualization to be
interactive, I again need to write more code.

Either at some point I have visualizations for the first kind of code, and also
for the code of the visualizations, etc.—I converge—or I just throw up my arms
in the air and leave some code unexplained, or self-evident.

Will only know if I try.

** Declarative automaton for linking nodes interactively           :visualjs:
The linking nodes code is /clearly/ an automaton, and /clearly/ is spaghetti
code at the moment.  Dealing with listeners that should only exist on one state
is especially nasty, since we have to register them, then toggle them off, and
this is a repeating pattern that surely could be taken care of by a declarative
automaton.

As it stands, here is the description of the functionality needed to make the
linking:

#+BEGIN_EXAMPLE
Complete (functional description of) automaton

ready --click on circle--> select-dst
       |
       +- create temp line from circle to mouse

select-dst --move mouse-> select-dst
            |
            +- set end point of temp line to mouse position

select-dst --click on a free node-> ready
            |
            +- remove temp line
            +- add link between src and dst to model
            +- add link to view (update view)

select-dst --click elsewhere-> ready
            |
            + remove temp line

Animations and highlights:

ready --enter circle-> ready
       |
       +- grow circle

ready --leave circle-> ready
       |
       +- reduce circle to original size

select-dst --enter node-> select-dst
            |
            + stroke node in green

select-dst --leave node-> select-dst
            |
            + stroke node in default color (black)
#+END_EXAMPLE

I’m pretty sure there is a fluent API there that can take care of the
administrative details of entering a state, and setting up/destroying events
listeners as it goes through a transition.  Anything that need to be done on a
transition can be passed as a function.

Transitions, for my case, are always events happening on some element.  Then 4
things happen, in order:
1. We execute whatever needs to be done when leaving the state (cleaning up
   event listeners)
2. We execute the transition function
3. We change the state, internally
4. We execute whatever needs to be done when entering the new state (setting up
   new listeners)

If the transition is a loop (to and from the state), then only step 2 is needed.

That’s it!  Initially I don’t think any more control is needed for my use case.

Here is how I would sketch the API:

#+BEGIN_SRC js
var link_automaton = automaton()

var ready = link_automaton.state('ready')
      .on('circle.mouseenter', grow)
      .on('circle.mouseleave', shrink)
      .to('select-dst', 'circle.click', create_tmp_link)

link_automaton.init('ready')
#+END_SRC

Need to prototype that to know if it works in practice, and make sure it is
composable (can add states and transitions in multiple steps, not just one
monolithic call).

* [2016-01-19 mar.]
** Declarative automaton API choices                               :visualjs:
Nearly done.  The code is much clearer using the automaton.  For now I’m just
declaring state objects and adding callbacks to their transition/enter/leave
events, and not using a fluent interface at all.

However, the fluent interface can come on top of that, to alleviate two problems
with the lower-level interface:
1. All the states must be declared beforehand.  If A refers to state B (in a
   transition, say), then B must be declared.

   Using a fluent interface, we can just give the name of the state rather than
   a reference to it, and let the interface build the actual state objects for
   us.

2. Adding a callback to a transition is done with ‘on’, but a callback to an
   enter/leave event of a state uses ‘addListener’.  The fluent interface can
   merge the two calls based on the arguments.

There remains a problem with the automaton that I would like solved before
moving forward: how to deal with state that is local to the automaton.  The link
automaton needs to keep a reference to the first element selected, in the ready
state, for use in the select-dst state.

I elected to add an empty ‘data’ object to the automaton.  It’s basically the
same as closing over a variable, but at least it’s namespaced.  And in the
future, maybe I can provide a way to get a ref to the automaton from callback
calls.



An issue I encountered in this version is that I can’t add multiple callbacks to
one transition.  Or even add a callback after creating the transition without
any, at first.

To solve that, transitions should be first class, either through giving them a
name, or returning a new object.

As an added nicety, I think I know how to settle the dilemma of having to choose
whether transition callbacks happen after or before we leave the current state:
let the user choose.  Callbacks can be added either at the ‘debut’ of the
transition (before leaving the old state), or at the ‘end’ (after entering the
new state).  Maybe the ‘middle’ (after leaving the old state, but before
entering the new one) can also be useful.

* [2016-01-22 ven.]
** More design decisions                                           :visualjs:
I’ve pondered whether using the automaton as a pure event emitter.  When
entering a state, when a transition is made (3 stages), just emit custom events
and define the behavior only in the listeners to these custom events.

This is better for decoupling the code.  But the cost is that you lose track of
the control flow.  Some animation bugs are subtle, and require you to know
precisely what happens and in which order.  Animation is part of the
interaction, and the code should not be declared separately.

* [2016-01-29 ven.]
** Event binding troubles                                          :visualjs:
So, I was on the fence about binding listeners to elements themselves, rather
than on the containing SVG, fearing performance issues.  Since boxes can be
added/removed, and we add several listeners to different element of each box,
AND we add/remove listeners depending on the current state of the automaton.

The upside is code that is free of ~if~ statements, since the dispatching is
taken care of by the event dispatcher.

However, it has come to bite me back.  If I define the automaton only once (as I
should have from the start), then when a new box is created, no listeners are
bound to it.  Can’t be dragged.

Of course I could add the drag behavior to newly created boxes.  But, it might
not be correct if we are not in the ready state.  What we should do is add the
listeners for boxes (and sub-components, cell and circle) valid /in the current
state/.  That seems like it’s easy to forget, and it is.  Also, it seems
a bit wasteful, because I would select all boxes again, and reassign the
listeners for all.

Another solution is to catch all events at the container level, let them bubble
up and identify the original target.  But now the problem is that sometimes I
don’t want to just know the original target, but I need the path in the DOM that
the event took.  So now I need to walk up the tree, duplicating the bubbling
phase.

And, ultimately, the drag behavior from d3 need to be called on a selection, not
on the container.

The more pragmatic solution is just to call drag_box when a new box is created.
Since I know the user is in the ready state.  Even though it’s not correct, I
might find a better way to organize this stuff later down the road if need be.

* [2016-03-22 mar.]
** Comparing approaches to deal with state                         :visualjs:
Ronan has been using RxJS for an application that presents a GUI in the
browser.  I was wondering how the reactive programming approach would handle my
situation, for which I found that a state automaton was the best approximation.

But at the same time, it seems odd that I have to resort to an explicit state
automaton to handle my elementary interaction.  So, how do others deal with it?

Looking at RxJS docs, it seems that it is a complete algebra of events, meaning
I could use the basic operators to build richer ones, and eventually create
streams of predicates that would give me exactly the same information that a
state automaton gives.

But, would the complex operator be as clear, or clearer than the description of
an automaton?  And what about the performance of the thing, as this is always a
worrying concern when techniques from functional programming are naively ported
to JavaScript.

I need to find out:
1) the way ‘traditional’ GUI systems deal with this kind of interaction (Swing,
   GTK, Qt, Cocoa?)
2) if there is a ‘canonical’ way to handle this kind of interaction using RxJS
   (or in reactive programming)
3) if there is a standard, or well-known technique to bind listeners to DOM
   elements ‘lazily’, that is, whenever an element matches the given selector, it
   should trigger the listener.

For point 3, if I set up a single listener at the root of the document, I can
capture any click and match the given selector against event.target.  But what
if I want to match against a /parent/ of the target?  Knowing that clicks
bubble, I could walk up the DOM and test the selector against each element,
until I hit the root.

Except now I’m duplicating logic done by the browser, and it’s incompatible with
stuff like ~event.stopPropagation()~.



Okay, on 3, there is an [[https://developer.mozilla.org/en-US/docs/Web/API/element/matches][~Element.matches~]] predicate to know if the element would
have matched the given CSS selector.  Better than having to check the ~tagName~
and ~classList~.  But doesn’t solve the need to look up the parent.

The name of the technique is “event delegation”.  [[https://api.jquery.com/on/][Jquery]] has an argument for
that, but for some reason, it doesn’t work on SVG.  And indeed, it walks the
tree:

#+BEGIN_QUOTE
jQuery bubbles the event from the event target up to the element where the
handler is attached (i.e., innermost to outermost element) and runs the handler
for any elements along that path matching the selector.
#+END_QUOTE

On point 1, there are certainly a number of hits for “GUI state machine”, and
the pattern seems recognized.

* [2016-03-28 Mon]
** Trying out a Sparkets rust server                          :sparkets:rust:
Since server is in need of a rewrite, to be faster, cleaner and more robust.

Since we already compiled Coffeescript, that does not change the compilation
time much.

** Choosing a library                                         :sparkets:rust:
I’ve got a fast and simple [[https://github.com/housleyjk/ws-rs][websocket library]].

Now, I know I will want to benchmark binary messages vs. text messages.  So I
should design around this choice by presenting a common interface.

** Testing input latency                                      :sparkets:rust:
I want to test how the game feels with a moderately high latency (~50 to 100ms
roundtrip).  I thought Chromium was able to do that, but it seems the throttling
option of the network panel only works for initiating the connection, and is not
applied to all subsequent frames when the websocket is established.

But, there is an option to add latency directly on the loopback interface
through [[https://daniel.haxx.se/blog/2010/12/14/add-latency-to-localhost/][netem]]:

: tc qdisc add dev lo root handle 1:0 netem delay 50ms

this sets 50ms of delay.  It does affect ~ping~, and it visibly affects
websocket frames on my machine.

To reset:
: tc qdisc del dev lo root

It seems you need to reset before applying a different delay.

** Multi-threaded server or asynchronous?                     :sparkets:rust:
Building up a small prototype.  Not familiar at all with how to build a game
server in Rust.  And I have to deal with memory management explicitly.

The nodejs server was asynchronous, because nodejs.  One event loop where input
was collected, and one setTimeout to deal with game updates.

In Rust, I guess I could also do that, but I have to look up how.  Meanwhile, I
could also use a multi-threaded approach.  One thread per client might be
simpler to code, and since we are not expecting thousands of players, the
performance scaling of thread is not an issue.

In any case, I need to brush up on coding concurrency in Rust.

Been reading:

- [[http://fabiensanglard.net/quakeSource/quakeSourceNetWork.php][Network code review of Quake]]

  Yes, I know it uses UDP, and WebSocket is on TCP.  But I want to know how
  clients are handled.

  Well, it’s not clear from that article.

** What’s the ideal solution to input latency anyway?         :sparkets:rust:
I’ve always wondered if the treat input/update logic/render loop was optimal.
I’ve been doing that for ages.  I remember it bit me because updates were tied
to graphical frames, and lagging on frames made the game slow.

But this was an issue of handling time in the updates.  If the game updates by
doing ~player.x++~ each frame /and/ you assume the game runs at 60fps, then when
an old machine churns out 30fps, the game plays in slow motion.  Because what
you really wanted to say is ‘x increases by one each 16.66ms’; the simulation is
tied to continuous time.

A game is a simulation.  The simulation, to feel good, needs to be as responsive
as possible.  If I act on the real world, I expect an immediate feedback.  The
simulation, to feel real, must do the same.  It means that a player must be able
to react on input, and see his impact on the simulation in /realtime/.  Of
course, the computer cannot do realtime, only discrete.  But, the computer can
compute the simulation and redraw it much faster than the brain can notice.

25fps is good enough for our brain to believe that movies are real.  But when
you add interaction, you usually need to be a bit faster than that.  25fps means
40ms between two frames.

Let’s say it takes 10 ms to update the simulation, and another 10ms to draw the
scene and refresh the display.  Out of 40ms, the CPU is only busy for 20ms,
which is good.

#+BEGIN_EXAMPLE
   late input                            early input
   |                                     |
--UUUUUUUUUURRRRRRRRRR--------------------UUUUUUUUUURRRRRRRRRR----------------
  |  compute s        |                   |  compute s+1      |
 screen shows s-1     |  screen shows s                       | screen shows s+1
#+END_EXAMPLE

Already something is troubling.  The simulation should render things as they are
/right now/.  But as it /takes time/ doing so, the display is already outdated
as it is shown on the screen!

It’s like when I give you the time, by the time you hear it and process it, it’s
already false.  Now, luckily, the time is still useful to you because I only go
to the minutes.  Seconds are trickier.  Milliseconds are already hopeless.

Same thing for the simulation.  It’s in some state ‘s’, then at the scheduled
time (every 40ms), it starts updating to state s+1.  When the screen is
refreshed, we are already 20ms in.  What time does the simulation reflect?

If it reflects the time of the world at the /beginning/ of the update, then the
image on the screen is already 20ms outdated when it comes up.

That means that if a user action is made just before the update comes along, we
will see the result 20ms at the earliest.  Worst case, the input is made just
after the update component reads them, then we have to wait for the current
frame to draw, then the next: 60ms before our action impacts the world we see.

So, for any random button press, the screen might display the changed world
after a delay that is anywhere between 20ms and 60ms.

If that delay is long enough for the brain to have time to think “did I press
that button?”, for the brain to /notice/, then the simulation is not fluid, and
the illusion breaks.

The question is then, how long can this delay be before the brain starts to
notice?



Running some tests...

Typing a key (down key event) paints a square on the screen.  The square
alternate between pink and green colors to distinguish each key stroke.

Delays are chosen randomly, I just type to see if it feels responsive.
Delays are just lower bound on the actual perceived delay: the screen might take
some milliseconds longer to refresh.

I’ve noticed that typing just one key is vastly different than stringing a few
keys together.  If I type once, and wait to see if I notice the delay before the
square is painted, 100ms feels immediate.  But string 3 keys rapidly, and it
does not feel instant anymore.

400ms is definitely noticeable, and feels sluggish at all.

A delay of 200ms is noticeable, but can still feel responsive for one key.  Not
for 3 keys.

100ms feels immediate.  But I can feel the delay when stringing keys.

50ms feels immediate.  Stringing keys also.

10ms feels a bit faster than 50ms, but not really much.



Another test, on input speed this time.  Measuring time between key downs.

Double-tapping the same key: I can hit 87ms minimum reliably, but with effort.
Effortless is more 150ms.

Stringing two different keys: now there is an issue with measurement.  Tapping
two or more keys /at the same time/, I can never get below 8ms.

Since each key down is a separate call to the listener, I suspect that the time
is spent dispatching and cleaning up.  So, 8ms is the effective resolution of
the browser in this setup.  Sometimes I get a 3, or even 0.5, but quite
randomly.

Now, stringing two different keys: I can do 8ms (same time for the browser) and
16ms reliably (the earliest to distinguish between two key down), without
effort.

With 3 fingers, I can do <100ms for each successive tap, effortlessly.



What does this mean?  Well, if I am able to hit two keys with 20ms between them,
I can also hit them with a 60ms interval.  If I can feel the difference in
my fingers, the game should also reflect this difference.

But, if I sample the input every 40ms (by polling the keys at the beginning of
the update loop), keys hit with an interval <40ms are counted as being hit at
the same time.

It’s basic sampling.  The signal is 1 when the key is down, and 0 when the key
is up.

#+BEGIN_EXAMPLE
----------|----------|----------|----------
0000000111110000000000011111000000011110000
#+END_EXAMPLE

As long as the key is held down for longer than the polling interval, we are
sure to get every key.

And if we want to distinguish between two successive key pressed, we just have
to use a reasonably low polling.

On my browser, the lightest tap I can muster holds the key for 32ms.  Meaning
that if the polling was 40ms, I could miss that key down from time to time,
depending on how it falls with respect to the update.

In this case, 30ms would suffice.  Poll interval of 30ms, or you start losing
keys.



So I guess the morale of the story is: faster feedback is always better.  But
below 50ms of visual feedback, the gains are negligible.

Polling keys at the start of a monolithic update loop is okay, as long as the
polling interval is less than the time a key can be held down.  Should check on
target hardware how low the resolution can be (browser + keyboard is certainly
not the optimal setup).

** Carmack on movement prediction                             :sparkets:rust:
To alleviate server latency in QuakeWorld, Carmack tried to use prediction.  The
player movement is duplicated on the client, starting from the last known good
state received from the server.

The server works by directly answering to received packets: update only the
world around the player, and send the state back.  There is no global time
anymore.  But the player does not have to wait for the fixed update.

Carmack notes that simulating 300ms of player movement on the client is
hopeless.  But, for <100ms delays, client prediction helps smooth out the
movements.  Because server updates may not always arrive on time, we can keep
the framerate constant on the client with prediction.

* [2016-04-02 Sat]
** Setting the MTU on Archlinux                                        :arch:
I had issues connecting to wiki.archlinux.org, but other websites were fine.

Apparently, that was caused by a misconfigured MTU.  Under Windows, the MTU was
1480 for ipv6, and 1500 for ipv4, but in Linux it was 1500.

To find out the correct MTU, I used ping:

: $ ping -4 -l 1452 -M do www.dslreports.com

‘-M do’ tells ping to look for MTU discovery packets.  The host has to be
configured to send these packets back, which few of those I tested (8.8.8.8,
google.com, free.fr) did.

Setting the MTU temporarily:

: # ip link set eth0 mtu 1480

(replace ‘eth0’ by interface name)

Then wiki.archlinux.org loaded correctly.

To set the MTU permanently, the wiki advised to use an udev rule, but I could
not get it to match the interface name for some reason.  Too lazy to RTFM, turns
out there is an MTUBytes option for systemd-networkd.service.  In
/etc/systemd/network/my.network:

: [Link]
: MTUBytes=1480

Voilà.

** Mounting a WDTV Live Hub                                            :arch:
Did not want to install/configure Samba.

But luckily, only ~cifs-utils~ is required:

: # mount -t cifs //SERVER_IP/WDTVLiveHub/ /mnt/wdtv -o uid=USER,gid=USER

To find what shares are up on the network:

: $ smbclient -L //SERVER_NAME

To find the IP of the server:

: $ nmblookup SERVER_NAME

* [2016-04-03 Sun]
** Making progress                                            :sparkets:rust:
Spent a few hours trying to find a way to emulate a setInterval on the server.
Well, good old thread::sleep is still the state of the art, apparently.  It was
used by a [[https://github.com/mvdnes/rboy/blob/master/src/main.rs][gameboy emulator]], and measurements show it as accurate enough.

I ought to make a [[http://gafferongames.com/game-physics/fix-your-timestep/][“right” timestep]] this time around though.

And I’m sure I’ll run into all kind of ownership fun when I start accessing the
game state from the logic thread as well as from the websocket handler.

One thing I haven’t settled, is whether to send game updates to clients when we
receive a message, or broadcast in the logic thread.  I recall reading Carmack
switching to the former for Quake3.  Cuts time between updates for the client,
but every client will have a slightly different state (although the interval are
so small, it should not be noticeable).

Serialization was another issue.  I found a library, [[http://tyoverby.com/bincode/bincode/][bincode]], so I don’t have to
write a struct to [u8] function.  But on the JS side, I still need to write a
deserializer.  So I might end up writing the serializer by hand, to have more
control over endianness.  And for diffing snapshots to send updates.

And while I’m experimenting, maybe find a way to use the unreliable WebRTC data
channel, rather than websocket.  Should be quite faster especially out of the
LAN.

- http://www.html5rocks.com/en/tutorials/webrtc/datachannels/
- https://hacks.mozilla.org/2013/03/webrtc-data-channels-for-great-multiplayer/

But on the Rust side, it’s rather bleak:

- https://github.com/phsym/sctp-sys

** SCTP experiments                                           :sparkets:rust:
Tried to use the rust-sctp library.  For some reason it always returns an error
when I try to accept a connection.

Tried to bind the socket in C.  It gets past the accept and blocks.

So, I guess if I can chat with a JS web page over RTCDataChannel, it might be
worth to try to see how to integrate the C code into Rust?

* [2016-04-09 Sat]
** Understanding WebRTC                                       :sparkets:rust:
Found a pretty [[http://chimera.labs.oreilly.com/books/1230000000545/index.html][comprehensive book]] on WebRTC and browser networking.

Managed to build a minimal example of a client page using WebRTC to setup an
unreliable data channel to itself.

Now, the sad part of that is that setting up a WebRTC connection is /much more/
than just creating a socket.  You need an SDP, to setup ICE candidates, and then
let the browser establish the SCTP connection over DTLS over UDP under the hood.

I only found an SCTP library for Rust for now, so I’m missing a few components
to make a Rust binary talk to a WebRTC JS client.

Nodejs can talk to a WebRTC browser, right?  The goto library on npm seems to be
[[https://github.com/feross/simple-peer][simple-peer]].  To use it in node, they point to [[https://www.npmjs.com/package/wrtc][wrtc]].  Seems /they/ mostly wrap
around the WRTC implementation of Chromium, and export that to node bindings.

So using that with Rust seems... not fun, at all.

On the other hand, I /could/ use simple-peer and wrtc in Sparkets directly, and
have an UDP protocol for messages.  Less work, more benefits.

* [2016-04-19 Tue]
** One fat listener                                                :visualjs:
I like simple approaches.  Watching [[http://mollyrocket.com/861][Immediate-mode GUIs]], I want to try writing
a catch-all listener that will handle all the logic in one place.

I suspect that he had in mind to repaint the components in the single update
function.  I don’t need to do that here, as I deal with SVG elements inserted
into the DOM.

The single update function works.  But it made me realize I really ought to
decompose the ‘box’ functionality into independent behaviors, or traits:
- a movable behavior that adds a moveTo command for manual positioning
- a draggable behavior for mouse dragging
- the box is just a container, doesn’t need to know what’s inside to draw itself
- a snappable behavior for snapping to a grid

Properties are distinct components also.  And links too.

* [2016-04-29 ven.]
** Diving into V8 optimizations
Trying to find out if, in a simple ~for~ loop:

#+BEGIN_SRC js
var a = []
for (var i=0; i < a.length; ++i) {}
#+END_SRC

the ~i < a.length~ check is optimized as:

#+BEGIN_SRC js
var a = []
for (var i=0, l=a.length; i < l; ++i) {}
#+END_SRC

or not.

Via nodejs, we can pass a bunch of flags to V8 in order to obtain more
information about the optimization, GC calls, intermediate representations, and
generated code.

After putting the loop in a function that's called 10000 times, the function is
/hot/ and will be compiled and optimized.  We can see that with the --trace-opt
option.

#+BEGIN_EXAMPLE
[compiling method 0xad4d20bd9c1 <JS Function f (SharedFunctionInfo 0x187ace9573f1)> using Crankshaft OSR]
[optimizing 0xad4d20bd9c1 <JS Function f (SharedFunctionInfo 0x187ace9573f1)> - took 0.061, 0.151, 0.038 ms]
#+END_EXAMPLE

To find out the generated code, we can use --print-opt-code:

#+BEGIN_EXAMPLE
--- Optimized code ---
optimization_id = 1
source_position = 72
kind = OPTIMIZED_FUNCTION
name = f
stack_slots = 10
compiler = crankshaft
Instructions (size = 696)
0x2335adc58220     0  55             push rbp
0x2335adc58221     1  4889e5         REX.W movq rbp,rsp
0x2335adc58224     4  56             push rsi
0x2335adc58225     5  57             push rdi
0x2335adc58226     6  4883ec30       REX.W subq rsp,0x30
0x2335adc5822a    10  488b45f8       REX.W movq rax,[rbp-0x8]
0x2335adc5822e    14  488945d8       REX.W movq [rbp-0x28],rax
0x2335adc58232    18  488bf0         REX.W movq rsi,rax
...
#+END_EXAMPLE

Now, unfortunately, that's a bit low level.

I tried to generate the same code for the hand-optimized for loop, and diff the
outputs.  But there many random addresses that gets in the way of seeing if
instructions differ.  One thing that's easy to spot though is the Instructions
(size) line.

My guess is it's the size of the compiled function.  But the hand-optimized
version has size=812, which seems counter-intuitive.

Or maybe, the hand-optimized version actually /defeats/ optimization made on the
more common idiom by the compiler.

We can get a look at some of the optimization phases made on the high-level
representation (HIR) through the --trace-hydrogen flag.  My guess is Hydrogen is
responsible for high-level representation.

The file contains multiple control flow graph, with the helpful name of the pass
that generates it.

When ~f~ is optimized, it triggers a full compilation phase.  The graph is full
of "blocks" of code:

#+BEGIN_EXAMPLE
                               +----------+
                               v          |
B0 -> B1 -> B2 -> B4 -> B5 -> B6 -> B7 -> B8
             |          ^        -> B9 -> B10 (return)
             +--> B3 ---+
#+END_EXAMPLE

Clearly, the loop is B6 -> B7 -> B8, and B9 is the exit path.

If we look at B6, we can see our length check:

#+BEGIN_EXAMPLE
      0 0 v48 BlockEntry  type:Tagged <|@
      0 0 t52 CheckHeapObject t39 <|@
      0 1 t53 CheckMaps t39 [0x2a5dde306c51] <|@
      0 1 i54 LoadNamedField t39.%length@24 t53 type:Smi <|@
      0 0 i55 CompareNumericAndBranch LT i44 i54 goto (B7, B9) type:Tagged <|@
#+END_EXAMPLE

So, at this point, we are checking the ~length~ field of the array.

But, after the "H_Global value numbering" phase, all that's left of this block
is just the comparison:

#+BEGIN_EXAMPLE
      0 0 v48 BlockEntry  type:Tagged <|@
      0 0 i55 CompareNumericAndBranch LT i44 i54 goto (B7, B9) type:Tagged <|@
#+END_EXAMPLE

i54, the integer that holds the length value, has moved to block B5, which is
not part of the loop:

#+BEGIN_EXAMPLE
      0 0 v45 BlockEntry  type:Tagged <|@
      0 0 v46 Simulate id=30 type:Tagged <|@
      0 0 t52 CheckHeapObject t39 <|@
      0 3 t53 CheckMaps t39 [0x2a5dde306c51] <|@
      0 2 i54 LoadNamedField t39.%length@24 t53 type:Smi <|@
      0 2 t70 Constant 0x2ffe54fafc79 <JS Array[0]> [map 0x2a5dde306b49]  <|@
      0 0 t71 CheckMaps t70 [0x2a5dde306b49](stability-check) <|@
      0 2 t72 Constant 0x2ffe54facc81 <an Object with map 0x2a5dde306519> [map 0x2a5dde306519]  <|@
      0 0 t73 CheckMaps t72 [0x2a5dde306519](stability-check) <|@
      0 4 t74 LoadNamedField t53.%elements@16 type:Tagged <|@
      0 0 t75 CheckMaps t74 [0x2a5dde304209] <|@
      0 0 v47 Goto B6 type:Tagged <|@
#+END_EXAMPLE

So, it seems that the length check is indeed optimized by V8.  And that is done
in the "Global value numbering" phase on the HIR.

* [2016-05-02 lun.]
** Someone who actually knows V8 optimizations
already [[http://mrale.ph/blog/2014/12/24/array-length-caching.html][covered]] the ~array.length~ case in depth.

He also built a [[http://mrale.ph/irhydra/2/][tool]] to visualize V8 HIR, contron flow graph, and
deoptimizations output.  Much better than recreating the graph by hand.

He mentions that manually caching the ~array.length~ may actually be worse,
because it creates an additional variable that is assigned to a register.

The morale here is, again, to measure before optimizing.

The compiler does a good job a optimizing common idioms.  And it actually
produces less-efficient code if you are trying to optimize things yourself.

This was [[http://www.infoq.com/presentations/chrome-v8-optimization][reiterated]] by V8 engineer Ben Titzer for heap optimizations.  Someone
asked if using an object pool is a good idea when you have allocations
problems.  The answer: probably not, because V8 /assumes/ a usage pattern of
creating objects and throwing them away.  An object pool is an uncommon pattern,
and it might defeat optimizations.

Measure first, understand how the runtime works, formulate a strategy, implement
and measure again.

* [2016-05-06 ven.]
** Testing the GB CPU emulator                                          :gbs:
The [[http://blargg.8bitalley.com/parodius/gb-tests/][Blargg test suite]] is a good start.  But there is a slight bootstrapping
issue, as it needs a mostly-working CPU to actually start running the tests.

And the output requires minimal screen emulation, which I don't really wanted to
cover.

And the GB rom files are not the same format as GBS files... again, I don't want
to parse those.  On that front, since the assembly source is provided, I can
actually recompile the tests for GBS.

In shell.inc, you find:

#+BEGIN_EXAMPLE
; GBS music file
.ifdef BUILD_GBS
     .include "build_gbs.s"
.endif
#+END_EXAMPLE

The readme mentions that 'wla-dx' was used to compile and link those assembly
files.  The project is [[https://github.com/vhelin/wla-dx][still alive]], and also in [[https://aur.archlinux.org/packages/wla_dx/][AUR]] (gotta love Arch).

To compile a GBS file from an individual test file, you just need to define
~BUILD_GBS~ like so:

: wla-gb -o -DBUILD_GBS FILE test.o
: wlalink linkfile test.gbs

Two issues for the moment with that ROM.  The play address of the header is
0xC6D5, which is outside the 0x400--0x7FFF range of the GBS spec...  and if I
remove the checks there is an infinite loop (maybe because I haven't implemented
all flags for instructions yet).

Maybe a basic test harness in Rust is a better idea.

** Testing single instructions                                          :gbs:
Created a ~step~ function that goes through one instruction and returns the
number of cycles.  More useful for unit testing than ~run~.

Using macros for testing, since I have lot of repetitive code for each register.
But now, running into a strange SIGSEGV error when I have too many macro
calls... strange.

#+BEGIN_EXAMPLE
error: Process didn't exit successfully: `gbs-4725f7ba8db983e2`
(signal: 11, SIGSEGV: invalid memory reference)
#+END_EXAMPLE

Trying to debug by finding out what is generated after macro expansion.  Need an
(undocumented, of course!) option:

: rustc --test --pretty=expanded -Z unstable-options src/cpu.rs

~--test~ means compile the test module, I suppose.  And ~--pretty~ is the option
to output pretty printed code after macro expansion.

Ok, I have code like this:

#+BEGIN_SRC rust
#[test]
fn test() {
  ld!(b, c);
  ld!(b, d);
  ...
}
#+END_SRC

and the macro creates a new ~Cpu~ each time:

#+BEGIN_SRC rust
macro_rules ld! {
  ($r:ident, $r2:ident) => ({
    let mut cpu = Cpu::new();
    ...
    assert!(..)
  });
}
#+END_SRC

In the generated code, ~test~ contains as many blocks as there are ~ld!~ macro
calls.  I suppose that the code generator doesn't like code that has too many
blocks... Maybe I should split those into functions?

Ok, changed the tests to generate one function for each test case.  Only
slightly more verbose, but greatly increases my number of tests!

** Wait, was that a compiler bug?                                  :gbs:rust:
The SIGSEGV with too many macros... no unsafe code, but still an invalid memory
reference?  How come?

Building a minimal example now.

: rustc --test main.rs; and ./main
: fish: “and ./main” terminated by signal SIGSEGV (Address boundary error)

Ok, just a single test function that calls 32 ~Cpu::new~ does it, but 31 calls
does not SIGSEGV.  I emptied the ~Cpu~ struct to contain only the ~ram~ field,
which has 65536 u8, hence each Cpu eats 64K.

Let's see, 32*64K = 2048K = 2M.

That's a suspiciously round number.  <2M, no SIGSEGV, >=2M, SIGSEGV.

According to [[https://play.rust-lang.org/][play]], happens on stable, beta and nightly.  But only in debug mode
(release optimizes everything away probably).

Aaaand there we have it: [[https://github.com/rust-lang/rust/issues/31748][#31748]].  Rust has a default stack size of 2M, so we
overflow that.  But there should be a stack overflow message that's skipped for
some reasons, and the devs are aware of it.

* [2016-05-07 sam.]
** Fixing flycheck-rust                                            :flycheck:
flycheck-rust is confused when you have both a lib.rs and a main.rs in the same
folder.  Because cargo needs to know what target to build: the lib, or the
binary?

flycheck-rust does not specify the target, and spouts an error, and fails to
check the buffer (and any buffer in the project).  This has been [[https://github.com/flycheck/flycheck-rust/issues/23][reported]], but
not yet fixed.

Now, we can get the all targets from cargo itself, thanks to the ~read-manifest~
command:

: cargo read-manifest

returns a JSON with all targets.

Now, which one to chose?  I suppose the 'lib' target will start with the
'lib.rs' file, and compile all files that are included in it, recursively.  And
the 'main' target is the same, but starting from the 'main.rs' file.

Flycheck works per-buffer, so we should chose the target that will end up
compiling the current file.  Ideally we would compile only the current file, but
in larger projects, there are dependencies to keep track of.

So, which target to chose?  I don't think there is a way to get that information
directly from cargo right now, that is:

: cargo which-target src/a.rs

which would return the target name.

In my use case, the project is a library, that also contains a binary as an
example.  So, we should always build the 'lib' target (there's only one of
those), and build the 'bin' target only when looking at the 'main.rs' file.

If the current buffer is a match for the src file of any target, then chose the
according target.  Otherwise, chose 'lib' by default.

That seems to work locally.  Now, onto the PR!

** Making the pull request                                         :flycheck:
Forked flycheck.

Made the changes.  Tried to run the tests... fail!  Ah.

: make specs test

fails because it asks me for passphrase during the tests.  What?

Looking around the source, the passphrase is "spam with eggs".  Now it passes:
: Ran 71 out of 105 specs, 0 failed, in 10.0 seconds.

Some tests are canceled because they need Emacs 25.

Apply back my changes, there is a documentation failure.

I ~ag~ for the option above mine, to look where it appears in the source.  There
is a documentation entry in 'languages.rst'.  I document the new variable, test
passes.

Now, onto the integration tests:

: make LANGUAGE=rust integ

Okay, two tests fail: warning and multiline-error.  Actually, the second failed
without my patch.  Probably a change in the compiler output.  Fixed the test.

The first fails because there is no value for my new variable.  The test project
is a crate named "flycheck".  Put that, all tests pass.

Done.  Now, flycheck-rust!

** Finding the right build target                                  :flycheck:
Had to change the approach a little, because we cannot default to "lib" crate
type in a crate that contains only a 'main.rs'.  So instead of guessing, I just
look the targets up in ~cargo read-manifest~.  First one is the default, and if
we are looking at a file that is specified by the targets, this is the target we
pick.

Simple cases: only one target (lib or bin), that is the one chosen.  Works with
"simple" setups.

Multiple targets: lib, main.rs bin and multiples source files in src/bin.  If
looking at 'main.rs', or any of the 'src/bin' files, those are targets, so they
are chosen.  Any other file will default to the first target.

It's not ideal.  I think it might miss cases like:

: src/a.rs src/b.rs src/lib.rs src/main.rs
: src/lib.rs depends on 'a.rs'
: src/main.rs depends on 'b.rs'

If the default target is 'lib', then Looking at 'b.rs' will pick lib, even
though it's a dependency for the binary.  Converse is true for 'b' and a default
target of 'bin'.

Haven't encountered the issue, because I only have the case where 'main.rs'
depends solely on the lib, and every other file is part of the lib, and the lib
is the default target.

Anyway, unless there is a way to find the target for a file, this will do.  This
can always be overridden by setting the `flycheck-rust-binary-name` manually.

Reviewed the code and added a docstring.  No test suite this time (though it
would not be a bad idea to ensure we don't break any convoluted setups).

* [2016-05-10 mar.]
** Checking the state of Rust tool support                         :flycheck:
Error output seems to have changed in nightly: [[https://github.com/rust-lang/rust/pull/32756][PR#32756]].

That means Flycheck will soon break in parsing them.  Luckily, there is also a
new [[https://internals.rust-lang.org/t/rustcs-json-output-format/3446][unstable option for JSON output]].  The JSON format should hopefully stabilize
soon.

Speaking of which, using ~-Z no-trans~ for faster compilation is an unstable
flag, and currently outputs a warning.  [[https://github.com/rust-lang/rust/issues/31847][This]] is the issue to follow if we want
this flag to stabilize.

On the horizon, there is also the [[https://github.com/rust-lang/rust/issues/31548][Rust Language Server]], which aims to be a
direct interface for IDEs, providing error checking, completion candidates, find
definition, etc.  But this is only a RFC, awaiting for incremental compilation
progress in rustc.

A good place to check for news on all of this is the [[https://internals.rust-lang.org/c/tools-and-infrastructure][tools and infrastructure]]
forum.

* [2016-05-11 mer.]
** Checking that flycheck-rust works right for everyone's use case :flycheck:
I've tested the basic layouts of src/lib, src/main and src/bin/.  But cargo
allows for some fancy overrides, and I don't even have dependencies in my
projects for now.

[[https://github.com/flycheck/flycheck-rust/issues/7][I see]] that the cargo project itself is a corner case, and indeed it doesn't work
as intended when looking at the src/bin/cargo.rs file.

The cargo.toml of cargo sets the library path directly rather than relying on
the project layout:

: [lib]
: name = "cargo"
: path = "src/cargo/lib.rs"

Note that the path is relative.  And it still is in ~cargo read-manifest~:

#+BEGIN_EXAMPLE
  {
    "kind": [
      "lib"
    ],
    "name": "cargo",
    "src_path": "src/cargo/lib.rs"
  },
#+END_EXAMPLE

But it's an absolute path when ~path~ is not set in the TOML.  Which isn't
really helping as a machine-readable output.  The issue was raised in the [[https://github.com/rust-lang/cargo/pull/1434#issuecomment-94117884][original]]
[[https://github.com/rust-lang/cargo/pull/2196#issuecomment-171411921][pull requests]], but not picked upon.

Solution?  I guess either ensure that the ~src_path~ is always relative to the
crate root, or always absolute.  Leaning towards the latter, as it should be
easier to debug.

However, even if it does check the correct file, it takes several seconds for a
project as large as cargo.  Not sure if that's a good use case of flycheck.

*** metadata replaces read-manifest
In the future, it [[https://github.com/rust-lang/cargo/issues/2356][looks like]] ~read-manifest~ might be replaced by ~metadata~,
which gives much more information, especially on the dependencies.  For the
moment though, the targets section looks identical.

On surprising effect of the ~cargo metadata~ command is that it fetches
dependencies on first invocation before returning the JSON.  Which means that
the first invocation is slow, and the stdout is not a correct JSON, since you
have lines like:

: Updating registry

Though that can be skipped with the ~--no-deps~ flag.

~jq~ can be useful to wade through the metadata dump:

: cargo metadata | jq '.packages | .[] | select(.name == "cargo")'

*** subcrates
A use case of subcrates is the [[https://github.com/rust-lang-nursery/regex][regex crate]], which has regexp-syntax has a
"subcrate": a dependency crate hosted inside the same repository.

In this case, ~cargo read-manifest~ will report the targets for the current
crate.  So if we are in the main crate, or in the subcrate, it picks the right
target.

*** cargo declares mod at compile time
Using macros, which means that files that are part of the binary target are not
picked up by flycheck.

But even without macros, I don't think we would pick it up:

~src/bin/read-manifest.rs~ is a ~pub mod~ (via macro) in ~src/bin/cargo.rs~.
But there's no target corresponding to read-manifest, so how do we know that's
part of the ~cargo~ binary target?

* [2016-05-20 ven.]
** Toying with JITs                                                 :chipers:
Always wondered how you build one.  Another pretext to use more Rust.

Found a [[http://www.hydrocodedesign.com/2014/01/17/jit-just-in-time-compiler-rust/][couple]] [[http://www.jonathanturner.org/2015/12/building-a-simple-jit-in-rust.html][tutos]].  They showed how to create a memory region, mark it as
executable, write a few opcodes, and the magic ingredient: cast the memory
region as a function.  Then, invoke the function, and boum.

Technically, that's just injecting binary code at runtime.  A kind of "metal
eval"... meteval?  meval?

Anyway.

I wanted to know the order of magnitude difference between JITed code and
emulated code.

I wanted to JIT the GB emu.  But that's not done yet.  So, I thought about a
Chip8 emu.  But I didn't have that.  I do have a JS Chip8 emu.

If I code a Chip8 pure interpreter in Rust, then code a JIT interpreter in Rust,
I could compare the performance of each, and see how much a JIT would gain.

I'm also curious as to whether I can compile most of the ROM code directly to
native binary, without inspecting "hot loops" first.  So, technically, AOT.

Started converting that Chip8 emu by following the JS code and looking up how to
deal with slices, or build up an SDL screen as I went.

Works, although there is a strange display bug at the moment.  But didn't have
time for the JIT version tonight.

So I thought, if I want to compare JIT performance to pure interp, and I already
have a JIT for a fixed piece of x86 binary, why not quickly whip up a hackish
x86 pure interp, and see how /that/ fare?

My test program is a loop that counts down from 0xFF000000.  This takes 1.24
seconds to execute JITed.

The pure interpreter is hackish, but does minimal work on top of decoding and
executing opcodes.  It takes 120.96 seconds in debug mode, and 23 seconds in
release.

So, this preliminary test shows a 20 times improvement in performance for the
JITed version.  Quite impressive.

That's enough to entice me to try that on a real emulator!

* [2016-05-25 mer.]
** Gameboy JIT opportunities
Making a note here of the fact that, due to hardware quirks, the following
snippet is the recommended way to access the state of all the buttons in the
Gameboy:

#+BEGIN_SRC asm
LD A,$20       ; bit 5 = $20
LD ($FF00),A   ; select P14 by setting it low
LD A,($FF00)
LD A,($FF00)   ; wait a few cycles
CPL            ; complement A
AND $0F        ; get only first 4 bits
SWAP A         ; swap it
LD B,A         ; store A in B
LD A,$10
LD ($FF00),A   ; select P15 by setting it low
LD A,($FF00)
LD A,($FF00)
LD A,($FF00)
LD A,($FF00)
LD A,($FF00)
LD A,($FF00)   ; Wait a few MORE cycles
CPL            ; complement (invert)
AND $0F        ; get first 4 bits
OR B           ; put A and B together
#+END_SRC

Cycles are wasted with repeated instructions (/debouncing/), because the polling
is not instantaneous.

In an emulator, we don't have that hardware quirk.  So we could coalesce all
these ~LD A~ into one (but still add the cycles of all the ~LD~ calls).

In fact, if this whole sequence is frequent in ROMs, we could just emit binary
that constructs the full byte of button states directly.

Another hint of optimizations is to look for redundant operations, like the ~LD~
above, and systematically coalesce them into one.  These optimizations would be
useful for any piece of code, not just this snippet.

* [2016-06-07 mar.]
** The fastest Chip8 emulator                                       :chipers:
So, I ported my Chip8 emu to Rust.  To have a smaller code base to test a JIT
with.

I have two ways to recompile a rom. It might be possible to compile the rom when
loading it (AOT): just create a function that does as much as possible in native
code, and jumps back to Rust code for things I don't know how to code in
assembly (e.g., drawing).

I don't yet know how I would jump back to a Rust function.  Is calling the
pointer address enough?

Otherwise, I can watch the code for hot loops, and try to compile those.  So I
need to visualize hot paths, in order to understand what patterns I need to
match.  Which brings me to the second point.

** GUIs in Rust                                                     :chipers:
Been looking for a nice and minimalist way to view the rom disassembly that
updates in real time as the interpreter goes through each opcode.

There's nothing provided by SDL.  Nor OpenGL.  Even writing text in those is a
PITA, and I don't want to be writing code to align two lines of text, to detect
mouse clicks, etc.

There are Rust bindings for GTK, but that does not strike me as friendly nor
minimalist.  And I'm not sure about the portability.

Luckily, I found ImGui which seems to fit the bill.  It renders to vertex
buffers, which can be plugged into an OpenGL renderer, so it's as portable as
OpenGL.  It's certainly minimalist, but it's good enough to have been used in
games and emulators for... debuggers and disassemblers!

Now, the only trouble is: the Rust bindings are light on the documentation
(read: there is none).  The only code example uses Glium as a renderer.  But I
already have an SDL window.  I could launch two windows: one with a SDL backend,
and one with Glutin (the backend of Glium).  But do I have to use threads?  That
could degenerate quickly, and seems opposed to the way ImGui is supposed to be
used.

Maybe I can just keep one loop that polls SDL, draws the frame, then does the
same for the Glium window.

Otherwise, I could switch my SDL rendering to Glium, or any other GL binding,
replace the drawing code with OpenGL calls, then draw the ImGui on top of that.

[later]

Tried only one loop to handle the two windows: one SDL, one Glium.  The ImGui in
Glium works fine, but the SDL windows does not update anymore.  Console is full
of debug errors caught by /Glium/, but the backtrace indicates that the error
originate in SDL2 calls.  Craziest thing.

I can only guess that SDL2 uses a GL context under the hood for accelerated 2D
rendering and, /somehow/, Glium takes hold of that GL context, and that,
/somehow/, they do not like sharing.

The errors caught by Glium are things like "~glVertex2f~ or ~glEnd~ is
deprecated".  Maybe SDL2 uses the old OpenGL API, whereas Glium is only
compatible with 3.0+?  Who knows.

In any case, that means doing the right thing: sticking to OpenGL for drawing
the emu AND ImGui.

* [2016-06-20 lun.]
** Switching chipers to OpenGL                                      :chipers:
Went full glium/glutin.  Glium is the library for high-level OpenGL bindings.
As I understand, it takes care of allocating GL objects and disposing them for
you.  It also help avoid the unsafeness of the GL API.

Glutin deals with the display manager of your OS to give you keyboard and mouse
events, to create a window, etc.  SDL handled both.  As I understand, Glium is
not tied to Glutin, but both are from the same author, so...

Anyway, using Glium/Glutin is not the hard part.  The hard part is understanding
how to draw things in OpenGL, especially with shaders.

** Drawing colored squares with triangles                           :chipers:
With SDL I was just drawing a "point" for each pixel of the Chip8 screen (cixel
henceforth).  And since I only knew how to draw triangles in OpenGL, I thought:
"Hey, let's draw a quad for each cixel!"

And that was a few hours, just to get something on the screen.  Because I had to
allocate a vertex buffer and modify it each frame, figure out how to pass my
vertices to this VBO, how to setup shaders just to get something, how to use a
projection matrix in the vertex buffer so that cixel coordinates would translate
to screen coordinates...

After a copious amount of ddging ([[https://tomaka.github.io/glium/book/tuto-01-getting-started.html][helpful tutorial]] from Glium dev
notwithstanding), I managed to get a Chip8 screen back.  Albeit clipping when
resizing.  And ... with horrible FPS performance after a few seconds.  What?

** Switching to drawing on a texture                                :chipers:
I figured that, since I didn't know what I was doing in OpenGL, I must have done
something wrong there.

The SDL version was smooth in frame time (constant 16.666ms).  Since I hadn't
touched that in the conversion, my GL-fu was to blame.

Maybe I was allocating a new VBO needlessly every frame?  Surely that would cost
me.  I don't know how Glium is implemented, but that looked like a potential
inefficiency right there.

So I started to question my rendering solution.  I knew that drawing triangles
was not the only way to draw the Chip8 screen in OpenGL.  It was the only way I
knew /how/.  But what solution did other choose?

Turns out, there are at least 30 chip8 interpreters written in Rust on Github.
And a dozen that use glium for rendering.  As far as I can tell, /every one of
them/ elected to draw the screen to a 2D texture.

The texture is then drawn to a single quad that spans the entire output screen.
No VBO allocation after initialization.  Not even a new texture allocation.

That... seemed alright.  And maybe even simpler that my approach, considering.

Some re-create a new texture for the quad each frame.  I read somewhere on the
Glium API that rewriting the texture contents can cause a CPU/GPU
synchronization, which I guess is bad for performance.  Have not tried to
compare the two approaches in frame time.  I just followed the [[https://github.com/Gekkio/mooneye-gb/blob/master/src/frontend/renderer.rs][guy who wrote a
GB emulator in Rust]].  Good enough for GB, good enough for Chip8, right?

Anyway, I was thrilled to see that the texture approach solved the clipping
issue that drawing quads had.

But, the horrible performance drop after a few seconds was still present.

** Did I enable VSync?                                              :chipers:
Lots of fumbling around, trying things with timing and what not.

In the end, I /though/ I'd found the issue.  My Nvidia driver had "force VSync"
enabled.  It's weird, because Glutin has a vsync option, which was disabled by
default.  And based on the fact that, in the SDL version, disabling VSync
actually worked, I figured it would be the same for Glutin.  Apparently not.

Disabling this option made the performance drop disappear.. for a while.

But I did encounter it a few times after that.  I guess it's a timing issue,
like not meeting frame time and still going after it.  Then there must be a sort
of mad race of the CPU trying to catch up to a shorter and shorter frame
time...

Probably should fix the main loop next.

** Anyway, ImGui is great                                           :chipers:
Once rendering to Glium was done, integrating ImGui was a breeze.

Could had a FPS counter, a memory view, and register info.

The only downside of using the Rust binding imgui-rs, is that porting C++ ImGui
examples is not straightforward.

The [[https://github.com/ocornut/imgui/wiki/memory_editor_example][memory editor example]] has nice features, like editing.  But you cannot just
"port" its code imgui-rs, because the API is not at the same level.  ImGui has
~begin~ and ~end~ blocks, while imgui-rs has closures.  Inside Rust closures,
there are mutability issues: you cannot borrow ~self~ mutably more than once for
instance.  I might find a way around it, or I might implement the memory view
using imgui-sys, the low-level binding.

** And GLSL can be great, too!                                      :chipers:
After battling with GLSL just to get a single color on the screen, I at least
put them to good use.

In my JS version, I wanted a CRT-like effect, since straight big quads on LCD
screen were boring.  Unfortunately, scared of OpenGL ES, I was rendering on
canvas, which meant that the CRT effect was done in software.  JavaScript +
software rendering effects = 10 FPS fullscreen for an emulated 64*32 screen.
Rather sad.

So I was delighted to see that fullscreen CRT + phosphor trail effects were
easily achievable on my machine.  And since I was using GLSL, I figured
/someone/ had battled the language long enough to produce a nice-looking CRT
effect that I could re-use.

Turns out, there are dozens of CRT shaders (especially for retro emulation).
Some of them are in a defunct shader language for NVIDIA hardware, Cg.  Some of
them target the D3D shader language, HLSL.  Some of them use various versions of
GLSL (compatible with OpenGL 2 to 4.. with mystifying shader language versions).

Anyway.  I took one that was convincing enough, banged on it until it worked for
my setup, and voilà.  Convincing effect.

Though I also tested it on my work box (integrated intel chipset from '07), and
it is unbearably slow.  Will add a flag, and might look into optimizations later
on...

* [2016-07-11 lun.]
** Thinking about perspective in 2D games
For a moonshot project.  I was envisioning a side-scrolling view, but I knew
from games I'd played that a top-down view lent more to exploration.  It got me
thinking of perspective choices in classic games.

Zelda 1 is top down.  Top-down gives you two axes of freedom.  It's much more
"open" than a side-scroller like Mario.   In Mario, it is evident you have to go
to the right.  There's no choice.  The difficulty is in getting there.   The
contrast with Zelda is evident: as you start, there are already four choices of
directions: up, left, right, and a cave.  Most of the screens have two exits or
more.  This choice helps convey a real sense of an open world, left to explore.
There's no pressure to the player, even though there is an implicit progression
path.

Contrast that with Zelda 2.  Zelda 2 has top-down overworld, but side-scrolling
dungeons, towns, and encounters.  The towns feel empty and repetitive, even
though they have people moving around.  You are just passing by.  Contrast to
Kokoriko village in Zelda 3: the structures there hamper your movements, they
are real.

But the overworld of Zelda 2 is rather limited as well: there are obvious
paths you should take.  The map is too much gated: you cannot go there yet,
cannot go there yet, etc.

The dungeons in Zelda 2 mostly feel like corridors.  The side-scrolling make
combat harder than it should be.  There are strong Castlevania vibes, except
with a puny dagger instead of a satisfying flail.

Castlevania, Megaman, Duck Tales... the side-scrolling lends itself more to
action than exploration.

But Metroid shows you can still pull off exploration in a side-scroller.

Roguelikes have been predominently top-down.  Rogue, Nethack, and the like.
Although this might have been motivated by technical limitations, the choice has
been deliberate in modern variations: Isaac and Necrodancer.  Though Isaac was
clearly inspired by Zelda 1, and Necrodancer rhythm component might have left
only the top-down option.  Risk of Rain chose a side-scrolling view, and it
makes the level much less interesting.  But again, that might just be because
levels are mostly empty, rather than caused by the perspective choice.

One thing is certain: in a side-scroller, the character usually obeys gravity.
Jumping becomes the basic way to use the second axis of freedom.  Otherwise you
have the clunky stairs of Castlevania.  Now, a game with jumping will lend
itself more to platforming than pure exploration.  This opens opportunities for
combat design: the fights in Zelda 2 are more involved than in Zelda 1.  But
Isaac shows that a top-down perspective can also have deep combat: it's mostly
about constraining the space the player can move to.

* [2016-07-26 mar.]
** About DSLs
So when you build any application, at some point you realize that you want a DSL
for maximum expressivity.

But there are various needs for a DSL, and various ways to build them.

For instance, in JS, there's a common idiom called a fluent API:

#+BEGIN_SRC js
$('#a')
  .css('color', 'blue')
  .toggle()
  .on('click', ...)
#+END_SRC

jQuery and D3 make heavy use of it.  I like to think of it as a DSL: it really
is a different language than plain JS, with different composition rules.  When
you begin an expression with ~$()~, you mentally switch into jQuery mode, to
know what you can follow.

The jQuery language is actually rather simple, the usual pattern is:

: $(selector)
:     .more_selection()
:     .manipulation()

First you target the elements you want to manipulate, then you manipulate them.
Pretty simple.

It happens to like the builder pattern used in Rust to build objects:

#+BEGIN_SRC rust
let display = glium::glutin::WindowBuilder::new()
  .with_title("Chipers")
  .with_dimensions((screen::SCREEN_WIDTH * zoom) as u32,
                   (screen::SCREEN_HEIGHT * zoom) as u32)
  .build_glium()
#+END_SRC

Here we are just building a configuration object.  The grammar is also rather
simple:

: FrobinatorBuilder::new()
:          .with_a()
:          .with_b()
:          ...
:          .build()

Bonus: there are actual types to these functions so the compiler can complain if
you mess up the grammar, like ~build~ before ~new~, or two ~build~ in a row.

D3 also has a fluent API.  There, the grammar can be a little more complex, with
the select/join mechanism, and things like ~enter~.

A simple language is one that builds an AST, you just compose functions:

: seq(assign(var(x), plus(num(1), num(2))), print(deref(x)))

The grammar is simply:

: expr: seq | assign | var | plus | num | print | deref

I'm wondering what happens when you take object algebras, but you only really
need one interpreter, not many?

#+BEGIN_SRC js
/* eslint-disable */

var e1 = m => { with(m) {
  return plus(num(1), num(2))
}}

e1 //: function

var interp = {
  plus(a, b) { return a + b },
  num(n) { return n },
}

e1(interp) //: 3

// Might as well

var plus = (a, b) => a + b
var num = n => n

var e2 = plus(num(1), num(2))

e2 //: 3

// thunk it

var e2t = _ => plus(num(1), num(2))

e2t //: function
e2t() //: 3

// How about partial evaluation?

// Here is a program

var e3 = m => { with(m) {
  _def('rec', _ =>
       _if(_less(0, 2),
           _ => 1,
           _ => _call('rec')))
  return _call('rec')
}}

e3 //: function

var _eval = {
    _v(n) { return  },
    _def(f, b) {
      this[f] = b()
    },
    _if(c, t, e) {
      if (c) { return t() } else { return e() }
    },
    _less(a, b) { return a < b },
    _call(f, a) {
      return this[f]
    }
  }

e3(_eval) //: 1

// Well, that's not very interesting

// Thunk everything?

var ast = {
  plus(a, b) { return {
    eval() { return a.eval() + b.eval() }
  }},
  num(n) { return {
    eval() { return n }
  }},
}

e1(ast).eval() //: 3
#+END_SRC

Okay, that was crap.  Time to forget.

* [2016-07-29 ven.]
** Revisiting the Game Loop
All [[https://www.youtube.com/watch?v=fdAOPHgW7qM][these]] [[https://www.youtube.com/watch?v=jTzIDmjkLQo][talks]] helped me understand how a game loop should work.  But it also
applies to any simulation, including emulation.

As usual, I prefer to go from most straightforward solution, and understand
/why/ it's wrong, and /why/ the correct solution is not the first that pops into
my mind.

So, the first game loop I remember writing was an OpenGL Pong.

I lifted code from NeHe's OpenGL tutorials, and hacked it until I had a game
working.  The tutorial code already took care of pushing a triangle to the
screen.  It used OpenGL direct mode, which was easy to pick up, so I just
changed it to have two rectangles at the edges of the screen.

Then came input.  Here again, the tutorial had code for grabbing input from
Win32.  I just had to find the right place, the correct keycode, and move the
rectangles by a reasonable amount.  The code looked like so:

#+BEGIN_SRC c++
void handle_input(...) {
  ...

  if (is_keydown(VK_UP)) {
    player1 += 0.12f;
  }
  if (is_keydown(VK_DOWN)) {
    player1 -= 0.12f;
  }
  if (is_keydown(VK_A)) {
    player2 += 0.12f;
  }
  if (is_keydown(VK_X)) {
    player2 -= 0.12f;
  }

  ...
}
#+END_SRC

Now I had moving rectangles!  Then I moved to collision detection, which as I
remember was solved with a bunch of ~if~.  Anyway, it worked great!  Surely I
had to tweak the move values above until if felt right–not too slow, but not too
fast either.  At this point I was rather proud.

So I copied the game onto a floppy, and brought it into school.  It so happens
that we had a computer room, to which I had access at any time between classes
because I helped set it up, along with other students.  So I put the floppy in,
launch the executable and behold!

Oh wait, it's all going /much too fast/.  Even the slightest input will move the
paddle half a screen worth; it's barely playable.  And the ball just passed
right through the right paddle without hitting it!  What happened?  It was
working right on my machine.  Needless to say, my friends were only mildly
impressed.

Of course, now I understand perfectly why it happened, and why I made that
mistake.  The computers at my school were simply faster than the one I had at
home.  I don't remember if there was any syncing to a fixed framerate or to the
monitor refresh rate in the NeHe code.  If there was, it might be that I was not
hitting that framerate at home, but I doubt it as it was /OpenGL/ for rendering
two rectangles paddles and a square ball, not software rendering.  So maybe
there wasn't any framerate limit in place, and the computer at the school just
went as fast as possible.

Now, having only written a handful of programs, this was my first simulation.  I
had written interactive text-based games, but these were turn-based.  You print
something to the screen, wait for user input, then print something else.
Running it on different computers would get you the same results.  For other
programs that sort numbers or print something to the screen even without
interaction, you usually /want/ them to run faster on beefier computers.  So I
did not even think twice at how that would play out for a simulation.

In a game like Pong, you want it to behave the same from one computer to
another, regardless of the specific hardware that supports it.  If you think
that the ball moves at 1 pixel per frame, then the game will feel faster at a
lower resolution, or at a higher frame rate.  OpenGL already frees you from the
actual display resolution, by giving you a continuous space for positioning
objects: the paddle moves by ~0.12f~ each frame, not 1 pixel.  You have to think
of time as being continuous as well: say, the ball at ~0.3f~ each 60th of a
second.

Then you understand why you cannot write the game with a ~while(true)~ loop that
just simulates and renders as fast as possible.

** Sampling player input
One thing that I might have missed from my [[*What’s the ideal solution to input latency anyway?][previous discussion on input latency]].

Consider sampling a simple button press:

: __________----------__________
:  10ms        10ms      10ms

This is continuous from the player point of view.  But if we sample, say, every
20ms, depending on where the sampling begins, we might miss the button press
altogether:

: __________----------__________  signal
:    |   20ms            |        samples
: ______________________________  reconstructed

Because the game has to reconstruct the signal from the sampled points, and the
two samples are 0 (button up), the game never sees that the player has pressed
the button:

Now if have a 6ms sample rate:

: __________----------__________
:   |     |     |     |     |
: _________------____________
: _________-----------_______
: ______________------_______
: ____________-----__________

If the signal changes between two sample points, there is an issue.  You don't
know exactly when the signal changed, so you have many ways to interpret it.

Usually in the code I write, I just look if the button is pressed, then simulate
as if it was pressed for the duration of the frame.  So you end up with:

: __________----------__________
:   |     |     |     |     |
: ______________------_______

we see that are already losing information.

Now, if we are sampling every 4ms:

: __________----------__________
:   |   |   |   |   |   |    |
: __________------------_____

it does not matter if we are below the Nyquist frequency for sampling, because
the function is not continuous I guess?

Anyway, if we sample at a high enough rate, hopefully the user won't notice the
discrepancy between their input and the input synthesized at the screen.

* [2016-07-30 sam.]
** Revamping S3C for evaluation inside blocks                           :s3c:
See [[https://github.com/fmdkdd/s3c/issues/4][issue 4]].

Managed to make it work using esprima + estraverse + escodegen.

First: using those on the browser is kind of a shitty situation without modules.
I can install esprima with bower and use that directly.  Fine.  Then, estraverse
is also on bower, so I install that.  But the file is not browser compatible,
you have to use browserify.  Ok fine, I install browserify and run it, then get
something I can import in my HTML and it works.  Finally I need escodegen.
Surprise, the bower package does not work.  I try npm, I see that one can build
a browser version from that (not using browserify mind you, but another tool,
cjsify).  Does not build.  Ok, there's an issue and even a pull request for
that.  You can't build the browser version from the npm package; you have to
clone.  I clone, build, and now I have a browser build!

Three related modules, three ways to get the browser version.

So at the moment I have the basic functionality of evaluation markers working,
even in blocks.  There are changes from the previous evaluation model though.

Previously, we split the evaluation of the code everywhere there was a marker.
So if an expression evaluated to an error (even a syntax error), we would
evaluate the rest of the program without the error impacting us.

Now, we evaluate the whole program at once, and collect the values of the
expressions that have markers.  If there is an error at the start of the file,
it's less resilient.  Also, esprima will fail to produce an AST if there is a
single syntax error.

I don't have errors working yet, but we are already losing functionality I'm not
sure I can get back.

Cleaning up the logic.  I can't use the backlog method because now because
markers in blocks may receive multiple results.

Cleaning up more, I have errors and timeouts working again.  For the moment one
error stops evaluation for the whole program.  Maybe I can capture them by
wrapping the expression statements in a try/catch...

Speaking of which, I tried to put a marker inside a try/catch and it did not
work.  Must investigate later.

Now I'm trying to see if the code I have from my PhD manuscript works with the
new logic.  And... SYNTAX ERROR AT LINE 2.  Esprima fails to parse ES6
syntax... sigh.  Wait, the README says it /does/ support ES6.  Latest version is
2.7.2, and ... the heck.  I have 2.0.0.  Well, thanks bower.  Guess I'll just
grab the latest version and manage it by hand then.

Ah, now that's funny.  Because ESLint also uses esprima, but an obsolete
version that's bundled inside the file.  That's at least three different parser
for the same project.

Anyway, updated Esprima, and the example works!  Except I have to try/catch the
one deliberate error.

Oookay.  Fixed evaluation markers in IIFEs.

Problem was twofold: first could have multiple markers associated with the same
evaluation comment.  But only one them would receive a result back from the
worker.  So ~undefined~.

I fixed this by using a map to keep track of comments already seen and the
marker we constructed for it.

The we did not associate evaluation comments to the nearest parent expression
statement, but to all expression statements above.  Thus, in an IIFE like:

(function(x) {
  x //:
})(1)

there would be two ExpressionStatements: ~x~, and the IIFE.  Both would be
associated to the one evaluation comment, and receive a result from the worker.
And the second result would overwrite the first, so ~undefined~.

I fixed that by doing a first traversal of the AST to find evaluation comments
and associate them with the nearest parent expression statement.

IIFEs work.  Try/catch works.  Loops work.  ~with~ works.  Useless braces blocks
work.

I have slightly changed the semantics of the evaluation marker though.  Before,
it would give the result of the /last expression/.  Now it gives the result of
the nearest parent expression.

Okay, wrapping the expression in a try/catch allows me to prevent errors from
polluting the rest of the results.  I added an alternative syntax for this
behavior though, as it can be unexpected inside a try/catch.

* [2016-07-31 dim.]
** Updating ESLint                                                      :s3c:
So I want to update ESLint because the parser is out of date.  And the
browserified file is disgustingly huge (671K).  I get the latest version, well
they are still using browserify.  The output is now 2.7M.

Okay, been looking around.  It's a bit ridiculous to charge that 2.7M, but there
might not be an easier way to get an up to date version of ESLint.

I've noticed that ESLInt is using a fork of Esprima, espree, so I can't factor
that out.  It might make sense to use espree as well, or even Acorn.  Shouldn't
be too much a bother since the interface seems compatible with Esprima's.

Maybe I'll just try to uglify ESLint and see how that goes.

Making a note here that there's a way to get back the parsed AST from ESLint.
Should I want to reuse it.  But I'm not sure it would make a difference.

Using Uglifyjs compression and mangling slims down ESLint to 808K.  An
acceptable size bloat for the gained functionality.  Okay, let's minimize
everything while I'm at it.

aaand updated CodeMirror to latest version.

Done & uploaded.

* [2016-08-01 lun.]
** Performance issues                                                   :s3c:
It didn't feel like the new version of s3c was any slower than the previous
one.  On my home machine.  On my work machine there is perceptible delay.
Around 500ms I would say, but can't say exactly since profiling does not even
work under Firefox.

So on my machine a full eval cycle + rewrites takes 75ms:
- triggering the eval takes 45ms with 35ms spent in ~reval~ (15ms parsing, 8ms
  clearing the markers on the page) and 10ms lost in ~endOperation~.
- the remaining 30ms are spent in ~write~ calls.  Each write averages 1.5ms.

And that is /after/ doing a first optimization, which is fixing the size of the
editor.  Previously the editor had ~height: auto~.  But that meant that any
change to its content would be written back to the DOM, even if that content was
outside of view.  CodeMirror does not do a hit test to check if it's in view.
Instead, you should let CodeMirror handle the scrolling.  Doing that shaved 30ms
off.

Also of note is the time to evaluate the JS: 277ms, and 47ms to finish ~init~.

Reusing the AST from ESLint is a big improvement.  But, it's not equivalent.
Linting happens sporadically (debounce + 500ms), so Ctrl+enter just after an
edit will have an outdated AST.  Linting takes 121ms on the same buffer: 50ms
parsing and the rest applying rules and update the DOM.  121ms is the first
time, after I get around or below 50ms.  Maybe JIT optimizations kicking in?
Might be worthwhile to reduce the linting delay and have linting always happen
before we have time to trigger evaluation.  Then we reuse the AST.

Was trying to reuse the ESLint AST in this fashion, but hit a weird behavior
where after a first eval, the subsequent evals did not refresh the markers.
The markers are empty the second time around.  Not sure why.  But it negates the
visual feedback of clearing the markers.  Maybe I can get the visual feedback by
flashing the Run button instead?

Reusing the AST shaves 15ms off, but is not quite correct yet, since we have to
detect if the text has changed since before the last lint, otherwise triggering
eval reuses the obsolete AST and it does nothing.  I have to think through the
whole pipeline as:

user changed text -> debounce to 250ms -> reparse (ideally, with a parser that
does not start from scratch) -> give AST to linter

But if reval is triggered and we don't have a fresh AST, then reparse, eval, and
save the AST for linting afterwards.

In the meantime, I've got it down to spending only 10ms to reval and 10ms to
rewrite.

But, only now I finally find that the worker takes 25ms to actually eval the
code.  And from hitting Ctrl+Enter to seeing the eval results, it's around
350ms, mostly of waiting around for debouncing.

* [2016-08-02 mar.]
** Links on incremental parsing                                         :s3c:
Not sure it would be worth it for the scale of the code that s3c deals with, but
here are some resources on incremental parsing would I want to pursue it (or
just out of curiosity):

- [[http://harmonia.cs.berkeley.edu/papers/twagner-parsing.pdf][this paper]] from 1998 seems to cover the theory, and even provides the Java
  code for its algorithms for incremental parsing based on LR grammars.
- [[https://github.com/Eliah-Lakhin/papa-carlo][this project]] is an incremental parser in Scala using PEG grammars.

Intuitively, we might get good mileage out of a few heuristics like looking at
blocks: if I change a character inside function ~f~, then at worst we only need
to reparse the node for this function.  Given a change, walk up the tree to the
first block, throw the node, reparse and replace.  Now, 1) I don't know how
sound that actually is, and 2) now sure how it holds with larger changes (a find
and replace, or an undo).  The pathological example would be: erase everything.
Now parsing from scratch the empty string should be faster than walking the tree
checking if every node is still there.

The problem can also be entirely side-stepped with an editor that would only
allow actions that modify the AST without ever creating an invalid one.  Rather
than editing at the character level, you edit at the AST node level.  But I
don't know how practical that can be in the end.

Anyway, all of that might not even matter for speeding up s3c, since parsing
might not even be the biggest bottleneck.

* [2016-08-13 sam.]
** Using the JSON error format of rust for flycheck                :flycheck:
*** Restoring functionality
Previous message parser was rather straightforward: error appeared as errors,
warnings as warnings, and note or help lines appeared as info squiggles.

In the JSON output, we have multiple spans that corresponds to squiggles.  One
span is the primary (the root cause or main line of the error), and the others
seem to correspond to notes in the compiler human readable output.

*** Passing tests
The JSON output is the same format for stable and nightly, but the exact output
can change from version to version.

*** Changing flags triggers a rebuild?
There was a mention on a thread somewhere that using RUSTFLAGS to ask for
~--error-format~ in IDE can trigger a full rebuild of cargo.  Can't reproduce in
our setting; maybe because we don't use RUSTFLAGS but call ~cargo rustc~?

*** Flycheck does not use line or column end points
Squiggles only overlap the symbol at the given line/column, but rustc will
output the start and end position already.  Flycheck does extra calculation for
nothing, and it's less accurate than rustc's info.

Sebastian outlined the steps for accepting column pairs in flycheck ([[https://github.com/flycheck/flycheck/issues/89][issue 89]]),
but that might be outdated.

*** Looking up explanations from Emacs
rustc provides explanations, but I don't think that's flycheck's job to show
them to us.  I could write a function ~explain-rust-error~ that looks at the
code of the error under the cursor (when flycheck is loaded) and opens a
temporary help buffer with the explanation.  Without flycheck, it asks for an
error code interactively.

* [2016-08-15 lun.]
** Using column end points for rustc in flycheck                   :flycheck:
I started by using cons cell for columns instead of a number.  Then flycheck
complained the checker returned an error.  But since it caught the error, I
could not use the debugger to trace it.

There are multiple places where columns are used.  I managed to hack my way
through them until it worked.

I had assumed that just reusing the column value of rustc for the overlays would
work... but overlays only use a single coordinate for their start and end
points.  I had to convert the (line column) information to a single character by
piggy-backing on ~flycheck-error-column-region~.

And it works!

But it is at odds with the notion of flycheck highlighting modes.  I think the
behavior we want is: try to use the line/column info returned by the checker,
otherwise fallback on the selected mode: lines, columns, symbols, sexps.

We want to fallback because not all tools might give column end information.

* [2016-08-16 mar.]
** Imaginary property
This morning when coming over to work, I was having an internal debate about one
of my pet peeves: copyright.  Or, how I prefer to call it, /imaginary property/.

Note that I have no claim of originality on this moniker.  To the extent that
someone /can/ claim precedent on a juxtaposition of two words.  But after all,
since many companies do hold rights to such juxtapositions in the form of
slogans, brands, or product names, you never know.  I can however cast any doubt
that I thought of it first, as I encountered it years ago on the news site
Slashdot, where a user went by the asserting handle
"I_do_not_believe_in_imaginary_property".

I was having this internal debate.  Oh, an internal debate is basically what it
says on the cover: me having an argument in my head, with at least two voices
making their points in order.  These debates tend to play like a mix of chess
and golf.  Each side is carefully considering their next move to find the best
play.  They want to corner the opponent, and not leave him options to escape.
At the same time, I, as the observer, want to find arguments that have the most
weight, that raise the most interesting questions.  I try to take each argument
charitably, as the purpose is not so to that one side wins, but to better
understand each side's point of view.

Thus, this morning debate's was about imaginary property.  Now, I like this term
because it is not neutral at all; it's a moral statement.  Not unlike the word
"copyright" itself: the "rights of copying" is not an innocent denomination.  If
you accept the word, you accept its moral premise: that copying should be
regulated by rights.  The same happens In the french terminology, where our
copyright law is an "intellectual property law".  If you silently accept the
name, you tacitly agree that there is such a thing as an intellectual property.
The assumption here is that coining terms such as "intellectual property" is a
weasely way to conjoin your mental representations of both concepts.  With this
connection unconsciously made in your brain, you are eased into taking this
chimeric concept as a fact.  The choice of words here is truly Orwellian.

One of my mental orator disagrees with this premise, and counters with a loaded
term of his own: you speak of intellectual property, but I say it is imaginary.
The whole premise is refuted, so that any further arguments on the specifics of
copyright is moot.  It steers the debate to whether it is even /possible/ to own
thoughts in the first place.  It is powerful opening move.

The answer may seem obvious.  /Cogito ergo/ dibs.  Who is doing the thinking?
/I/ do, therefore the thoughts are mine.  Consequently, any product of these
thoughts is also mine.  Well, that may be tautological for some, but I do not
see how it follows.  We could again argue the premises: that there is an "I",
that there are thoughts to be had, that our experience of individuality is not
just an illusion, a side effect to the working of our brains.  However, at this
point in the debate most interlocutors would question my sanity and leave the
room (through my ears).

Fine, we'll take another route.  We can concede the reality of consciousness and
individuality.  These are convenient concepts after all—allowing me to use "I"
all along this text without eliciting existential conundrums.  But we can still
question the ownership of thoughts.  For me, ownership by the thinker is not
obvious.  Here's how I /think/ it works: thoughts are not created; they are not
elaborated by the sheer power of our will, they are merely witnessed as they
happen in the brain.  The brain is made of neurons; neurons stimulate each other
by chemistry and electricity; so much we know for a fact.  Now, to the best of
my knowledge there are no sound explanation of what a thought is in term of
neurons.  It might forever be an ill-defined notion, even if we someday crack
the brain's secrets.  I will make the reasonable assumption that if we have any
thoughts, they are caused by neuronal activity.  I like to picture the neurons
as a large and dense graph.  Millions of nodes, billions of edges.  A thought is
then a collection of /bounded walks/ along the graph.  Some neurons are excited,
they light up, thinking happens.  Due to the size of the graph, the number of
different walks in just one brain is practically infinite.  In this view, one
can have infinitely many different, unique thoughts.  But two exact same walks
would produce the exact same two thoughts.  All our thoughts thus depend on two
factors: the brain configuration, the way it is wired up; and the initial
stimuli, the start of the neuronal promenade.

If we accept this model, we must see that in order to claim ownership over these
thoughts, we must be in total control of these two factors.  But how can it be
the case?  The initial stimuli is clearly not entirely in our control.  We have
no way of forcing thoughts through some neuronal pathways.  It can /feel/ like
we are steering the boat, but there clearly is something happening at the
unconscious level that is doing the heavy lifting.  And the other factor is
mostly genetic and/or environmental, depending on your stance.  If you do not
believe in free will, then trivially you cannot say to be in control of your
thoughts.  If you do believe in free will then you can think your daily actions
may have an impact on your brain configuration.  But this impact is at best
indirect.  Your thoughts are what they are because you where brought up in
/that/ city, in /that/ neighborhood, in /that/ country, on /that/ planet, and
you grew up with /that/ family, /these/ friends, and you read /these/ books, and
listened to /that/ music, and visited /that/ place where you had all /these/
memories...  All of that shaped who you are, and what you think, and it
continually keeps doing so.  And you cannot reasonably claim ownership of all of
these factors.

-----

Most proponents of copyright conflate two topics: the regulation of copying
intellectual works, and the remuneration of the authors of said works.  A
simplified, but too common argument goes like this: "Well, copyright is a good
thing because that's how artists get paid."  To which one would answer: "No,
copyright is /wrong/, because it goes against our instincts to share".  Stop!
You are arguing different things!  One is for the remuneration of artists, while
the other is against the criminalization of sharing!

How I can conceive that we have these views:

- making your mark in imaginary space, obtaining a plot of intellectual land

That's by analogy to physical property.  But maybe this analogy is not
appropriate?

Intellectual colonists?  Who go and appropriate themselves a plot of
intellectual land.  We do seem to treat intellectual property as we do physical
property.  We can cede these lands: rights of exploitation.  We even have a word
for intellectual property trespassing: plagiarism.


The view of an untainted vision, the lone genius:

- seeing your work as optimal?  Then any deviation would invariably lessen it.

I stumble upon an indie gameboy color game.  The sources are given in a CC-NC
license, but the music strictly forbids /derivatives/.  How can anyone sustain
this position?

Here are the musicians in question:

#+BEGIN_QUOTE
The discussion and copyrights are mostly to protect the original score and its
original vision. I can't really give you much more info than that, mostly
because I wasn't the one who was negotiating all of this.

I had a lot of fun converting and worked really hard with the music translating
it to GBC though. It'd be a shame to hear it modified. So I believe the
negotiations were meant to protect our involvement as well.
#+END_QUOTE

#+BEGIN_QUOTE
The status of the game as of now is open source with special rights, music
cannot be used elsewhere/modified without my consent (Eric E. Hache) and no
commercial endeavours. For the rest of the licensing, please check Affinix’s
Github license file.
#+END_QUOTE

* [2016-08-17 mer.]
** Chasing a failing build under emacs snapshot                    :flycheck:
Trying to reproduce the Travis errors on my machine.

~./autogen.sh~ suggest I run ~./autogen.sh git~ after it.  This is not done in
the makefile.

Trying to run the tests with emacs 25, cannot find ~dash~.  ~make clean~ and
~make init~ fixes it, and now I have an error because warnings (same thing as
the Travis build):

#+BEGIN_EXAMPLE
In toplevel form:
flycheck.el:5443:1:Warning: Unused lexical variable ‘\.label’
flycheck.el:5443:1:Warning: Unused lexical variable ‘\.column_start’
flycheck.el:5443:1:Warning: Unused lexical variable ‘\.line_start’
flycheck.el:5443:1:Warning: Unused lexical variable ‘\.file_name’
flycheck.el:5443:1:Warning: Unused lexical variable ‘\.is_primary’
#+END_EXAMPLE

So just having warnings trigger a non-zero exit in Emacs 25?

As I suspected, the warnings are caused by nested ~let-alist~ calls.  With just
one ~let-alist~ the compiler does not complain, but when they are nested all the
~.name~ inside the nested calls are considered free variables.

Unnesting these calls make the warnings disappear.

But why are warnings appearing in the first place?  My understanding is that
~let-alist~ is a macro that adds syntactic sugar for looking up the alist.  This
is expanded at compile-time, and thus all ~.name~ should disappear.  But the
byte compiler still sees those that are in nested calls to ~let-alist~, so the
macro expansion is not recursively done?

#+BEGIN_SRC emacs-lisp
(cl-prettyexpand
 '(let-alist '((a . 1) (b . 2)) .a))

(let ((alist '((a . 1) (b . 2))))
  (let ((\.a (cdr (assq 'a alist))))
    \.a))
#+END_SRC

#+RESULTS:
: 1

#+BEGIN_SRC emacs-lisp
(cl-prettyexpand
 '(let-alist '((a . 1) (b . 2))
    (let-alist '((c . 3) (d . 4))
      .c)))

(let ((alist '((a . 1) (b . 2))))
  (let ((\.c (cdr (assq 'c alist))))
    (let ((alist '((c . 3) (d . 4))))
      (let ((\.c (cdr (assq 'c alist))))
        \.c))))
#+END_SRC


Ah!  The first ~let~ line triggers the warning.  This is because ~let-alist~
thinks every ~.name~ under it should apply to it, but this is false when nesting
calls.

If the expansion happened from the innermost ~let-alist~ first, I guess this
would work.  So it's a bug in ~let-alist~.

The docstring of ~let-alist~ acknowledges that you can nest it, with the
downside that you cannot access the variables of the outer ~let-alist~.  This is
obviously because the inner ones shadow the ~alist~ variable.  This is a hygiene
issue.

So, two bugs for nesting.  Thanks, ~let-alist~!

** Checking out the competition                                    :flycheck:
Saw a thread on users.rust about how IntelliJ-rust had great support for rust.
Struck by a sudden fear of missing out, I investigate.

IntelliJ does it own font aliasing, which has clearly gone wrong, because every
piece of text has a thin blue outline.  Subpixel aliasing problem?  If only use
"Greyscale" aliasing it's bearable.  But, I'm using OpenJDK, and they note that
it is unsupported, so let's not throw the stone here.

Well, first it complains that the project is using a virtual ~Cargo.toml~ file.
Ok, I'm using vulkano to test it out, and I just read that the rust plugin does
/not/ support this setup.  But it works with flycheck, so I assumed it was fair
game!

Okay, switching to the vulkano library inside the meta project.

I can get feedback on parse errors rather quickly.  I suppose that's a plus of
using their own parser.  On the other hand, I can't seem to get errors from the
compiler.  I can't seem to build from the 'Build' menu, as it requires an SDK,
but I see only Java options in there.

Okay, I can use cargo commands in the 'Run' menu.  Let's try ~cargo build~.  It
opens up a lower panel with the command output.  There are two errors (that I
inserted), but no visual feedback in the file itself.

But maybe this just a work in progress.  It does notify when a constructor field
is missing.  So I guess they are just using their own sauce to give error
feedback, rather than using what the compiler tell them.  That's a waste.

Okay, not impressed on this front.  I thought I would at least parse the errors
and put them in the current buffer.  Especially as there is a cool fringe
preview: to the right of the buffer is the fringe, that shows points of
interest.

The fringe is a mix between the overview map of Sublime Text and the left margin
of Eclipse.  The fringe always represent the whole file, not just the current
view.  You have lines with different colors in the fringe that give different
information.  For starts, it highlights all the places where there is a ~TODO~
comment.  The nice thing is that if you just mouse over the fringe line, you get
an instant preview of that place in the code.  You don't have to jump there.
Similarly, when the cursor is on a symbol, it highlights all instances of this
symbol in the buffer, and places line in the fringe for all of them.

But, I see that the highlighting only works for symbols that are defined in the
file.  The upshot is that it's not just a text search, it's really trying to
give you the occurrences of /this/ symbol, and avoid false positives.  The
downside is that sometimes it does not work at all.

The find usage is the same thing, but can also report the usage across different
files.  That's definitely a useful feature.  I wonder if it's something ~racer~
could provide.  Having a text search with ~ag~ is useful regardless of the
language, but having no false positives is also great.

There is a block selection thing like ~expand-region~.  It is more fine grained
than ~expand-region~ does for me in Rust, which reminds that ~expand-region~ is
extensible and that it may be worthwhile to make it work a bit better with Rust.

Automatically matching of braces when editing.  Well, this is basically
~electric-insert~, except that:

: Cpu {}

deleting the closing brace in IntelliJ does not delete the opening one.  So it's
not matched anymore.  But funnily enough, electric has the complementary quirk
where deleting the /opening/ brace does not work when they are on different
lines:

: Cpu {
: }

I'm always fighting these modes anyway.

Go to definition works, but mostly on definitions from the project.  It can go
to structs from the stdlib, but not inside crates.  I think racer does it for
stdlib, not sure for crates.  Curiously enough, the "Find symbol" feature can
find those functions in crates.  Maybe it cannot determine the definition place?
Again, no false positives, which is good.

Then there are snippets.  One thing I'm not sure ~yasnippet~ provides is
surrounding a piece of code with a snippet: start with a line, then surround by
a loop for instance.

So:

- check if racer can find definitions and usages, and how to leverage that in
  Emacs

(for a quasi-correct solution that's also fast, check ~dumb-jump~ which seem to
support rust)

Other minor features:
- see how to teach ~expand-region~ about Rust
- check if we can surround code with a snippet
- see how a whole-buffer fringe and buffer previews would work

Currently, when doing a symbol search with ~ag~ in Spacemacs, it opens up a Helm
window with multiple candidates.  Since there can be false positives, I need to
to quickly glance at each line to see it's the file/location I'm interested in.
Unfortunately, I can't seem to get Helm to open the current line in a preview
buffer, /while keeping the list of candidates open/.  I can recall the list of
candidates, and keep going, but that's not as fast as just glancing.

I'm pretty sure that having a preview like the fringe from IntelliJ would be
difficult to add without hacking emacs itself.  And I'm also pretty sure it has
zero chances of being adopted.  But, we'll see what we can do.

** Preview in helm-ag
Oh, I just looked up the bindings for Helm, and hitting ~TAB~ on a candidate
does what I want.  Even faster is ~helm-swoop~, but this works preferably on
open buffers.

Good news!

** Testing dumb-jump
Er... it works.  Sometimes.  And it's slow.  Maybe something can be done to
improve the functionality there.  But really, I think racer covers it and more.

* [2016-08-18 jeu.]
** Workarounds for let-alist                                       :flycheck:
The nesting bug has been bugging me.

Looking at the [[https://lists.gnu.org/archive/html/emacs-devel/2014-12/msg00231.html][original discussion]] for introducing ~let-alist~ to Emacs, there
a couple alternatives are mentioned.

Inline macro for reducing the ~cdr assq~ noise:

#+BEGIN_SRC emacs-lisp
(cl-macrolet ((a (field) `(cdr (assq ,field '((c . 1) (d . 2))))))
  (a 'c))
#+END_SRC

#+RESULTS:
: 1

And using ~pcase~:

#+BEGIN_SRC emacs-lisp
(pcase (alist-get-keys alist key1 key2 key3)
  (`(,val1 ,val2 ,val3) body))
#+END_SRC

but this one requires ~alist-get-keys~, which presumably is in Emacs 25.  But
Emacs 25 also includes ~alist-get~, which already reduces the noise a bit.

* [2016-08-22 lun.]
** Testing C# under Linux
To see if it's bearable, and whether there are no obvious discrepancies between
platforms when running the same app.

First I installed 'dotnet-cli' from AUR.  Took forever to build.  It had a
dependency (lttng-ust) that was in AUR also, and cower does not handle that.
Luckily, that was the only one.

Then the example of:

: dotnet new
: dotnet restore
: dotnet run

works as advertised.  Though the notice to dotnet is chilling:

#+BEGIN_QUOTE
Telemetry
--------------
The .NET Core tools collect usage data in order to improve your experience. The
data is anonymous and does not include commandline arguments. The data is
collected by Microsoft and shared with the community.  You can opt out of
telemetry by setting a DOTNET_CLI_TELEMETRY_OPTOUT environment variable to 1
using your favorite shell.  You can read more about .NET Core tools telemetry @
https://aka.ms/dotnet-cli-telemetry.
-------------------
#+END_QUOTE

Now, onto a more serious app, one from [[https://github.com/merwaaan/shader-study/][merwaaan]].  Let's build that.

Wait, it does not have a 'package.json' file?  Only a 'csproj' you say?  Hmm.

Do I need visual studio as well?  Ah, [[https://blogs.msdn.microsoft.com/dotnet/2015/03/18/msbuild-engine-is-now-open-source-on-github/][maybe not]], just 'msbuild.exe' should do
it.  Let's build that.  Again, it's in AUR.  Waiting.

Now let's build it.

#+BEGIN_EXAMPLE
> msbuild.exe Shaders.csproj
Microsoft (R) Build Engine version 14.1.0.0
Copyright (C) Microsoft Corporation. All rights reserved.

...

 /tmp/shader-study/Shaders/Shaders.csproj(229,5): error : This project
references NuGet package(s) that are missing on this computer. Use NuGet Package
Restore to download them.  For more information, see
http://go.microsoft.com/fwlink/?LinkID=322105. The missing file is
..\packages\AssimpNet.3.3.1\build\AssimpNet.targets.
#+END_EXAMPLE

Oh.  I need to install dependencies.  Fair enough.  Let's get this nuget thing.
This time it's in Arch.

#+BEGIN_EXAMPLE
> nuget install
Installing 'AssimpNet 3.3.1'.

...

The 'System.Runtime.InteropServices 4.1.0' package requires NuGet client version
'2.12' or above, but the current NuGet version is '2.11.0.0'.
#+END_EXAMPLE

Hmm, okay.  Let's get nuget3 from AUR then.

Once more:

#+BEGIN_EXAMPLE
> nuget install
Feeds used:
  /home/fmdkdd/.local/share/NuGet/Cache
  /home/fmdkdd/.nuget/packages/
  https://api.nuget.org/v3/index.json

Restoring NuGet package System.Runtime.InteropServices.4.1.0.
Adding package 'System.Runtime.InteropServices.4.1.0' to folder '/tmp/shader-study/Shaders'
Added package 'System.Runtime.InteropServices.4.1.0' to folder '/tmp/shader-study/Shaders'
#+END_EXAMPLE

Smooth.  Ah, but wait, it doesn't build.  msbuild is still confused.

Hmm, let's try that again.

: rm -rf shader-study
: git clone --depth=1 ...
: cd shader-study
: nuget restore

And now...

#+BEGIN_EXAMPLE
> msbuild.exe Shaders.csproj
GUI.cs(18,23): error CS0227: Unsafe code may only appear if compiling with /unsafe [/tmp/shader-study/Shaders/Shaders.csproj]
GUI.cs(52,28): error CS0227: Unsafe code may only appear if compiling with /unsafe [/tmp/shader-study/Shaders/Shaders.csproj]
GUI.cs(102,29): error CS0227: Unsafe code may only appear if compiling with /unsafe [/tmp/shader-study/Shaders/Shaders.csproj]
#+END_EXAMPLE

Interesting.  Let's try this flag:

#+BEGIN_EXAMPLE
> msbuild.exe /unsafe Shaders.csproj
MSBUILD : error MSB1001: Unknown switch.
Switch: /unsafe
#+END_EXAMPLE

Kidding me.  Let me ddg that for you.  Hey, that's actually the "property"
"AllowUnsafeBlocks".

#+BEGIN_EXAMPLE
> msbuild.exe /p:AllowUnsafeBlocks="true" Shaders.csproj

GUI.cs(15,33): error CS1069: The type name 'Vector4' could not be found in the
namespace 'System.Numerics'. This type has been forwarded to assembly
'System.Numerics, Version=4.0.0.0, Culture=neutral,
PublicKeyToken=b77a5c561934e089' Consider adding a reference to that
assembly. [/tmp/shader-study/Shaders/Shaders.csproj]
#+END_EXAMPLE

Hmm that looks problematic.

I /do see/ a ~System.Numerics.Vectors~ package.  Maybe not quite the right
version?  Trying to change the version in the packages.config does not work, nor
does changing it in the (bloody XML!) csproj.

I'm puzzled.

And yet, surprised that it went that far.

* [2016-08-23 mar.]
** More on that C# error
#+BEGIN_EXAMPLE
> strings packages/System.Numerics.Vectors.4.1.1/lib/net46/System.Numerics.Vectors.dll
  | grep Vector4
Vector4
#+END_EXAMPLE

There is a trace of ~Vector4~ in the package.  Not sure if it's the class
needed, but maybe it's not loading the right thing?

In the sources I built dotnet with, I can find the class:

#+BEGIN_EXAMPLE
> find . -name 'Vector4.cs'
./src/corefx-1.0.0/src/System.Numerics.Vectors/src/System/Numerics/Vector4.cs
#+END_EXAMPLE

When calling ~msbuild.exe~, we can actually see the included libraries.  And
here is the relevant excerpt:

#+BEGIN_EXAMPLE
-reference:/usr/lib/mono/4.5/System.Numerics.dll
-reference:/tmp/shader-study/packages/System.Numerics.Vectors.4.1.1/lib/net46/System.Numerics.Vectors.dll
#+END_EXAMPLE

It loads ~System.Numerics~ from mono first, which is 4.5.  And then
~System.Numerics.Vectors~ from the local package.  But [[https://msdn.microsoft.com/en-us/library/dn877639(v=vs.110).aspx][MSDN lists]] ~Vector4~ as
"available since 4.6".

Now, it seems that whatever is defined in ~Vector4.cs~ from corefx is not what's
known as ~System.Numerics.Vector4~ in 4.6.

But crucially, it means that even though the .NET runtime was open sourced, it
still relies on Mono, the free implementation.  And Mono is lagging behind on
versions.  What's the point?
* [2016-08-26 ven.]
** More tries at let-alist                                         :flycheck:
Ronan gave me a couple of tips that may help.

- https://www.reddit.com/r/emacs/comments/2u5uzq/i_wrote_a_somewhat_useful_elisp_macro/
- http://www.greghendershott.com/fear-of-macros/

*** Can ~eval~ help?
The [[https://www.reddit.com/r/emacs/comments/2u5uzq/i_wrote_a_somewhat_useful_elisp_macro/][first link]] highlights the use of ~eval~ around a macro call.

#+BEGIN_SRC elisp
(cl-prettyexpand
 '(let-alist '((a . 1) (b . 2))
  .a
  (let-alist '((a . 2) (c . 3))
    .c)))

(let ((alist '((a . 1) (b . 2))))
  (let ((\.a (cdr (assq 'a alist)))
        (\.c (cdr (assq 'c alist))))
    \.a
    (let ((alist '((a . 2) (c . 3))))
      (let ((\.c (cdr (assq 'c alist))))
        \.c))))


;; Trying to add `eval' to force evaluation of the inner let-alist.
(cl-prettyexpand
 '(let-alist '((a . 1) (b . 2))
    .a
    (eval '(let-alist '((a . 2) (c . 3))
             .c))))

(let ((alist '((a . 1) (b . 2))))
  (let ((\.a (cdr (assq 'a alist)))
        (\.c (cdr (assq 'c alist))))
    \.a
    (eval '(let-alist (quote ((a . 2) (c . 3))) \.c))))


;; Okay, maybe with backquotes?
(cl-prettyexpand
 '(let-alist '((a . 1) (b . 2))
    .a
    (eval `(let-alist '((a . 2) (c . 3))
             .c))))

(let ((alist '((a . 1) (b . 2))))
  (let ((\.a (cdr (assq 'a alist)))
        (\.c (cdr (assq 'c alist))))
    \.a
    (eval '(let-alist (quote ((a . 2) (c . 3))) \.c))))


;; Nah, backquotes are like quotes when there are no commas
#+END_SRC

So ~eval~ does not actually force the evaluation of the inner let-alist, but
rather delays the macro-expansion.  That's not what I want here.

*** Fixing let-alist
Failed to mention last time that I spent hours trying to come up with a correct
macro to do what let-alist does.

One fix that worked was to prevent ~let-alist--deep-dot-search~ from expanding
dot symbols ('.a.b') inside another ~let-alist~.  Actually, I even made it to
stop searching when a supplied predicate function would return true on the
current node.

(Which could have been made more generally into a tree recursing function
operating on a predicate, like ~-tree-map-nodes~ from dash)

So the first ~let~ did not generate unused bindings.  But we could still not
access outer let-alist bindings, because if you look at this expansion:

#+BEGIN_SRC elisp
(cl-prettyexpand
 '(let-alist '((a . 1) (b . 2))
   (let-alist '((a . 2) (c . 3))
    .b)))

(let ((alist '((a . 1) (b . 2))))
  (let ((\.b (cdr (assq 'b alist))))
    (let ((alist '((a . 2) (c . 3))))
      (let ((\.b (cdr (assq 'b alist))))
        \.b))))
#+END_SRC

The second let will always bind all dotted symbols under it, shadowing the outer
bindings.  Even though 'b' is not a key in the inner alist, it still tries to
get that key.

Now, let-alist supports looking up deeply into nested alists:

#+BEGIN_SRC elisp
(let-alist '((a . 1) (b . ((a . 2) (b . 3))))
    .b.a)
#+END_SRC

#+RESULTS:
: 2

But in our case the JSON objects has arrays, so that's why we need a second
let-alist call.

I tried to define a macro, ~alist-let~ that would have worked like so:

#+BEGIN_SRC elisp
(alist-let (a. '((a . 1) (b . 2)))
   (a. 'a))
;; => 1
#+END_SRC

The plan was for that to expand to:

#+BEGIN_SRC elisp
(let ((alist-a '((a . 1) (b . 2))))
  (cl-macrolet  ((a. (field) `(cdr (assq ,field alist-a))))
    (a. 'a)))
#+END_SRC

which in turn would expand to:

#+BEGIN_SRC elisp
(let ((alist-a '((a . 1) (b . 2))))
   (cdr (assq 'a alist-a)))
#+END_SRC

But since ~alist-let~ was a macro that invoked ~cl-macrolet~, I never got it to
work correctly.  The tricky part was to pass ~alist-a~ to ~cl-macrolet~ even
though the binding inside ~cl-macrolet~ is inside a backquote.

Maybe I should have used two macros?

Also, it's not zero-cost still, since all calls to ~a.~ would expand to ~cdr
assq~.  With ~let-alist~, the looking up is done once in a surround ~let~.

So the expansion of ~alist-let~ that you want is:

#+BEGIN_SRC elisp
(let* ((alist-a '((a . 1) (b . 2)))
       (alist-a-0 (cdr (assq 'a alist-a))))
   alist-a-0)
#+END_SRC

*** Gensym trouble
Version 1 of my ~alist-let~ macro:

#+BEGIN_SRC elisp
(defmacro alist-let (getter alist &rest body)
  (declare (indent 2))
  `(let ((alist-a ,alist))
     (cl-macrolet ((,getter (field) `(cdr (assq ,field alist-a))))
       ,@body)))

(cl-prettyexpand
 '(alist-let a. '((a . 1) (b . 2))
    (a. 'a)))

(let ((alist-a '((a . 1) (b . 2))))
  (progn
    (cdr (assq 'a alist-a))))
#+END_SRC

Works, but ~alist-a~ is not hygienic:

#+BEGIN_SRC elisp
(defmacro alist-let (getter alist &rest body)
  (declare (indent 2))
  `(let ((alist-a ,alist))
     (cl-macrolet ((,getter (field) `(cdr (assq ,field alist-a))))
       ,@body)))

(alist-let a. '((a . 1) (b . 2))
  alist-a)
#+END_SRC

#+RESULTS:
: ((a . 1) (b . 2))

So you want to generate a symbol to use instead of ~alist-a~:

#+BEGIN_SRC elisp
(defmacro alist-let (getter alist &rest body)
  (declare (indent 2))
  (let ((alist-a (gensym)))
    `(let ((,alist-a ,alist))
       (cl-macrolet ((,getter (field) `(cdr (assq ,field ,alist-a))))
         ,@body))))
#+END_SRC

Now, as I understand it, ~alist-a~ is a symbol that's generated at expansion
time by ~gensym~.  The macro expands to the same let as before, except now the
binding ~alist-a~ is variable, that's why there is a comma in front.  Inside
~cl-macrolet~, ~alist-a~ must also refer to the generated symbol, hence the
comma.

The problem is that the expansion of ~cl-macrolet~ fails to find ~alist-a~.

We can check that binding the gensym at expansion works:

#+BEGIN_SRC elisp
(defmacro alist-let (getter alist &rest body)
  (declare (indent 2))
  (let ((alist-a (gensym)))
    `(let ((,alist-a ,alist))
         ,@body)))

(cl-prettyexpand
 '(alist-let a. '((a . 1) (b . 2))
    (a. 'a)))

(let ((G23147 '((a . 1) (b . 2))))
  (a\. 'a))
#+END_SRC

Using ~pp-macroexpand~, we can see that indeed after expanding ~alist-let~, but
not ~cl-macrolet~, we get:

#+BEGIN_SRC elisp
(pp-macroexpand-expression
 '(alist-let a. '((a . 1) (b . 2))
    (a. 'a)))

(let ((G23158 '((a . 1) (b . 2))))
  (cl-macrolet
      ((a\. (field) `(cdr (assq ,field ,alist-a))))
    (a\. 'a)))
#+END_SRC

~alist-a~ is not expanded, and the binding is lost.  So that's why it fails.

Is it because of the second backquote?

#+BEGIN_SRC elisp
(defmacro test ()
  (let ((var "somethin"))
    `(,var `(,var))))

(cl-prettyexpand
 '(test))

("somethin" (list var))
#+END_SRC

#+BEGIN_SRC elisp
(defmacro test ()
  (let ((var "somethin"))
    `(,var (,var))))

(cl-prettyexpand
 '(test))
("somethin" ("somethin"))
#+END_SRC

Ah, indeed.

Another way to see why it's wrong is to look at the line of the macrolet:

: (cl-macrolet ((,getter (field) `(cdr (assq ,field ,alist-a))))

See, both ~field~ and ~alist-a~ have a comma in front.  But clearly, we want
~alist-a~ to be expanded when ~alist-let~ expands, and ~field~ to be expanded
only when the macrolet ~getter~ expands.

So what if we build the binding form of ~cl-macrolet~ beforehand?

#+BEGIN_SRC elisp
(defmacro alist-let (getter alist &rest body)
  (declare (indent 2))
  (let* ((alist-a (gensym))
         (str (read (format "`(cdr (assq ,field %s))" alist-a))))
    `(let ((,alist-a ,alist))
       (cl-macrolet ((,getter (field) ,str))
         ,@body))))

(cl-prettyexpand
 '(alist-let a. '((a . 1) (b . 2))
    (a. 'a)))

(let ((G23159 '((a . 1) (b . 2))))
  (progn
    (cdr (assq 'a G23159))))
#+END_SRC

Ah!  It works.  Gosh.  I spent hours trying to workaround that, and even tried
the string route with ~eval~.  I was so tired that I did not bother taking notes
as I went along, but taking notes would have forced me to work out what was
wrong in the approach.

Anyway, there is still the downside of not saving all these lookups into a let.

But elisp has one last surprise in store for me!  When I try to nest ~alist-let~
calls:

#+BEGIN_SRC elisp
(alist-let a. '((a . 1) (b . 2))
  (alist-let b. '((c . 3) (d . 4))
    (b. 'c)))
;; => Symbol's value as a variable is void: G23170
#+END_SRC

Damned!  How does the expansion looks like?

#+BEGIN_SRC elisp
(cl-prettyexpand
'(alist-let a. '((a . 1) (b . 2))
  (alist-let b. '((c . 3) (d . 4))
   (b. 'c))))

(let ((G23198 '((a . 1) (b . 2))))
  (progn
    (let ((G23199 '((c . 3) (d . 4))))
      (progn
        (cdr (assq 'c G23199))))))
#+END_SRC

#+RESULTS:
: 3

Wait, it works?  What the hell?  Expansion works, but not evaluation?

Actually, it also fails without nesting:

#+BEGIN_SRC elisp
(defmacro alist-let (getter alist &rest body)
  (declare (indent 2))
  (let* ((alist-a (gensym))
         (str (read (format "`(cdr (assq ,field %s))" alist-a))))
    `(let ((,alist-a ,alist))
       (cl-macrolet ((,getter (field) ,str))
         ,@body))))

(alist-let a. '((a . 1) (b . 2))
  (a. 'b))
;; => Symbol's value as a variable is void: G23199
#+END_SRC

Now, a clue might be that even a simple ~macroexpand~ also fails:

#+BEGIN_SRC elisp
(macroexpand
 (alist-let a. '((a . 1) (b . 2))
     (a. 'b)))
;; => Symbol's value as a variable is void: G23200
#+END_SRC

Whereas ~cl-prettyexpand~ does not.  I've read somewhere that CommonLisp has
different macro expansion semantics than Elisp.  The ~cl-~ prefix stands for
CommonLisp.  Coincidence?

Well, macrostep-mode seems to agree with me:

#+BEGIN_SRC elisp
(let
    ((G24180 '((a . 1) (b . 2))))
  (cl-macrolet
      ((a\. (field) `(cdr (assq ,field G24180))))
    (a\. 'b)))
#+END_SRC

then:

#+BEGIN_SRC lisp
(let
    ((G24180 '((a . 1) (b . 2))))
  (progn
    (cdr (assq 'b G24180))))
#+END_SRC

So if the expansion under CommonLisp rules is not the same as under the Elisp
rules, I get a working expression but evaluating the expression without
expanding it first fails.  Still, it's weird that macrostep concurs with
~cl-prettyexpand~ then.

But the debugger reveals that is not the correct explanation.  If I 'C-x C-e'
the expression:

#+BEGIN_SRC elisp
(alist-let a. '((a . 1) (b . 2))
  (a. 'b))
#+END_SRC

Here is what the debugger says:

#+BEGIN_SRC elisp
Debugger entered--Lisp error: (void-variable G56132)
  (assq (quote b) G56132)
  (cdr (assq (quote b) G56132))
  (progn (cdr (assq (quote b) G56132)))
  (let ((G56132 (quote ((a . 1) (b . 2))))) (progn (cdr (assq (quote b) G56132))))
  eval((let ((G56132 (quote ((a . 1) (b . 2))))) (progn (cdr (assq (quote b) G56132)))) nil)
#+END_SRC

So actually, the debugger sees exactly what ~cl-prettyexpand~ gives me.  But for
some reason, it claims to fail evaluating that?

If I execute myself:

#+BEGIN_SRC elisp
(let ((G56132 (quote ((a . 1) (b . 2))))) (progn (cdr (assq (quote b) G56132))))
#+END_SRC

#+RESULTS:
: 2

It works.  Grumble.

I think I need to consult an expert.

Trying one more thing.  I read on the emacs-devel thread announcing ~let-alist~
that ~make-symbol~ is to be preferred to ~gensym~.  Don't know why.  Does it
make a difference here?

#+BEGIN_SRC elisp
(defmacro alist-let (getter alist &rest body)
  (declare (indent 2))
  (let* ((alist-a (make-symbol "alist"))
         (str (read (format "`(cdr (assq ,field %s))" alist-a))))
    `(let ((,alist-a ,alist))
       (cl-macrolet ((,getter (field) ,str))
         ,@body))))

(cl-prettyexpand
 '(alist-let a. '((a . 1) (b . 2))
   (a. 'b)))

(let ((alist '((a . 1) (b . 2))))
  (progn
    (cdr (assq 'b alist))))


(alist-let a. '((a . 1) (b . 2))
           (a. 'b))
;; => Error
#+END_SRC

Nope.

I want to get to the bottom of this.  Let's make a minimal test case.

This still fails, but executing the expansion does not:

#+BEGIN_SRC elisp
(defmacro test-gensym ()
  (let* ((sym (gensym))
         (sexp (read (format "%s" sym))))
    `(let ((,sym 12))
       ,sexp)))

(cl-prettyexpand
 '(test-gensym))

(let ((G57457 12))
  G57457) ; => 12

(test-gensym) ; => Error
#+END_SRC

While the very similar:

#+BEGIN_SRC elisp
(defmacro test-gensym ()
  (let* ((sym (gensym))
         (sexp `,sym))
    `(let ((,sym 12))
       ,sexp)))
#+END_SRC

works.

So I'm assuming that reading a symbol makes it different somehow?

#+BEGIN_SRC elisp
(let (sym (gensym))
  (eq sym `,sym)) ; => t

(let (sym (gensym))
  (eq sym (read (format "%s" sym)))) ; => t
#+END_SRC

Nope.  Weird.

Still:

#+BEGIN_SRC elisp
(defmacro test-gensym ()
  (let* ((sym (gensym))
         (read-sym (read (format "%s" sym))))
    `(let ((,sym 42))
       ,sym)))

(cl-prettyexpand
 '(test-gensym))

(let ((G57812 42))
  G57812) ; => 42

(test-gensym) ; => 42
#+END_SRC

Reading about gensym in the manual, I find a related gentemp that creates an
interned symbol.  With gentemp, it works:

#+BEGIN_SRC elisp
(defmacro test-gensym ()
  (let* ((sym (gentemp))
         (read-sym (read (format "%s" sym))))
    `(let ((,sym 42))
       ,read-sym)))

(test-gensym) ; => 42
#+END_SRC

Does interning means we can still have name clashes?  If I know G57833 to be the
next generated symbol?

#+BEGIN_SRC elisp
(defmacro test-gensym (&rest body)
  (let* ((sym (gentemp)))
    `(let ((,sym 42))
       ,@body)))

(test-gensym G57833) ; => error G57833 not found
#+END_SRC

Hmm.  Actually, gentemp keeps increasing when the symbol exists.  So it's
guaranteed to be fresh.

Does this mean...?

#+BEGIN_SRC elisp :results verbatim
(defmacro alist-let (getter alist &rest body)
  (declare (indent 2))
  (let* ((alist-name (gentemp))
         (str (read (format "`(cdr (assq ,field %s))" alist-name))))
    `(let ((,alist-name ,alist))
       (cl-macrolet ((,getter (field) ,str))
         ,@body))))

(alist-let a. '((a . 1) (b . 2))
  (alist-let b. '((c . 3) (d . 4))
    (list (a. 'a) (b. 'c))))
#+END_SRC

#+RESULTS:
: (1 3)

Yes!  YES!

So it:
- provides a shortcut for getting values out of alists
- can be nested
- is hygienic

The only downside is that each call to ~(a. 'a)~ expands to ~(cdr (assq~, which
maybe you want to avoid.  But, the good news is, if you want to save the result,
you can do it YOURSELF:

#+BEGIN_SRC elisp :results verbatim
(defmacro alist-let (getter alist &rest body)
  (declare (indent 2))
  (let* ((alist-name (gentemp))
         (str (read (format "`(cdr (assq ,field %s))" alist-name))))
    `(let ((,alist-name ,alist))
       (cl-macrolet ((,getter (field) ,str))
         ,@body))))

(cl-prettyexpand
'(alist-let a. '((a . 1) (b . 2))
  (alist-let b. '((c . 3) (d . 4))
    (let ((val (a. 'a)))
     (list val val (b. 'c))))))

(let ((G57854 '((a . 1) (b . 2))))
  (progn
    (let ((G57855 '((c . 3) (d . 4))))
      (progn
        (let ((val (cdr (assq 'a G57854))))
          (list val val (cdr (assq 'c G57855))))))))
#+END_SRC

#+RESULTS:
(1 1 3)

Considering that to have results auto-memoize you would need to do a tree
recursion when macro expanding... which led to one of the (fixable) issue with
~let-alist~... I'd say it's not worth it.  I prefer to be explicit anyway.

*** Another alternative to let-alist
The [[http://www.greghendershott.com/fear-of-macros/pattern-matching.html#%2528part._hash..refs%2529][second link]] Ronan gave me was about a Racket macro with a very similar goal.
Given a parsed JSON 'js' (in Racket, these are parsed to hashmaps), we can get a
value out of it using dot notation:

: (hash.refs js.a.b.c)

which is just syntactic sugar for:

: (hash-refs js '(a b c))

which is just a function.

Maybe following this route would be easier for an alternative to ~let-alist~
work.  But you really want a zero-cost abstraction, so using a macro to provide
syntactic sugar for a function call is a no go.  You need to generate the code
for lookups at compile time.

*** Destructuring using dash
Ronan also found that there is alist destructuring in the latest dash (2.13).

#+BEGIN_SRC elisp
(-let [(&alist 'a a) '((a . 1) (b . 2))]
  a)
#+END_SRC

#+RESULTS:
: 1

And to get values from nested alists:

#+BEGIN_SRC elisp
(-let [(&alist
        'a a
        'c (&alist 'ca ca))
       '((a . 1) (b . 2) (c . ((ca . 3))))]
  ca)
#+END_SRC

#+RESULTS:
: 3

So this would definitely work.  You just have to destructure in advance.

*** Silence the warnings
Otherwise, since the issue is that nested ~let-alist~ calls issue a warning when
byte-compiling, but the expanded code is actually harmless, why not just silence
the warnings?  ~with-no-warnings~ silence warnings in its body.  The problem is
it disables /all/ warnings, and not just the unused variables.

* [2016-08-28 dim.]
** Compiling chipers on Windows                                     :chipers:
Following the instructions on rust-lang, `cargo update` to update the
dependencies, and it failed to build in my code.

Turns out imgui updated to use glium 0.15 now, and I was using 0.14, which
caused hairy "unsatisfied trait bound" errors since we weren't targeting the
same code... sigh.

Anyway, after that and a trivial arguments update, it built.

** Compiling rust windows binaries from linux
Can I cross-compile easily?

[[https://github.com/rust-lang-nursery/rustup.rs#cross-compilation][Let's see]].

: rustup target add x86_64-pc-windows-msvc
: rustup run stable cargo build --release --target=x86_64-pc-windows-msvc

Argh, an error because std was not compiled with the correct version.  Hmm, this
looks like a deep rabbit hole I don't want to go into right now.  Here is [[#+BEGIN_EXAMPLE][a
guide]] for the moment.

* [2016-08-29 lun.]
** Low-resolution rendering in OpenGL
To achieve a more low-tech look.  [[https://stackoverflow.com/questions/7071090/low-resolution-in-opengl-to-mimic-older-games][This SO answer]] seems to be it.  Just have to
find out how to do the same thing in glium.

* [2016-08-31 mer.]
** Reading up on how modern (>1) OpenGL works
My knowledge of OpenGL is largely based on direct mode.  Pre-shaders.

The r/opengl subreddit has a few links to get started.

Here is [[http://duriansoftware.com/joe/An-intro-to-modern-OpenGL.-Table-of-Contents.html][the one]] I'm reading right now.

It confirms that framebuffer objects are just target to render to.  I could
render to several different FBOs without directly rendering to the screen.
But how to render an FBO to the screen?  By using it as texture and drawing it
on a quad?

Also, I did not understand how to combine shaders or do "multiple pass" of
rendering in chipers.  I just baked two fragment shaders in one.  But it seems
we can actually have multiple draw calls, with potentially different programs
(hence, different shaders, or even buffer objects).  How do the multiple draw
calls compose however?

* [2016-09-03 sam.]
** Experimenting with multiple draw calls                     :spacebangbang:
So if I understand the pipeline correctly, a single Frame.draw call takes a
vertex buffer and a shader program.  I get that a vertex shader will only work
with the vertices from the vertex buffer.  Presumably, if the vertex buffer has
4 vertices, there will be 4 instances of the vertex shader running in parallel
to produce a ~gl_Position~.  Then, using the index buffer, it constructs
triangles, rasterizes them, and now the fragment shader is called for every
pixel in these triangles.

We can use a fragment shader like this to show that is the case:

: color = vec4(gl_FragCoord.x / 2000, gl_FragCoord.y / 2000, 0.0, 1.0);

every pixel of the triangle has a different color, depending on its position on
the screen.

** Tools for browsing documentation locally
Discovered [[https://zealdocs.org/download.html][Zeal]] to browse documentation à la Devdocs, but Devdocs did not have
OpenGL documentation :(

Zeal actually uses the same documentation format than Dash.  There are plenty of
frontends to read these formats.  I tried a terminal-based one (dasht), did not
get anything inside the terminal when looking for ~gl_FragCoord~, and the
browser opening did not work either.  I was enthusiast for the Helm-based one,
but only the search works correctly from inside Emacs, the viewing should happen
in the browser (using eww does not work for XML file apparently).  Zeal is a
dedicated GUI for local docs, and it's fast.  Good enough.

** A potential rotation bug                                   :spacebangbang:
By keeping the current heading in a float, we can potentially grow to very large
value by holding one direction, and then hitting float inaccuracies.

Testing, after 2^20 you cannot turn left anymore.  Admittedly, you would take a
rather long time to get there, but hey.  Still incorrect.

Using a small integer that wraps around you are guaranteed to avoid rounding
errors.

** Vsync on my machine                                        :spacebangbang:
Ah!  I did not enable vsync in the application, but I still obviously got
60fps.  Adding the frame period histogram confirmed it.

So I double-check my nvidia settings to confirm I had disabled vsync, yes I did.
But hey, after quitting the nvidia settings application, now when I launch my
glium window vsync is indeed off (avg frame period: 0.2 ms, nice).

So on my machine somehow my nvidia settings are not taken into account until I
launch the application once.

Still, better enable vsync for this application until I make the logic
independent of frame rate.

** Low-resolution render on framebuffer                       :spacebangbang:
So that's what I had in mind [[*Low-resolution rendering in OpenGL][the other day]].  A low-resolution effect.  Now that
I understand how this ties together a bit better, I can explain how it works.

Instead of rendering directly to the framebuffer that is presented to the
screen, we draw the scene to a framebuffer object of a lower resolution.  The
framebuffer tied to the screen has as many pixels as the window contains, but
the framebuffer object has the resolution we want.  Creating a framebuffer that
renders to a texture is rather simple in Glium:

: SimpleFrameBuffer::new(&display, &texture);

Of course we need a texture, so I reused the code from chipers:

#+BEGIN_EXAMPLE
  let texture = Texture2d::empty_with_format(&display,
                                             UncompressedFloatFormat::U8,
                                             MipmapsOption::NoMipmap,
                                             256, 256).unwrap();
#+END_EXAMPLE

Here we draw on a 256*256 virtual screen, the same resolution as the original
Macintosh.

Then it's just a matter of drawing the ship on this virtual framebuffer, then
drawing the texture on a quad to the screen framebuffer.

Of course we need different shaders for the quad, a vertex buffer, index
buffer...  It's a bit verbose, but it works!

Hmm... why is the ship red?  Shouldn't it be white?

Ah, it's the ~UncompressedFloatFormat::U8~: it stores only the first color
component, which is red.  I used a single u8 for chipers because the texture
there as a boolean: either the pixel is on, or it is off.  Switching to an
U8U8U8U8 fixes the redness.

** TODO make simulation independent of frame rate             :spacebangbang:
** DONE import ship model from Blender                        :spacebangbang:
CLOSED: [2016-09-11 dim. 15:10]

* [2016-09-04 dim.]
*** DONE Maintain aspect ratio                                :spacebangbang:
CLOSED: [2016-09-04 dim. 15:33]
I always have trouble will all these matrix multiplications that pile onto each
other.  Anyway, the strategy here has two part:

First, we adjust the model coordinates for the aspect ratio of the virtual
framebuffer.  The intent is that, regardless of the dimensions of the
framebuffer, the ship is always the same size/shape.

But that's not all, because when drawing the framebuffer to the quad, the quad
is still stretched out to fill the window.  So there also we have to stretch the
quad so that it fills the window, while maintaining its aspect ratio.  And
that's another matrix multiplication.

*** DONE Add tweakable gameplay values to GUI                 :spacebangbang:
CLOSED: [2016-09-04 dim. 16:43]

* [2016-09-05 lun.]
** Making a note of a nice way to deal with opcodes in Rust             :gbs:
As done in [[https://github.com/yupferris/rustendo64/blob/master/src/n64/cpu/opcode.rs][Rustendo]], a Nintendo 64 emulator in Rust.  Opcodes are transformed
into an enum:

#+BEGIN_SRC rust
pub enum Opcode {
        Special = 0b000000,
        RegImm =  0b000001,

        Addi =    0b001000,
Addiu = 0b001001,
#+END_SRC

so the code that executes these opcodes can match on enum variants, rather than
binary:

#+BEGIN_SRC rust
match instr.opcode() {
  RegImm =>
  Addi =>
  Addiu =>
#+END_SRC

I'm sure this emulator has other Rust tricks as well.  Good source.

* [2016-09-22 jeu.]
** Trying to use the ~keyboard-layout~ layer of spacemacs             :emacs:
As suggested by [[https://github.com/syl20bnr/spacemacs/issues/6631][issue 6631]].

I tried to add a simple Colemak substitution, JKHL instead of HJKL.  It works
for the normal evil mode and Helm, but for some reason not in magit.  Actually
the code for magit does not look up to date, since there is no
~evil-magit-map~.  Also, it's macros all the way down, and there are no
docstrings :(

** Finding the keymap where a key is bound                            :emacs:
~describe-key~ seems to take care of it.

* [2016-09-23 ven.]
** Diving into a smartparens bug for rust-mode                  :smartparens:
Namely [[https://github.com/Fuco1/smartparens/issues/642][issue 642]].

Maybe this line?

: (sp--do-action-p (sp-get active-sexp :op) 'autoskip)

Instrumenting the function with edebug, when I insert the closing bracket ">", I
would expect this expression to be ~t~, but it's nil.

Can I change the return value of something in Edebug?  Hmm looks like I can only
inspect.

Well, returning ~t~ here indeed fixes the issue.  So why doesn't
~sp--do-action-p~ return ~t~ here?

Okay it seems that the fault lies with the predicate that determines whether we
should consider the character as a pair, ~sp-rust-could-be-parameterized~:

#+BEGIN_SRC elisp
(defun sp-rust-could-be-parameterized (&rest args)
  "Return t if we could add a <T> in this position.
If nil, the user is probably using < for something else."
  (and (apply #'sp-in-code-p args)
       (looking-back (rx (or letter (seq letter "<") (seq letter "::<"))))))
#+END_SRC

Hmm, ~rx~ is a macro to make readable regexps.  Yeah, another DSL!

Okay so if I read that correctly,

: (rx (or letter (seq letter "<") (seq letter "::<"))))))

this corresponds to the following cases:

: letter -> A|
: seq letter "<" -> A<|
: seq letter "::<" -> A::<|

In all these cases, when the cursor at | inserts an opening bracket "<", this
will insert a closing bracket ">".

But!  The same predicate is also used for auto-skipping the closing bracket, and
in that case the regexp fails.

Hmm wait, actually the first case will match:

: <T|>

there's a letter, 'T'.

So what's happening?

Ah! That's because autoskip let the user input the closing bracket, then try to
determine if it should remove it, thus auto-skipping transparently for the user.
But when testing for removal, this is the current code:

: <T>|>

and there are no regexp cases that match.

One way to fix that is to add a ">" case to the regexp.

** Writing an ERT test for this smartparens bug                 :smartparens:
Okay, now that it's fixed, it might be good idea to leave a test there to ensure
this does not pop up again.

Hmm running `make test` fails because of ruby tests... not my problem.  How do I
run only rust tests?

: cake exec ert-runner -p rust

Great.  Now, reading the existing tests, this looks pretty straightforward.
Done.

* [2016-09-26 lun.]
** A Github burndown chart                                         :burndown:
Motivated by answering a simple question: how is a project catching up with its
maintenance load?  Specifically, I always see that Spacemacs has >1000 open
issues and that scares me a bit.

Github has a [[https://github.com/syl20bnr/spacemacs/pulse][pulse page]] that shows how many pull requests were created/merged
in the last day/week/month.  Same for number of closed issues versus new ones.

That's one data point for the burndown chart.  But it's not the whole picture,
since you only see as far as one month back.  Also, you can't see the four weeks
of the month separately without to see the trends.

So at the very least I would like to see: how many issues were created and
closed in any given week from the birth of the project.  Same with PR.  I see
that PR are issues in the Github API, so I don't even have to duplicate the work
here!

For that, I just need to get the list of issues, their creation date and closed
date.

Do I need authentication to the Github API?  Spacemacs has 7200 total issues/PR.
We can get a max of 100 issues per call, so 72 calls to get everything, but the
rate limit is 60 calls per hour.  So I think I need an access.

Okay so I just need to get a personal authorization token from the settings
page.  Then I can pass it as a header to the request like so:

: curl -H "Authorization: token 1234..." -I https://api.github.com/users/fmdkdd

: X-RateLimit-Limit: 5000
: X-RateLimit-Remaining: 4999

That should be enough.

Also, I should include my username as user agent as [[https://developer.github.com/v3/#rate-limiting][per the docs]].

So I need something to get these numbers from a given repository.  That involves
going through each page of results and extracting the info I need.  And probably
do some caching to avoid hammering the Github API.

Then I need to do a visualization using these results.  Leaning towards D3.

Okay as a proof of concept I got all the issues for Spacemacs as a JSON file.
Now I can try a D3 visual.

Hmm D3 has finally landed v4, does that change anything important?  Well OK,
according to [[https://medium.com/@mbostock/what-makes-software-good-943557f8a488#.494ksjbbg][this post]], I only need to be aware of selection.merge.

I think I need to massage the data a bit.  I want to draw two areas: one for the
total number of open issues as a function of time, and another for the total
number of closed issues through time as well.  So, I need to sort the array by
creation date, go through each issue and emit a point for this date with the
total number of open and closed issues at this date.

* [2016-09-27 mar.]
** Trying to extract text from a bunch of game screenshots
Apparently [[https://github.com/tesseract-ocr/tesseract][Tesseract]] is good.

It automatically does image treatment, meaning I don't have to fiddle with gimp.
I mostly need to crop, and since this is always the same area, this can be done
with imagemagick in one fell swoop.

One issue though is that the screenshots are quite low-res, especially on the
text.  Tesseract seems more used to 300 dpi texts (paper scans).

The output is okay, but there are characters like 'j' it has trouble with.
Also, the text in the screenshot has a low line-height, so it occasionally
misses diacritics or capitals.

One thing to try is to train it on the specific font used.  Apparently, this
game uses Arial Narrow.

Okay, training was not very successful.  Maybe if I add a word list?  Hmm,
extracting data from the fra set was only slightly better.  But using the fra
set directly yields the best results.

Page segmentation method 6 seems to yield the best results as well.  Now I will
try to find how to best massage the original image.

Cropping + scaling with cubic interpolation to 400% seems good enough.

Using ImageMagick, the Catmull-Rom interpolation has better contrast.  Toying
with image size now.  Interestingly, smaller images are not always faster to
process by tesseract.  I'm assuming because Leptonica has a harder time finding
a good threshold value.  However, larger images will not automatically result in
better OCR recognition.

400% and 600% are both quite accurate.

However, I failed to filter out specific lines based on their text colors.  I
think I would need more control over the image, maybe by using OpenCV or
assorted tools.

** Going back to the burndown chart                                :burndown:
Outputting an SVG circle for every (7192) issue is clearly not the right way to
go about it.  I wanted to use SVG paths.  d3.area handles that nicely; I just
had to find an example with the updated v4 syntax.

Also, adding axes is a breeze.

Now, I guess I could add a text input for the repo name it could be a website on
its own.  But I would need to add caching for the results in the backend.

Meaning, I need to host the backend somewhere.  But I don't want to pay for a
server.  Are there any free hosting for apps anymore?

The graphs are interesting to look at.  You can clearly see Spacemacs is doomed
if the current trend is maintained.  A high number of open issues may just
indicate activity, but if the number of open issues keeps growing, you might
call that maintenance bankruptcy.

I mean, I just ran the same viz on flycheck, and there you can see a trend of
unclosed issues picking up early 2016, but reversing around july.  Also, the
number of open issues never went above 60.

Oh yes!  I also need to maybe separate issues and pull requests.  Same graphs,
but different colors.

* [2016-09-28 mer.]
** Finding a host provider for burndown chart                      :burndown:
Hmm, seems Heroku is not as gracious as it once was.  Can't seem to find a
decent free service for small applications that are not also utterly useless.

I can't splurge for a 4€/month commitment, especially for small demo.
Otherwise, hetzner.de looks good.  Or OVH has a very nice VPS at 3.6€/month.

But first things first, adding local caching.  I suppose the nodeJS part could
act as a simple fetcher of issue data for a given repo.  Given the user and repo
names, it would fetch all issue data since the latest sync, and merge that into
a DB.

Then this DB can be queried directly by the D3 page to generate the graphs.

Ultimately, the page will be the one to initiate the sync.

Also, why use a DB, when I can use perfectly reasonable JSON files?

** Caching version done                                            :burndown:
I've now got a nice little tool that fetches issues from Github and saves them
to a file.  When the file already exists, it only gets the newest issues.

I found a good argument for a DB: for dealing with lock and concurrent accesses
to a file.  Clearly, I've thought of the program as a command-line tool, not as
a web server.

Well, I prefer the CLI approach anyway.  I guess that means I don't need all
these callbacks.  Sync all the things!

Well, actually, since I need to deal with the errors anyway, I prefer handling
them as callbacks than exceptions.  This way the code stays asynchronous if need
be.  Although I will try to see if promises would make it nicer.

** Using promises                                                  :burndown:
Okaaaay.  Using promises was not straightforward.  I clearly don't grasp the
idioms yet.  I might need to implement the Promises/A+ or look at an
implementation to understand what's going on.

But I managed to make it work.  Is the resulting code clearer?  It got rid of
~if (err) bail~ calls, and now all errors are caught in one place, which is a
plus.  It does make the control flow a bit awkward, with ~P.join~ in one place.
But maybe there's a better way to write it.

** Making an app                                                   :burndown:
So now the good thing would be to add a text input to the web page, and fetch
the corresponding issues.  Or a file selector, and drag and drop.

* [2016-10-03 lun.]
** Fixing a bug with macro errors in the JSON parser for flycheck errors :flycheck:
Seems macro errors are not captured by the JSON parser actually.

See flycheck/flycheck-rust/issues/36.

With macro errors, like forgetting an argument to println:

: println!("{}")

the JSON output is pretty big, and contains multiple nested expansion errors,
like so:

#+BEGIN_EXAMPLE
{
  "message": "invalid reference to argument `0` (no arguments given)",
  "code": null,
  "level": "error",
  "spans": [
    {
      "file_name": "<std macros>",
      "is_primary": true,
      "text": [{
          "text": "( $ fmt : expr ) => ( print ! ( concat ! ( $ fmt , \"\\n\" ) ) ) ; (",
        }],
      "expansion": {
        "span": {
          "file_name": "<std macros>",
          "text": [{
              "text": "( $ fmt : expr ) => ( print ! ( concat ! ( $ fmt , \"\\n\" ) ) ) ; (",
            }],
          "expansion": {
            "span": {
              "file_name": "<std macros>",
              "text": [{
                  "text": "$ crate :: io :: _print ( format_args ! ( $ ( $ arg ) * ) ) ) ;",
                }],
              "expansion": {
                "span": {
                  "file_name": "<std macros>",
                  "text": [{
                      "text": "( $ fmt : expr ) => ( print ! ( concat ! ( $ fmt , \"\\n\" ) ) ) ; (",
                    }],
                  "expansion": {
                    "span": {
                      "file_name": "lib.rs",
                      "text": [{
                          "text": "    println!(\"{}\");",
                        }],
                      "expansion": null
                    },
                    "macro_decl_name": "println!",
                    "def_site_span": {
                      "file_name": "<std macros>",
                      "text": [{
                          "text": "( $ fmt : expr ) => ( print ! ( concat ! ( $ fmt , \"\\n\" ) ) ) ; (",
                        }, {
                          "text": "$ fmt : expr , $ ( $ arg : tt ) * ) => (",
                        }, {
                          "text": "print ! ( concat ! ( $ fmt , \"\\n\" ) , $ ( $ arg ) * ) ) ;",
                        }],
                      "expansion": null
                    }
                  }
                },
                "macro_decl_name": "print!",
                "def_site_span": {
                  "file_name": "<std macros>",
                  "text": [{
                      "text": "( $ ( $ arg : tt ) * ) => (",
                    }, {
                      "text": "$ crate :: io :: _print ( format_args ! ( $ ( $ arg ) * ) ) ) ;",
                    }
                  ],
                  "expansion": null
                }
              }
            },
            "macro_decl_name": "format_args!",
            "def_site_span": null
          }
        },
        "macro_decl_name": "concat!",
        "def_site_span": null
#+END_EXAMPLE

But most of it is tossed away in the pretty compiler output:

#+BEGIN_EXAMPLE
error: invalid reference to argument `0` (no arguments given)
 --> <std macros>:1:33
  |
1 | ( $ fmt : expr ) => ( print ! ( concat ! ( $ fmt , "\n" ) ) ) ; (
  |                                 ^^^^^^^^^^^^^^^^^^^^^^^^^
<std macros>:1:33: 1:58 note: in this expansion of concat!
<std macros>:2:27: 2:58 note: in this expansion of format_args!
<std macros>:1:23: 1:60 note: in this expansion of print! (defined in <std macros>)
lib.rs:6:5: 6:20 note: in this expansion of println! (defined in <std macros>)
#+END_EXAMPLE

So hmm, two things:

- ~file_name~ is not always a file name... it can be "<std macros>"; we should
  probably avoid creating flycheck-error objects for these

- a span can have an ~expansion~ field which can contain a span, which can
  contain an expansion...

So we should get all these spans, discard the ones originating in "<std macros>"
files, and produce flycheck-errors for the others.

Argh.  So I got a workaround up.  But macro errors basically use the JSON error
output differently.  They do not use "label" (a string) but "text" (an array of
objects, of which "text", which is a string).  The spans in expansions are not
primary but they actually contain the expansion failures in a form of stack
trace.

So when walking the JSON error recursively, we need to know if we are in an
"expansion span" or not, to know what to extract in each case.  Might be
worthwhile to write different parsing functions for these cases.

* [2016-10-04 mar.]
** Fixing the parsing bug for macro errors in flycheck (cont.)     :flycheck:
To simplify the logic, it might be best to collect the "causes" (filename,
line/column info) first, then build the flycheck error objects afterwards.

* [2016-10-06 jeu.]
** Revamping flycheck-rust                                         :flycheck:
The goal of flycheck-rust is to set the flycheck variables used by the rust and
rust-cargo checkers automatically.

That is, the flycheck checkers' job is to run a command (rustc, or cargo), to
generate errors, parse them, and then let flycheck handle presenting them to the
user.  Importantly, flycheck checks /files/, not /projects/.  Flycheck has no
knowledge of a project hierarchy.  When visiting a file, it determines which
checkers it should enable, and when they should run.  For rust, variables can be
used to tell flycheck whether ~rust~ or ~rust-cargo~ should be enabled, and
pieces of the command to run.

The job of flycheck-rust is to set these variables automatically, based on the
currently visited file.  And for that, we need, for each rust file, to determine
which variables to set, what values they should have, in order to communicate
with the checker.

There are two checkers, ~rust~ and ~rust-cargo~.  These are their predicates:

For rust:

#+BEGIN_SRC elisp
  :predicate (lambda ()
               (and (not flycheck-rust-crate-root) (flycheck-buffer-saved-p))))
#+END_SRC

it kicks in when the current buffer has no crate root set.  Implicitly, it means
that whenever we find a cargo file, we assume the rust-cargo checker is a better
fit.

The rust-cargo predicate is:

#+BEGIN_SRC elisp
  :predicate (lambda ()
               (and (flycheck-buffer-saved-p)
                    (locate-dominating-file (buffer-file-name) "Cargo.toml"))))
#+END_SRC

Curiously, they do not use the same variable.  So, amusingly, if you set
~flycheck-rust-crate-root~ to nil, you can actually enable both checkers at the
same time.

But with ~flycheck-rust~ enabled, the variable should be non-nil when the
Cargo.toml file is present, so they amount to the same.  Still, weird that we
lookup the file manually.

One complication in flycheck-rust is that there may be many different files, not
just source files but tests, examples, benchmarks...

See http://doc.crates.io/guide.html#project-layout and
http://doc.crates.io/manifest.html#the-project-layout

But I think ~cargo read-manifest~ should now cover all these cases.  Building a
test crate now.  Yeah, it seems read-manifest has us covered when it comes to
finding actual build targets.

There are some added subtleties when it comes to additional source files.  The
convention is that every files in ~src/~ will contribute to the library target
(that is, be included by ~src/lib.rs~ directly or recursively), and the binary
targets (either ~src/main.rs~ or files in ~src/bin~) will import the library.
Clearly, the binary files could also import support files that are not part of
the library, but then as these files are imported by a build target, they should
report build errors.

The same goes for support files used by examples, benches, or test targets.
However, we can imagine dropping a rust file that is not imported by any
target.  This is unconventional, but flycheck's job is to check it.  We could
fallback on the rust checker here and it should work.  The problem is how do we
/know/ the target the file is associated with?

Cargo doesn't tell us.  In fact, there could be multiple targets associated to a
file.  For instance, ~src/lib.rs~ is needed by the ~lib~ target, but also by the
~bin~ target.  How do we know which target to build then?  To get errors, any
target would do, but the correct answer is to get the smallest target that
contains our file.

So that seems out of scope for flycheck.  I think we can safely guess
"conventional" targets:

- ~src/main.rs~ is for the main binary target
- ~src/bin/x.rs~ for the the 'x' binary target
- any other file under ~src/~ is for the library target (if it exists),
  otherwise the main binary target

- ~tests/x.rs~ is for the ~x~ test target
- ~benches/x.rs~ is for the ~x~ bench target
- ~examples/x.rs~ is for the ~x~ example target

Now, any files in subdirectories of tests, benches and examples are not picked
up as targets, but can be used by them.  Problem: if there's a file
~tests/common/a.rs~, and multiple test targets, which target do we choose?

Hmm, seems we still want an answer to that question from cargo.

Okay, can we still get meaningful errors from the file even if we don't know the
target it will be used for?  After all, the target is required when /building/,
but to get compilation errors maybe just starting from the file is okay?

It seems we /can/ use plain ~rustc~ on files to get compilation errors, but
troubles arise when there are dependencies on external crates.  Rustc alone
won't fetch the dependencies, because that's cargo's job.

So I'm in favor of embracing the way cargo works, rather than working around it.
That means: either cargo gives us a way to get compilation errors for a specific
file, managing dependencies, or we use cargo to build targets rather than files,
and deal with it.

In the second option, we could use flycheck variables to set the target in some
edge cases if none can be found.

I'll try the second option first.

** Using cargo to build targets                                    :flycheck:
There is one canonical way to build things with cargo: ~cargo build~.

We can determine the current target automatically for some files, using
heuristics for others, and using buffer-local variables for the rest.

Once we have the target, it's a matter of passing it to ~cargo build~ and we
will get errors.

BUT, to get errors in the JSON format with ~cargo build~ we need to pass the
option to ~rustc~ via the environment variable ~RUSTFLAGS~.  The problem is,
changing ~RUSTFLAGS~ will re-trigger a full recompilation, even if it's
unnecessary in this case since ~--error-format~ does not change the code.

That's not an issue in successive calls by flycheck, but if it's intermingled
with calls by the user to ~cargo run~ or ~cargo test~, then it will trigger a
full rebuild of the crate.

Another option is to wait for [[https://github.com/rust-lang/cargo/pull/3000][cargo#3000]] to land, where we can safely ask for
the JSON format without re-triggering compilation.  But we cannot use
~no-trans~, which speeds up the recompilation to get error feedback.

Yet another option, that we currently use, is to use ~cargo rustc~ directly.
There we can safely pass the ~--error-format~ option without triggering
recompilation, and we can specify targets in the same way as ~cargo build~.

One advantage of using ~cargo rustc~ is that the ~no-trans~ options seems to be
applied only to the target itself, whereas with RUSTFLAGS it seems to apply to
all invocations of ~rustc~ made by cargo, and it fails miserably to build a full
crate.

So the new plan is: use ~cargo rustc~ for now, try to match files files to build
targets exactly first, then using heuristics based on the conventional cargo
layout, then let the user use the buffer-local variables to specify the target.

* [2016-10-07 ven.]
** Adding tests to flycheck-rust                                   :flycheck:
Since we have many different crate types and targets, it makes sense to add
tests to catch them all and avoid regressions.

Should I use ERT or buttercup?  Let's try buttercup first, as it is presumably
easier to setup and use.

Yep, wasn't hard.

** Fallback strategy for non-exact file/target matches             :flycheck:
In the conventional cargo layout, it seems we can try to match the file by
stripping off directories:

'test/support/a.rs' won't match any target exactly, so try to match targets
beginning with 'test/support' (no match), then 'test/' (multiple matches);
assume the first one.  Stop at the project root.

** Cannot pass error-format to ~cargo rustc~                       :flycheck:
AAARGH.  Passing options to ~cargo rustc~ will only apply to the given target,
not to dependencies.  So if main depends on lib, and we build main, but lib has
an error, we will get errors as plain text, not in JSON...

Ok ok.  RUSTFLAGS it is then until ~--message-format~ stabilizes.  But RUSTFLAGS
will trigger recompilation.  Hmmm.

* [2016-10-12 mer.]
** Updating Spacemacs to 0.200                                        :emacs:
Especially dreadful, updating colemak-hjkl.

Seems the issues I was encountering with keyboard-layout are also present
there.  Hmm, maybe it would be worth it to complete the colemak support for
keyboard-layout rather than duplicating the work?

* [2016-10-12 mer.]
** Updating colemak-hjkl for Spacemacs 0.200                          :emacs:
Argh, the new transient maps with hydra are less easily redefined than the
micro-states.

Redefining the transient state with the ~spacemacs|define-transient-state~ macro
does not work.

Redefining the keys directly inside the keymap should work, but I need to find
the way to run the redefinition code after the keymap is defined.  Alas, the
hydra is defined in a ~spacemacs/defer-until-after-user-config~.  Which means
that spacemacs runs code after user config, although the docstring to
user-config implies otherwise.

Adding the redefinition code to the same ~defer~ hook does not work, because it
adds the hook at the front rather than at the end.  Does using the extra
argument to ~add-hook~ works?  Ok, it works.

One extra difficulty with the way this is setup is that the bindings also appear
in the docstring to Hydra, so we must also change the docstring.  Thankfully,
the docstring that is used by hydra and displayed to the user is put inside a
~defvar~, so we can alter it at any point afterwards.

But at this point, since we know that the ~defhydra~ call is happening in the
~post-user-config~ hook, might as well redefine the call there.  That could get
messy fast though.

Hmm okay, managed to ~pcase~ it.

* [2016-10-13 jeu.]
** The simple fix that was surprisingly tricky                     :flycheck:
Trying to fix [[https://github.com/flycheck/flycheck-rust/issues/40][issue 40]] by adding a `user-error` call when cargo cannot be found.
It works, but a bit too much.  Most command interaction after that will trigger
the error, rendering Emacs unusable.

Actually, this will also happen if ~cargo~ cannot be found, without the
~user-error~.  The error is credited to ~global-flycheck-mode-check-buffers~:

: Error in post-command-hook (global-flycheck-mode-check-buffers)

This ~check-buffers~ function is created by ~define-globalized-minor-mode~ and
is run in ~post-command-hook~, and in turn will call
~flycheck-mode-enable-in-buffers~, which will turn flycheck-mode in the buffer
after the ~post-command-hook~.  And turning flycheck-mode will, in turn, call
the flycheck-rust hook.

I'm not sure there's a point to call ~flycheck-rust-setup~ in the flycheck-hook
anyway.  It should only be loaded when the rust and rust-cargo checkers
predicates are true?

In any case, silently doing nothing if the executable cannot be found is not
really better.  You get a suspicious checker error, but at least it lets you use
Emacs.

* [2016-10-19 mer.]
** Rust Language Server first alpha is here                    :flycheck:rls:
[[https://internals.rust-lang.org/t/introducing-rust-language-server-source-release/4209][Announcement]]

It seems it does much more than just checking errors, but advertises "errors as
you type".

So, there might be an overlap with flycheck, especially as I expect there to be
a dedicated Emacs package to communicate with the RLS, if it's not included in
rust-mode outright.

Hmm, can't build without the nightly compiler.  I still have rustup, don't I?

I do, just updated and run:

: env PATH=/home/fmdkdd/.cargo/bin:/usr/bin cargo build

to run the rustc and cargo executables from rustup for this command only,
without changing my PATH globally.

After which I can `cargo run`, and ... hmm.  Do what exactly?
It's supposed to be a server, but `lsof` reports no open ports.

The doc says to set the SYS_ROOT env var.  Let's do it.  Still no ports.  Ah ok,
the doc mentions that the language server protocol uses JSON over stdin/stdout.
The HTTP server is actually a separate option `--http`.

That's interesting.  How do I write to stdin interactively?  Writing anything
stops the program without any message.

Okay, let's try the HTTP protocol.  It listens on port 9000 and is responsive to
curl.  But I can't find a way to get compilation errors.

Not ~/on_build~.  Maybe ~/on_save~?.  What does it take?  This:

#+BEGIN_EXAMPLE
pub struct SaveInput {
    pub project_path: String,
    pub saved_file: String,
}
#+END_EXAMPLE

but as JSON.  What should the JSON look like?

#+BEGIN_EXAMPLE
{
  "project_path": "sample_project_2",
  "saved_file": "sample_project_2/src/main.rs"
}
#+END_EXAMPLE

Yes, but for some reason if the outer braces are eaten in the process, and serde
cannot parse it as JSON, hence we should double them:

: curl -v -H "Content-Type: application/json" -d '{{"project_path": "sample_project_2", "saved_file": "sample_project_2/src/main.rs"}}' 127.0.0.1:9000/on_save

Success!  It returns some JSON with errors.

#+BEGIN_EXAMPLE
{
  "Success": [
    "{\"message\":\"field is never used: `x`, #[warn(dead_code)] on by default\",\"code\":null,\"level\":\"warning\",\"spans\":[{\"file_name\":\"src/main.rs\",\"byte_start\":137,\"byte_end\":143,\"line_start\":10,\"line_end\":10,\"column_start\":5,\"column_end\":11,\"is_primary\":true,\"text\":[{\"text\":\"    x: u32,\",\"highlight_start\":5,\"highlight_end\":11}],\"label\":null,\"suggested_replacement\":null,\"expansion\":null}],\"children\":[],\"rendered\":null}",
#+END_EXAMPLE

It's curiously returned as a list of strings, rather than a list of objects, but
it seems to match the JSON error format of rustc alright.

So, it seems we could add a checker to flycheck that interacts with the RLS by
sending "on_change" or "on_save" messages, and parse the result, *without*
caring about build targets.  Here we just say "this file in this project
changed, give me compilation errors".

Now, let's see if we can get the build target information for a file from cargo
directly.

** Can cargo gives us a list of build targets for a given file?    :flycheck:
The read-manifest command seems to return a ~Package~.  Does it build a
dependency graph to find targets, or does it only reads the ~cargo.toml~?

The real work seems to happen [[file:~/proj/flycheck-rust-tests/cargo/src/cargo/util/toml.rs::fn%20to_real_manifest(][in cargo/util/toml.rs::to_real_manifest]], but as
far as I can tell, it only reads the manifest without actually checking any
files.

Let's check that with strace:

: strace -e trace=file cargo read-manifest

#+BEGIN_EXAMPLE
stat("/home/fmdkdd/proj/flycheck-rust/tests/test-crate/Cargo.toml", {st_mode=S_IFREG|0644, st_size=48, ...}) = 0
open("/home/fmdkdd/proj/flycheck-rust/tests/test-crate/Cargo.toml", O_RDONLY|O_CLOEXEC) = 3
stat("/home/fmdkdd/proj/flycheck-rust/tests/test-crate/src/lib.rs", {st_mode=S_IFREG|0644, st_size=7, ...}) = 0
stat("/home/fmdkdd/proj/flycheck-rust/tests/test-crate/src/main.rs", {st_mode=S_IFREG|0644, st_size=40, ...}) = 0
open("/home/fmdkdd/proj/flycheck-rust/tests/test-crate/src/bin", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3
open("/home/fmdkdd/proj/flycheck-rust/tests/test-crate/examples", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3
open("/home/fmdkdd/proj/flycheck-rust/tests/test-crate/tests", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3
open("/home/fmdkdd/proj/flycheck-rust/tests/test-crate/benches", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3
#+END_EXAMPLE

It looks for some conventional files and folders, but that seems to happen when
creating the layout in [[file:~/proj/flycheck-rust-tests/cargo/src/cargo/util/toml.rs::pub%20fn%20from_project_path(root_path:%20&Path)%20->%20Layout%20{][cargo/util/toml.rs::Layout::from_project_path]].  They are
only checked for existence, not read.

So I guess the compiler is the one that reads the files and resolves the
dependencies.  Hence, to add the command I want we would need to interact with
rustc directly.

*** Reading /etc/passwd
On an unrelated note, there's a read to /etc/passwd.  What's up with that?  The
context:

#+BEGIN_EXAMPLE
stat("/.cargo/config", 0x7ffdb20a8690)  = -1 ENOENT (No such file or directory)
open("/etc/passwd", O_RDONLY|O_CLOEXEC) = 3
stat("/home/fmdkdd/.cargo/config", 0x7ffdb20a8690) = -1 ENOENT (No such file or directory)
#+END_EXAMPLE

Hmm, it's probably reading /etc/passwd to determine my $HOME folder.  This is
most probably in cargo/util/config.rs::walk_tree, which in turn relies on
std::env::home_dir.  Which calls sys::os::home_dir, which reads passwd.

** Avoiding errors in flycheck-rust                                :flycheck:
As expected, the absence of cargo is not the only potential source of errors.  I
should really catch all errors to avoid breaking emacs.

Here, opening ~env.rs~ in the stdlib triggered an error.

After RTFM, I found ~with-demoted-errors~ to catch any error and report them as
messages, which is exactly what we want here.  That way, I can still have custom
messages for specific errors (like a missing cargo executable, which ought to be
signaled to the user).

The "proper" way would be to create custom errors with ~define-error~ and signal
them for each case.  But that's a bit heavy-handed for so little code here.

* [2016-10-21 ven.]
** Source diving into a solitaire bot
A [[https://gist.github.com/CyberShadow/218d1ac4033b5d67d99ba5ec4e433b46%20][couple]] of [[https://gist.github.com/BlaXpirit/7a032c0675b56a78d3d0518ac8e4997e][someones]] wrote bots for the solitaire game in Shenzhen I/O.  It
stumped me as something I didn't know how to build.  The Crisp (yet another
language!) solution is less than 500 lines of code.

The problems I didn't know how to solve: extracting the info from the game
screen, sending input back, and then of course solving.

Well, the Crisp solution uses ~import~ to get a single screenshot of the
starting point.  Then it recognizes the cards, computes the list of moves to the
solution, and play them with ~xdotool~.

The D solution uses ~scrot~ to get the screen and talks to ~X11~ directly to
move the mouse.

To recognize the cards, since the cards position is fixed, you could just look
at the corner of each card with hard-coded coordinates, and find pixels that
determine its value.

But what they do is a bit more resilient.  They take a sample screenshot of the
opening game screen, and write down in a text file the value of each card:

: r1 g9 b8 f  g4 r3 g7 b
: ...

Then they look at the corner area of each card, get the pixels there and
associate them with the correct value in a hashmap.

Then, they take the actual game screenshot, look at the corners again, get the
pixels, and lookup the hashmap to get the card values back.

Now they know exactly how the cards are laid out, and just need to solve it.
The solving is not that simple, but the gist of it is a bread search in the
solution space, where the goal is to have no cards left in the lower area.

* [2016-11-22 mar.]
** Fixing my GBS emulator                                               :gbs:
So, after a full rewrite of the CPU, it /still/ doesn't pass Blargg's tests.
Now, I could carefully go over each instruction, /again/, and each opcode, to
see if I've messed up anywhere.  But doing this manually is prone to error.

Talking to Merwan about it, he wished for a way to compare the output of
different emulators, in order to find discrepancies automatically and quickly.

Now, I remember that testing the CPU for the NES emulator I wrote in JS was a
breeze, because some kind soul had written a ROM testing all the instructions
(like Blargg), but also provided a trace file containing the expected state of
the emulator before and after each instruction.  That way, you can pinpoint
exactly what failed.

So, for starters, I could take a working emulator or two, get them to generate a
trace for executing the cpu_instrs test ROM, and compare the output with mine.

** Running cpu_instr on Higan                                           :gbs:
Higan is one of the most accurate GB emulator out there.  So let's try that one
first.  It'll be interesting to compare its output to that of Boyo's.

The source is hosted on gitlab: https://gitlab.com/higan/higan.

To compile, I just had to install gtksourceview2 and tell make to use g++
instead of the hardcoded g++-4.9.  Version 4.9 was locked for linux builds;
there may be legitimate reasons for that, but they were not documented!

Running... Ah.  Higan needs BML files under ~/.local/share/higan/.  Okay, these
are in the source under higan/higan/systems.  Let's cp -r these.

Oh.  [[https://byuu.org/emulation/higan/game-paks][One does not simply run a ROM with Higan]].  I see.  Will creating a gamepak
folder manually suffice?  Hmm, creating a folder and putting a GB ROM inside
does not appear to be enough.

Okay, let's try icarus then.  First ~make~ it.  Then import a GB rom into a
dedicated "Game library" folder.  The structure is now:

#+BEGIN_EXAMPLE
.
└── Game Boy
    └── cpu_instrs.gb
        └── program.rom

2 directories, 1 file
#+END_EXAMPLE

Well, apparently I wasn't far off.  Maybe missing the ".gb" extension in the
folder name.

Trying to load that in Higan... Argh.  Missing manifest file.  Well, icarus
disabled the creation of a "manifest.bml" on the grounds of "backwards
compatibility".  Let's try that import again.

#+BEGIN_EXAMPLE
└── Game Boy
    └── cpu_instrs.gb
        ├── manifest.bml
        └── program.rom

2 directories, 2 files
#+END_EXAMPLE

What's in the manifest?  Not much:

#+BEGIN_EXAMPLE
board mapper=MBC1
  rom name=program.rom size=0x10000

information
  title:  cpu_instrs
  sha256: 8c5e12f41e0ba5bbca796944f92ffe6de28809198682c4332e38d1b3cf56fcf2
  note:   heuristically generated by icarus
#+END_EXAMPLE

Yes it loads!  And all tests pass.  Now to get a trace.

** Getting a trace out of Higan                                         :gbs:
For reference, here's how the traces looked like for the NES test ROM:

#+BEGIN_EXAMPLE
C000  4C F5 C5  JMP $C5F5                       A:00 X:00 Y:00 P:24 SP:FD CYC:  0 SL:241
C5F5  A2 00     LDX #$00                        A:00 X:00 Y:00 P:24 SP:FD CYC:  9 SL:241
C5F7  86 00     STX $00 = 00                    A:00 X:00 Y:00 P:26 SP:FD CYC: 15 SL:241
#+END_EXAMPLE

First column is the PC address, then the opcode and its arguments, then the
decoded opcode, then register values.  There's also the cycle count, and I can't
remember what SL stands for.

So let's try to get the current PC value on stdout from Higan.

Higan apparently has facilities for printing, maybe to be cross-platform.  So I
can just add:

: print(hex(r[PC]), "\n");

The ~auto LR35902::instruction() -> void~ method seems like a good place to get
a trace.  Let's add the whole line!

Wait, apparently this is already included in disassembler.cpp.  The disassemble
method returns a line with PC, decoded opcode, and CPU registers.

There is a bug though, as all the opcodes appear as "nop", and the emulator is
definitely not nopping around.

Ah, found it.  To read the memory, ~disassemble~ uses ~debuggerRead~, a virtual
method with a default implementation of ~return 0~.  The GameBoy CPU inherits
from LR35902 and defines a ... ~readDebugger~ method instead.  Correcting it to
~debuggerRead~ fixes this.

I could submit a patch, but the Gitlab repo appears to be a "unofficial mirror",
and even there I can't find a way to report an issue/submit a patch (it appears
to be disabled).

Anyway, now I have something I can compare with.  Just missing cycles.

Oh, cycles are counted in CPU, not LR35902.  No matter, I'll move the outputting
to CPU.  In fact, there is a very similar thing happening in the MegaDrive
emulator:

#+BEGIN_SRC c++
auto CPU::main() -> void {
  #if 0
  static file fp;
  if(!fp) fp.open({Path::user(), "Desktop/tracer.log"}, file::mode::write);
  fp.print(pad(disassemble(r.pc), -60, ' '), " ", disassembleRegisters().replace("\n", " "), "\n");
  #endif
#+END_SRC

But the disassemble methods in the GB emulator are public only when the DEBUGGER
flag is defined at compilation time.  I'll put the output print behind the flag
as well then.

Generating trace... SIGSEGV.  And here I was, thinking that C++ was not so
crappy after all!  The developer had written nice abstractions, and compiling
was a breeze (much faster than Rust).  And BAM.  Address boundary error.
Commenting the ~print()~ line makes it go away.

So it seems that ~disassemble~ is the cause.  Maybe some memory that's not
cleaned up properly.  In fact, without ~disassembleOpcode~ it does not SIGSEGV.

Hmm, let's try GDB.  Okayyy, the backtrace is huge, so it could be a stack
overflow.  But the stack is full of:

#+BEGIN_EXAMPLE
#92128 0x6666666666666666 in  ()
#92129 0x6666666666666666 in  ()
#92130 0x6666666666666666 in  ()
#92131 0x6666666666666666 in  ()
#92132 0x6666666666666666 in  ()
#92133 0x6666666666666666 in  ()
#92134 0x6666666666666666 in  ()
#92135 0x6666666666666666 in  ()
#92136 0x6666666666666666 in  ()
#92137 0x6666666666666666 in  ()
#92138 0x6666666666666666 in  ()
#92139 0x6666666666666666 in  ()
#92140 0x6666666666666666 in  ()
#+END_EXAMPLE

which is not particularly telling.  Maybe with -ggdb and without -O3?

Wow, 10FPS without -o#.  This time, I do have more information.  The backtrace
is the same, but I have the failing line:

#+BEGIN_EXAMPLE
Thread 1 "higan" received signal SIGSEGV, Segmentation fault.
0x00000000009aec94 in nall::hex<nall::Integer<8u> > (value=..., precision=4, padchar=48 '0') at ../nall/string/format.hpp:95
95	    p[size++] = n < 10 ? '0' + n : 'a' + n - 10;
#+END_EXAMPLE

Aha!  Found it.

The culprit is this loop:

#+BEGIN_SRC c++
  uint size = 0;
  do {
    uint n = value & 15;
    p[size++] = n < 10 ? '0' + n : 'a' + n - 10;
    value >>= 4;
  } while(value);
#+END_SRC

It constructs the hex representation of ~value~ by taking a nibble at a time,
and discarding it with ~>>= 4~.  When ~value~ is all zeroes, it cuts short.

But, some values are signed integers, and for signed integers ~>>=~ will
propagate the sign bit, meaning that a 0xFF value will stay at 0xFF infinitely.
Then the loop continues to write to ~p~, and ultimately writes outside the
program memory triggering the SIGSEGV.

Wait, according to [[https://stackoverflow.com/questions/7522346/right-shift-and-signed-integer][this SO thread]], the behavior of right shift on signed
integers is actually implementation defined!  That's terrible.  So at least
here, what happens is we get the arithmetic shift but wanted the logical one.
In fact, when constructing the hexadecimal representation of the argument, we
don't care about its sign at all.

Hmm okay, so a fix could be to bound the loop by the size of the argument rather
than taking shortcuts:

: while(size < buffer.length());

We allocated a buffer of ~sizeof(T) * 2~, the maximum number of nibbles, so use
that.

The correct fix would be use a proper logical shift rather than use the
unspecified right shift, but that will do for getting a trace.

Running all tests now... okay I have a trace!  And it's 2.1G!  No wonder no one
is hosting these kind of things.

But hey, it's 2016, I have free terabytes, and I can regenerate the trace in 30
seconds now.

I can cut the end a bit, as the tests end on a an infinite loop:

: 06f1  jp   $06f1

So I can keep only the first of such lines.  As long is I do the same in my own
emulator.

Now, I just need to generate a CPU trace from my end, then we diff!

** Getting a trace from my Rust GB emulator                             :gbs:
Hopefully the Rust stdlib will cover outputting padded hex values.  I can skip
writing a disassembler by cutting the second column.

Argh.  Forgot that Higan loads the GB BIOS ROM at first.  They actually
distribute it along the code source, hmm.

Wouldn't it be funny if starting with the BIOS ROM fixed, my CPU would work
fine?

Okaaaay.  Added a bit of code to load the BIOS ROM.  Got a trace.  Got a bit of
Awk to give me the first different line.  First error on line 24601, after an
~rl c~ instruction.

Tomorrow, I can start fixing.  Then it would be interesting to add a third
emulator (Boyo?) to the mix.

* [2016-11-23 mer.]
** Fixing my GBS emulator, one step at a time                           :gbs:
Now that I've got the trace set up, let's see where the errors are.

First error on ~rl c~ with:

: 0 11001110
: | |
: | c
: carry

Higan gives:

: 1 10011100

and GBS gives:

: 1 10011101

Well, obviously, the LSB of C should be 0.

The bug is here:

: let mut v = x.rotate_left(1);
: let c = v & 0x01 > 0;
: if self.f(CY) { v |= 1 }

We rotate x, now the LSB is the new carry value (c), and we erase the LSB with
the current carry value (CY).  But we only set the LSB if carry is set, and we
do not clear it when carry is clear.

The fix:

: if self.f(CY) { v |= 1 } else { v &= !1 }

I expect the same bug to happen with ~rr~.

Next bug... ~ld a,($ff44)~.  Ah, yes of course.  FF44 is LY, the current
vertical line being written to the LCD screen.  I don't emulate that, so no
wonder.

Strange things though, is that I can't seem to get the output of writing to the
link port anymore.  I guess there's something tricky with the boot rom that I
failed to understand.  How does it work in Higan?

There is definitely a flag ~bootromEnable~, which will give data from the boot
ROM when true, and data from the cartridge mapper otherwise.  Thing is, it is
disabled by writing to FF50 which does not appear to be documented anywhere.

Do I get a write to FF50? ... nope.  I guess what happens is that the boot rom
expect LY to work in order to time the Nintendo logo coming down and the chime.
Crap.

Skipping the boot ROm altogether does not make me closer to passing the tests.
At all.

So I guess I'll have to emulate at least part of the LCD.

* [2016-11-27 dim.]
** There's a language server protocol package for Emacs        :flycheck:rls:
[[https://github.com/sourcegraph/emacs-lsp][In development]].  Does it work with the RLS?

At the moment, it seems this mode only supports TCP.  It appears to work for the
Go language server, which has a ~--tcp~ option.  Last time I checked, the RLS
worked over stdio or HTTP, but I don't remember seeing an option for TCP.

The RLS looks like it supports only stdio now.  Shoot.

* [2016-12-06 mar.]
** This week's goal: improving Rust support for Emacs          :flycheck:rls:
Plan A: interface the RLS and Flycheck to get diagnostics support through the
language protocol.

Supposedly, the RLS works with VS Code.  There is already an implementation of
the LSP for Emacs, but it doesn't work over stdio yet.

So I need:
- to make the LSP Emacs plugin work over stdio, and
- to make a Flycheck plugin for the LSP

If that does not work, or if the RLS is still flaky, then I can fall back on
finishing my PR for flycheck-rust.

** Communicating with the RLS over stdio                       :flycheck:rls:
Last time I checked, I got nothing.  Only the HTTP server worked, but now it has
been removed.

First things first, update the repo, rebuild.

: rustup update
: env PATH=~/.cargo/bin cargo build

Oh, there's a cargo bug preventing me to update.  Hold on.

[[https://github.com/rust-lang/cargo/issues/3340][That's the one]].  Fix is another environment variable.

: env PATH=... SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt cargo build

Great.

Running it, now I see that they added errors on stdout when you try to type in
junk.

#+BEGIN_EXAMPLE
> env [...] cargo run
test
Content-Length: 83

{"jsonrpc": "2.0", "error": {"code": -32700, "message": "Parse error"}, "id": null}
#+END_EXAMPLE

That's useful.  Also, [[https://github.com/jonathandturner/rls/issues/111][someone mentioned]] the RUST_LOG=debug env variable to get
more output.  In this case, we get an additional bit of info:

#+BEGIN_EXAMPLE
> env [...] RUST_LOG=debug cargo run
DEBUG:rls::server: Language Server Starting up
test
INFO:rls::server: Header is malformed
DEBUG:rls::server: response: "Content-Length: 83\r\n\r\n{\"jsonrpc\": \"2.0\", \"error\": {\"code\": -32700, \"message\": \"Parse error\"}, \"id\": null}"
Content-Length: 83

{"jsonrpc": "2.0", "error": {"code": -32700, "message": "Parse error"}, "id": null}DEBUG:rls::server: Server shutting down
#+END_EXAMPLE

"Header is malformed".  Well, yes of course.

Now, let's see if I can get the Emacs plugin to talk to you.

If Emacs runs the RLS, it can certainly interact with it as a subprocess.
That's not the intended use case though.  We should rather connect to an
existing RLS.  Can I attach to the stdin/stdout of a running process?

Ok, after tinkering around with pipes, it seems there is no easy way to attach
to a the stdin/stdout of a running process from Emacs.  I /can/ write to a
running process ([[https://serverfault.com/a/297095][thanks]]).  But elisp facilities assume talking to a child
process.

Also, there's an [[https://github.com/sourcegraph/emacs-lsp/issues/8][issue for emacs-lsp]] discussing this.  And it advocates running
an instance of the RLS for each project.

Okay, that should be easier.  Instead of opening a network connection, you use
~start-process~.  Let's see.

: Error: (ls-connection nil)

when trying to run ~lsp-mode-init-conn~.

Hmm'kay.  Turns out this fails there:

: (gethash ws-cache lsp-ws-connection-map)

lsp-mode maintains a hashmap of workspaces to lsp processes.  A workspace is
what is returned by ~projectile-project-root~.  To avoid calling this function
every time a message is sent, it saves the result in ~ws-cache~.  But, as far as
I can see, ~ws-cache~ is never set, only when the mode is ~require~'d.  So when
you activate ~lsp-mode~ the first time, the it saves the workspace of the
current buffer.

I added

: (setq ws-cache ws)

in ~lsp-mode-init-conn~ as a temporary workaround.

Now it runs the process!  I had to create a dumb sh file to run the RLS:

#+BEGIN_EXAMPLE
#!/bin/sh

cd ~/proj/rustls
env PATH=~/.cargo/bin:/usr/bin SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt RUST_LOG=debug cargo run
#+END_EXAMPLE

Because I cannot point cargo to the RLS directory.  I could run the what's under
/target directly but... then I have to add dynamic libraries.

Okay then, now it dies when initializing:

#+BEGIN_EXAMPLE
DEBUG:rls::server: Language Server Starting up
thread '<unnamed>' panicked at 'called `Result::unwrap()` on an `Err` value: Syntax(ExpectedSomeValue, 1, 1)', ../src/libcore/result.rs:837
note: Run with `RUST_BACKTRACE=1` for a backtrace.
#+END_EXAMPLE

Let's see what is sent:

#+BEGIN_EXAMPLE
Content-Length: 163
Content-Type: application/vscode-jsonrpc; charset=utf-8

{"id":30541757336744412,"method":"initialize","params":{"capabilities":null,"processId":null,"rootPath":"/home/fmdkdd/proj/chipers/","initializationOptions":null}}
Content-Length: 136
Content-Type: application/vscode-jsonrpc; charset=utf-8

{"id":89837330512483255,"method":"textDocument/didOpen","params":{"textDocument":{"uri":"file:///home/fmdkdd/proj/chipers/src/cpu.rs"}}}
#+END_EXAMPLE

Okay, two messages are sent.  Which one is causing the error though?

From what I can tell, messages from the RLS are caught by the process filter
~lsp-filter~.  It looks like it waits for the "Content-Length" string, gets the
length, parses the message as JSON, and gives either passes it to an existing
callback or puts it in a hashmap (to be consumed later?).

It doesn't handle Rust error messages like the one above though.

Let's try to send only the first message.

No error so far, but no answer from the server either.  Which, [[https://github.com/Microsoft/language-server-protocol/blob/master/versions/protocol-1-x.md][according to the
protocol]], is unexpected.  Unless, I'm not /seeing/ the answer because
~lsp-filter~ eats it.

It seems it's the second message, didOpen, that triggers the parse error on the
RLS.  Good to know.

But I'm still not getting anything back from the first, the "initialize".

Is this getting through?  Trying to copy and paste the message in the terminal
when running the RLS there gives me:

#+BEGIN_EXAMPLE
Content-Length: 83

{"jsonrpc": "2.0", "error": {"code": -32700, "message": "Parse error"}, "id": null}thread '<unnamed>' panicked at 'called `Result::unwrap()` on an `Err` value: Syntax(ExpectedSomeValue, 1, 1)', ../src/libcore/result.rs:837
#+END_EXAMPLE

But, my message appears as:

#+BEGIN_EXAMPLE
Content-Length: 164

Content-Type: application/vscode-jsonrpc; charset=utf-8



{"id":376714719500194922,"method":"initialize","params":{"capabilities":null,"processId":null,"rootPath":"/home/fmdkdd/proj/chipers/","initializationOptions":null}}
#+END_EXAMPLE

No carriage returns (CR, \r).  Bloody typical.

Ok, let's try to send the message by opening the process in Python.  How many
yaks will I shave today.

Here's what I got:

#+BEGIN_SRC python
import subprocess as sp

initialize = b"""Content-Length: 133\r
Content-Type: application/vscode-jsonrpc; charset=utf8\r
{"jsonrpc":"2.0","id":12345,"method":"initialize","params":{"capabilities":{},"processId":1,"rootPath":"/home/fmdkdd/proj/chipers/"}}"""

print(initialize)

with sp.Popen(["/home/fmdkdd/proj/rustls/run-rls.sh"], stdin=sp.PIPE, stdout=sp.PIPE) as proc:
    out, err = proc.communicate(initialize)
    print(out)
#+END_SRC

Gosh.  What does it take to send a valid message here?  Okay, turns out the LSP
mode for Emacs seems to follow the 2.0 revision of the protocol.  I'm not sure
which version the RLS speaks, as I haven't got a non-error answer back.

Still trying to trace where the parsing fails over in the RLS...

So far, it seems to decode the header, get the content as a string, and
then... offloads the parsing to a thread.  But before that can happen, the main
process continues to read messages on stdin, and since there is nothing more to
read, it aborts prematurely.

Wait, I thought I had a pipe.  Does communicate closes the pipe?  Argh, it looks
like it waits for the process to terminate, so yes.  Hmm, can I just write to
stdin then?

Yes, is seems that is better:

#+BEGIN_SRC python
    proc.stdin.write(initialize)
    proc.stdin.flush()
    print(proc.stdout.read())
#+END_SRC

And here are the first answers from RLS:

#+BEGIN_EXAMPLE
DEBUG:rls::server: response: "Content-Length: 487\r\n\r\n{\"jsonrpc\":\"2.0\",\"id\":12345,\"result\":{\"capabilities\":{\"textDocumentSync\":2,\"hoverProvider\":true,\"completionProvider\":{\"resolveProvider\":true,\"triggerCharacters\":[\".\"]},\"signatureHelpProvider\":{\"triggerCharacters\":[]},\"definitionProvider\":true,\"referencesProvider\":true,\"documentHighlightProvider\":true,\"documentSymbolProvider\":true,\"workspaceSymbolProvider\":true,\"codeActionProvider\":false,\"documentFormattingProvider\":true,\"documentRangeFormattingProvider\":true,\"renameProvider\":true}}}"
DEBUG:rls::server: response: "Content-Length: 72\r\n\r\n{\"jsonrpc\":\"2.0\",\"method\":\"rustDocument/diagnosticsBegin\",\"params\":null}"
#+END_EXAMPLE

Strangely, without RUST_LOG=debug, these are not written to stdout.  What's the
point then?

Oh, it /should/ output:

#+BEGIN_SRC rust
        debug!("response: {:?}", o);

        print!("{}", o);
        io::stdout().flush().unwrap();
#+END_SRC

But it doesn't.  I get it through ~debug~, but the regular ~print~ does
nothing.  Maybe... maybe ~flush~ isn't flushing?  Maybe it's buffered somewhere
because it's too small?

Maybe if I print a bunch?

#+BEGIN_EXAMPLE
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
      print!("{}", o);
#+END_EXAMPLE

Ah.  The compiler crashed.

#+BEGIN_EXAMPLE
note: the compiler unexpectedly panicked. this is a bug.

note: we would appreciate a bug report: https://github.com/rust-lang/rust/blob/master/CONTRIBUTING.md#bug-reports

thread 'rustc' panicked at 'Box<Any>', ../src/librustc_errors/lib.rs:382
note: Run with `RUST_BACKTRACE=1` for a backtrace.
#+END_EXAMPLE

Weird.  Cannot reproduce.

Still not flushing the response.  Let's try... exiting.  Nope.

At this point, I'm turning to the Python wrapper script.  Maybe there's
something fishy in there again.

I'm not sure reading my proc.stdout.read() is right.  Let's just pipe that the
output to the stdout of the process:

#+BEGIN_SRC python:
with sp.Popen(["/home/fmdkdd/proj/rustls/run-rls.sh"], stdin=sp.PIPE, stdout=sys.stdout, stderr=sys.stderr) as proc:
    proc.stdin.write(initialize)
    proc.stdin.flush()
    while True:
        time.sleep(1)
#+END_SRC

And keep the pipe open by sleeping.

Ahhhh.  Much better:

#+BEGIN_EXAMPLE
Content-Length: 487

{"jsonrpc":"2.0","id":12345,"result":{"capabilities":{"textDocumentSync":2,"hoverProvider":true,"completionProvider":{"resolveProvider":true,"triggerCharacters":["."]},"signatureHelpProvider":{"triggerCharacters":[]},"definitionProvider":true,"referencesProvider":true,"documentHighlightProvider":true,"documentSymbolProvider":true,"workspaceSymbolProvider":true,"codeActionProvider":false,"documentFormattingProvider":true,"documentRangeFormattingProvider":true,"renameProvider":true}}}Content-Length: 72

{"jsonrpc":"2.0","method":"rustDocument/diagnosticsBegin","params":null}Content-Length: 70

{"jsonrpc":"2.0","method":"rustDocument/diagnosticsEnd","params":null}
#+END_EXAMPLE

I'm tempted to introduce an error to see if the diagnostics work.

Hm, does not seem to give me any diagnostics after the initialize, even though
it clearly says "diagnosticsBegon" and "diagnosticEnd".  Maybe I should trigger
the analysis?

Argh, it seems complicated for now.

Okay, I just wanted to know why I didn't get any response from the server when
communicating through LSP mode.  Now I know it will respond to the initialize
command, given an adequately formatted message:

#+BEGIN_EXAMPLE
Content-Length: 133\r
Content-Type: application/vscode-jsonrpc; charset=utf8\r
{"jsonrpc":"2.0","id":12345,"method":"initialize","params":{"capabilities":{},"processId":1,"rootPath":"/home/fmdkdd/proj/chipers/"}}
#+END_EXAMPLE

But the message sent by the LSP was:

#+BEGIN_EXAMPLE
Content-Length: 163\r
Content-Type: application/vscode-jsonrpc; charset=utf-8\r
\r
{"id":30541757336744412,"method":"initialize","params":{"capabilities":null,"processId":null,"rootPath":"/home/fmdkdd/proj/chipers/","initializationOptions":null}}
#+END_EXAMPLE

and this fails.  And the culprit is the empty line before the content.  There
/is/ code in the RLS to skip this line:

#+BEGIN_SRC rust
        // Skip the new lines
        let mut tmp = String::new();
        handle_err!(io::stdin().read_line(&mut tmp), "Could not read from stdin");
#+END_SRC

But for some reason it does not work.  Removing the empty line in the message
works.  Alternatively, adding the two bytes for the extra \r\n to Content-Length
also does the trick.

So, if I remove the empty line in LSP...

Not picking up any response.  But at least, the RLS process is not exiting.
That's progress.

Hmm hmm.  Seems RLS hangs there:

#+BEGIN_EXAMPLE
TRACE:rls::server: ["Content-Length:", "165\n"]
TRACE:rls::server: reading: 165 bytes
#+END_EXAMPLE

Probably not enough bytes to read on stdin?

In the Python test, I get:

#+BEGIN_EXAMPLE
TRACE:rls::server: ["Content-Length:", "165\r\n"]
TRACE:rls::server: reading: 165 bytes
TRACE:rls::server: content: "\r\n{\"id\":30541757336744412,\"method\":\"initialize\",\"params\":{\"capabilities\":null,\"processId\":null,\"rootPath\":\"/home/fmdkdd/proj/chipers/\",\"initializationOptions\":null}}"
#+END_EXAMPLE

notice the additional *\r*.  The \r\n is definitely present in the LSP code, but
it seems to be eaten before it arrives to RLS.  I had a bad feeling about CRLF
from the start.

If I run "cat" instead of the RLS, here is what is actually sent:

#+BEGIN_EXAMPLE
Content-Length: 164

Content-Type: application/vscode-jsonrpc; charset=utf-8

#+END_EXAMPLE

No body, even though the argument to ~process-send-string~ contains it.  Also no
\r.

Trying to mess the with the process coding system... no luck.  Is there any way
to send raw bytes to a process instead?  Doesn't seem so.

But even disregarding the missing carriage returns, ~cat~ doesn't even get the
body!  That's mystifying to me.

Even trying to hardcode it fails:

: (process-send-string net-proc "{\"id\":30541757336744412}")

nothing is seen by the child process ~cat~!  Madness.

Okay.  Adding \n at the end fixes it.  It's simply not flushing stdin without
it.  And I see no way of forcing flush.  So let's add this final \n.

Now I see that it's going through.  And I get a parse error, probably because of
the missing \r.  I guess if I cannot make Emacs behave, I can always use a
wrapper to insert carriage returns.

Again, trying to toy with coding-systems, to no avail.

Tomorrow I will try a wrapper around the RLS to insert carriage returns.

Unanswered question: which version of the LS protocol does RLS use?

* [2016-12-07 mer.]
** Making the LSP mode and the RLS talk to each other          :flycheck:rls:
Written a wrapper python script to add the damned carriage returns:

#+BEGIN_SRC python
#!/usr/bin/env python

import subprocess as sp
import sys
import time

def wrap(body):
    return "Content-Length: {}\r\nContent-Type: application/vscode-jsonrpc; charset=utf8\r\n{}".format(len(body), body).encode()

with sp.Popen(["/home/fmdkdd/proj/rustls/run-rls.sh"], stdin=sp.PIPE, stdout=sys.stdout, stderr=sp.DEVNULL) as proc:
    while True:
        # Read until \n
        line = sys.stdin.buffer.readline()
        # Turn the bytes into an UTF8 string
        line = line.decode()
        # Line should be a JSON object, we wrap it
        msg = wrap(line)

        # print(msg)

        # Pass it to the subprocess
        proc.stdin.write(msg)
        proc.stdin.flush()
#+END_SRC

In fact, it adds the whole header, since there's no point passing through a
botched header from LSP.

Additionally, it silences stderr as the LSP mode cannot handle it.  A better way
would be to open stderr on Emacs side in a separate buffer, and be ready to get
errors there.  But, I think in case of parse errors the RLS will answer with a
JSON message on stdout anyway, so we could still handle errors in LSP without
watching stderr.

And it works!  And it fails after the initialize, because RLS sends commands
that are not part of the protocol:

: error in process filter: apply: Invalid function: ((jsonrpc . "2.0") (method . "rustDocument/diagnosticsBegin") (params))
: error in process filter: Invalid function: ((jsonrpc . "2.0") (method . "rustDocument/diagnosticsBegin") (params))

Mysteriously, there are no diagnostics being printed out between these two
commands.  I suppose diagnostics are not implemented yet in the RLS.

So this approach works.  But it requires a wrapper, so that's not adequate for a
plugin.  I now remember that I wanted to try comint.

* [2016-12-08 jeu.]
** Comint                                                      :flycheck:rls:
Hmm reading the documentation for comint, it seems more suited for interacting
with REPLs.  I guess you can sort of see the RLS as a REPL, where you
communicate with JSON, but there's no interactive part at all.  Not sure this is
the right way.

** Another emacs-lsp in the wild                               :flycheck:rls:
That's hilarious.  A few hours ago, [[https://lists.gnu.org/archive/html/emacs-devel/2016-12/msg00263.html][someone]] asked on emacs-devel why did
"process-send-string" eat carriage returns, for the purpose of making an LSP
package to communicate with Rust.  Turns out, [[https://github.com/vibhavp/emacs-lsp][their implementation]] seems more
advanced than the one from @cmr.

No definite answer on the carriage return problem.

** Trying out the new emacs-lsp                                :flycheck:rls:
Seems more solid.  Works over stdout, I can see the messages going out to the
server and coming back.

They solved the carriage return issue.  The correct incantation was to use
make-process in order to set ~connection-type~ to ~pipe~, as suggested by the
answer in the emacs-devel thread:

#+BEGIN_SRC elisp
(make-process :name "lsp"
              :buffer bufname
              :command '("cat" "-A")
              :connection-type 'pipe)
#+END_SRC

(using ~cat -A~ to check for carriage returns here)

In addition, the mode sends the right messages when the document changes, or
when the cursor stays a while on a symbol (using eldoc).

But the answers from the RLS rarely provide any useful content.  On-hover only
worked on one variable, whereas ~racer-doc~ works on variables, functions...

And there seems to be code for sending out diagnostics, but it's never actually
triggered.

My conclusion at the moment if that the Emacs side of thing looks good.  We have
two plugins in the works, with at least one that works out of the box.

Now we just need the RLS to catch up.

That means that, for the rest of the week, I should probably focus on improving
the state of flycheck-rust using cargo and rustc directly.

** Flycheck issues                                                 :flycheck:
A few weeks ago, I was in the process of refactoring flycheck-rust, and adding
tests.  I didn't care much for the plain rust checker, since it seemed hackish
in the way it looked for dependencies.

Then one of the original contributors for the rust checker showed up for a PR,
and I looked up the history of this checker.  Initially, it had support for
highlighting errors as you type.  Then, when rust-cargo came along, this had to
be disabled because it came in conflict.

I just tried to restore this functionality.  It's really snappy for standalone
rust files.  It's better than working with the rust playground when testing
small snippets.

Now, this plain checker is also used for projects that do not make use of
cargo.  The rub here is that if you have external dependencies, you are on your
own to manage them and to tell flycheck where to find them.  That's currently
done through the ~flycheck-rust-library-path~ variable.

~flycheck-rust~ will try to fill this variable by looking around, but that's
best effort without any guarantees.

Something's bugging me though, since we already have ~flycheck-rust-args~ for
passing arbitrary arguments to rustc, why the extra
~flycheck-rust-library-path~?

While we are at digging history, it seems lunaryorn favored passing buffer on
standard input.  This can be done for rustc by passing "-" as the input
filename.  But now errors are reported with "<anon>" as source, and you also
need to change the error parser to not provide any filename in this case.
Otherwise, errors are filtered and do not appear in the buffer.

Also, there may be lingering flycheck issues over at Spacemacs that were never
reported upstream.  Might be worth to take a look.

* [2016-12-09 ven.]
** Finding the right target for a rust file                  :flycheck:cargo:
That's the sole goal of flycheck-rust, and I have seen that the RLS is not there
yet.  I suspect IntelliJ also requires you to specify which target to run, since
it gives you the results in a bottom window, and from there you can jump to the
incriminated files.

My heuristics method is sound, but does not cover all files adequately.  In [[https://github.com/rust-lang/cargo/issues/3211][my
ticket to cargo]], it was suggested that I look into racer for to determine the
DAG of dependencies between source files.

From a cursory glance, I cannot find this information in the CLI tool.  But
maybe this information can be given by the racer library.  But that means
depending on a third-party tool that is yet to be written.  Hmmm.

It was also suggested to use ~cargo rustc --emit dep-info~.  It can give us a
file with a dependency graph, with a syntax reminiscent of Makefiles:

#+BEGIN_EXAMPLE
> rustc --crate-type lib --emit dep-info=t -Z no-analysis src/lib.rs
> cat t
srclib: src/lib.rs src/a.rs

src/lib.rs:
src/a.rs:
#+END_EXAMPLE

From there, we can determine that ~src/a.rs~ will be checked by the compiler
when building the the requested target ~src/lib.rs~ for instance.  But that
requires three steps:

1. First find the list of targets by calling ~cargo metadata~
2. Generate a dep-info for each target, to get the graph of dependencies
3. Parse these files and hope to find a match with buffer-filename

With the following limitations:

1. Parse error will prevent the dependencies from being created
2. The dependencies might change as a result of editing buffers (so you'd want
   to rerun flycheck-setup)

Also, I cannot get ~cargo rustc~ to emit dependencies.  The trick I found is to
use:

: env RUSTFLAGS='--emit dep-info -Z no-analysis' cargo build --lib

But, I'm not sure calling cargo is necessary in order to get the dependency
graph, since using cargo is useful to manage external dependencies, not internal
ones.  So we should be able to get by with ~rustc~ directly.  But need to test
on real crate.

Ok so on the trust-dns crate, using ~rustc~ is not enough, since it chokes when
looking for external crates, even though we are just asking for dep-info.

With cargo?  Passing ~--emit dep-info~ in flags works ... for the first
invocation of rustc, which happens to be on an external crate, semver.  I get
dependencies for semver, and after that:

: error: An unknown error occurred

Hmm.  Using ~cargo rustc --lib -- --emit dep-info=bla~ now.  Doesn't write to
bla.

Okay then.  ~rustc --emit dep-info~ is not an option.  racer is third party
dependencies.  Then we'll stick to hackish heuristics then.

** On cargo metadata and workspaces                          :flycheck:cargo:
It the same [[https://github.com/rust-lang/cargo/issues/3211][PR]] was suggested to use ~cargo metadata~ rather than ~cargo
read-manifest~.  The reasons mentioned were that ~read-manifest~ was deprecated,
and didn't support workspaces (had to read up on what they are).

I think I used ~read-manifest~ because ~metadata~ will trigger a compilation,
which seems unacceptable for this use case.  But, turns out that ~metadata
--no-deps~ will output the JSON that we want without building.

There's just an additional complication: since it's compatible with workspaces,
we have to find our targets inside a list of packages now.  The question is, how
do we deal with multiple packages?

(A related question is: what's the difference between a crate and a package?  I
asked on #rust-tools, and ... no answer yet.  But [[So probably ][watch this space]])

So, workspaces solve the problem of having multiple crates in the same
repository, when all these crates are really parts of the same project.  [[https://github.com/rust-lang/rfcs/blob/master/text/1525-cargo-workspace.md][The RFC]]
does a good job of explaining why this is needed and how it works.

The point is to have only one Cargo.lock file, and only one target/ build
directory, to ensure that all dependencies are synchronized, and to save on
compilation time.

That's quite important for Flycheck.  At the moment, in a project with
workspaces, we will look for the nearest Cargo.toml and be able to build targets
alright.  But, we might not build in the right target folder, since we would
miss the workspace entirely, and this would cause long compilation times.

In any case, I think we are safe if we just aggregate all "targets" for all
"packages" when looking for a best match.

Testing against a project with workspaces... (looking for one on Github is
rather easy, just code search for workspace in TOML files).  Ok, [[https://github.com/bluejekyll/trust-dns%20][found one]].

cargo metadata --no-deps gives me two packages, and plenty of targets:

: > cargo metadata --no-deps | jq .packages[].targets[]

But you cannot build anything since it's the top-level Cargo.toml is a virtual
manifest: it has no associated targets.

: > cargo build
: error: manifest path `/home/fmdkdd/proj/flycheck-rust-tests/trust-dns/Cargo.toml` is a virtual manifest, but this command requires running against an actual package in this workspace

I would expect to be able to build the crate from the top level by passing
--package, but no:

: > cargo build --package client --lib
: <exact same error as above>

With or without --lib, it fails.

So, if I go into client/ and build, will it build in a target/ at the top level
or inside client/?

Building... oh, it outputs in the top level folder.  Hmm good then, because it
means we can invoke cargo on the first Cargo.toml we find, without messing
things up.

So: replace ~read-manifest~ by ~cargo metadata --no-deps~, gather all targets,
and build in the current directory is fine.

* [2016-12-12 lun.]
** Actually pushing PR                                       :flycheck:cargo:
The time for research is done.  Now is the time to actually code and push.

First things first: enhance the rust-cargo checker in flycheck proper, since
flycheck-rust is more tricky, but rust-cargo should work without flycheck-rust.

The thing we need to fix is to make the targets variable wider, to accommodate
test and benches targets.

And use the new ~--message-format=json~ flag to cargo rather than use the
RUSTFLAGS environment variable for two reasons: 1) currently compilation errors
in dependencies will not follow the JSON format since passing
~--error-format=json~ to rustc only applies to the final compilation, and 2)
using RUSTFLAGS will trigger a recompilation if the user manually runs ~cargo
build~ without the same RUSTFLAGS.  Using the new flag, cargo knows it does not
affect compilation output and does not recompile anything superfluous.

Remaining question: should ~-Z no-trans~ be passed in RUSTFLAGS?

On the topic of terminology: we can keep using ~flycheck-rust-crate-type~ even
for cargo targets, since according to [[https://github.com/rust-lang/cargo/issues/3380][this issue]], they are the same thing.

Argh, bug with --message-format in combination with -Z no-trans.  If I build
with:

: cargo rustc --message-format=json --lib -- -Z no-trans

I get:

: error: Could not compile `test-crate`.

but no errors, human or JSON on output.

If I do:

: cargo rustc --lib -- -Z no-trans --error-format=json

I get the error.

But!  If I remove the ~-Z no-trans~ and do:

: cargo rustc --message-format=json --lib

Then I get the error.

Hmm, weird, when I try:

: cargo rustc --message-format=json -- -Z no-trans

on chipers (which contain no errors), I also get:

: error: Could not compile `chipers`.

but no errors.  Running with ~--verbose~, as suggested by the compiler, yields:

: error: Could not compile `chipers`.
: Caused by: failed to parse output: `rustc src/main.rs --error-format=json <snip>`
: --stderr

#+BEGIN_EXAMPLE
error: Could not compile `chipers`.

Caused by:
  failed to parse process output: `rustc src/main.rs --error-format json <snip>`
--- stderr
warning: the option `Z` is unstable and should only be used on the nightly compiler, but it is currently accepted for backwards compatibility; this will soon change, see issue #31847 for more details

Caused by:
  compiler produced invalid json: `warning: the option `Z` is unstable and should only be used on the nightly compiler, but it is currently accepted for backwards compatibility; this will soon change, see issue #31847 for more details`
#+END_EXAMPLE

So it seems... cargo is parsing the JSON output of rustc (maybe to linearize the
output of parallel compiler invocations).  But having a Z flag writes on stder,
and if I interpret the last error correctly, it tries to parse the stderr output
as JSON.

Reading the source, it seems it expects the JSON output on stderr, and stdout to
be empty.

Running the ~rustc~ command manually given by --verbose, I see that the JSON
errors indeed appear on stderr, as well as the "warning" for Z options.  So it
seems cargo funnels the JSON messages to stdout, but rustc will write everything
to stderr.

I think I can patch it by skipping over lines from rustc which do not begin with
'{', same as we do in Flycheck.  Don't know if that's a long-term solution
though.  Let's try it.

Added the line:

: if line.starts_with("{") {

and now it compiles.  So I guess I can file an issue that with a tentative fix.
But that means it won't be fixed before ... a long time.  So I'll have to hold
off my PR for Flycheck until then.  Shoot.

Or we continue using --error-format on the latest target, with the current
caveats.  Or we skip -Z no-trans, since after all we are using a nightly feature
on stable rust, and since nightly does not raise the warning, the problem does
not arise there.  Or we /require/ nightly for Flycheck, but that's a bit steep.
Or we hold off for [[https://github.com/rust-lang/cargo/pull/3296][cargo check]] to land (but it's not merged yet).

I'd rather find a workaround for the current version of cargo, but I cannot find
any flag that would allow me to skip this.

Also, using --error-format without RUSTFLAGS quickly fails on the test-crate
when building a binary that requires the lib, since the lib is a dependency and
contains an error.  So at the moment I'm stuck, because RUSTFLAGS has the
downside of triggering recompilation.  As the status quo works, let's leave it
at that.

What I /can/ do though, is fast-track the clippy checker PR and integrate the
changes on top of it.  Since it requires nightly anyway, I will be able to use
--message-format without troubles.

** Fast-tracking the rust-cargo-clippy checker              :flycheck:clippy:
Downloading the PR locally to test.

Argh, cannot compile cargo-clippy with latest nightly.  Asking on IRC.  Oh, it's
a [[https://github.com/Manishearth/rust-clippy/issues/1381][bug]].  Installing the adequate nightly...  Yeah now it works.

Remember to ~rustup default nightly-2016-11-25~ to update the symlinks under
~/.cargo/bin.

Now I can run clippy with flycheck.  On the surface, it works.

Now what's that talk about clippy checking all the available targets?  I can't
seem to pass a ~--bin~ or ~--test~ flag, but if I have errors in a test binary
and in the lib crate, it only prints the error for the lib (even though the test
does not depend on the lib crate).

Removing the error though, it does pick up errors in examples/.  So it seems it
does run all targets, but one after the other, not concurrently.  So you cannot
have error feedback when there's an error in another target that's checked
before.

Gosh this is annoying, since in the case of Flycheck, it will run for /every/
target, even for a single buffer.

Well, at least it's opt-in.  But it means there's not much I can do to on it to
test nightly features.  Just ~--message-format~.

Also, we are not using ~-Z no-trans~ there.  Don't know if we should... maybe
~no-trans~ should be behind a flag for all rust checkers, that way we let users
choose between correctness and speed.

* [2016-12-14 mer.]
** Emulating the GameBoy screen                                         :gbs:
Trying to separate concerns a bit.

The background is 256*256 pixels, or 32*32 tiles, and tiles is 8*8 pixel.

256*256 = 65536 pixels
32*32 = 1024 tiles
32*32 * 8*8 = 65536

The tile map area is 0x1000 bytes long, and two bytes make 8 pixels.

0x1000 * 4 = 16384

0x1000 / 16 = 256

There can be 256 different tiles in the pattern table.

Tried to separate things... there's still a coupling between LCD and Screen.  I
think it's because Screen is the implementation of an abstract drawing surface,
and LCD tries to use it directly.  An alternative would be to declare a trait of
some kind on which I can draw, LCD would use that type, and Screen would
implement it.

Also, what would be really neat would be to have a mapping between memory places
and Rust structs for I/O registers and the like.  So you deal with mapping
to/from bits only once, and after that you deal with structs and fields that
mean something.  In C you can declare a large RAM, create a struct and map it to
some place in the RAM and that's it.  All fields now point to the correct
places.  Changing and accessing the RAM or the field is the same, no additional
work needed.

* [2016-12-16 ven.]
** Data-binding in Rust                                                 :gbs:
I did some tests yesterday.  What I can do in C, I can do in Rust using
mem::transmute at the very least.  Create a large RAM array, create a struct for
Tiles, and transmute a pointer to the RAM as a pointer to a Tile.

Of course, that's unsafe, and might even be undefined behavior.   And the struct
might not be densely packed in memory, so you could have holes that do not map
directly to the RAM.

Otherwise, the Tile struct can contain only /pointers/ to offsets in the RAM.  A
Tile is a certain view of the RAM, with friendlier field names.  That works, but
only when the pointers are non-mutable, otherwise you have a trouble of multiple
mutable borrows.  So that's only a one-way binding.  But actually, I realized
that's all I need realistically.  The ROM program will always access tile data
by reading from and writing to the RAM, not through the Tile struct.  The Tile
struct is useful as a more precise view of some RAM area, and as such can be
immutable.

In fact, if all the reads and writes go through the RAM first, then you can just
implement the redirection in the memory module to write in the tiles data rather
than in the RAM array.  That's what [[https://github.com/Gekkio/mooneye-gb][mooneye]] does.  One downside of this approach
though is that your (internal) RAM array is not a faithful representation of the
GB RAM, since parts are never read and written to.  So you cannot serialize it
by dumping the RAM array.  But, if you stick to the read method of the memory
module (its interface), then it is consistent.

It might even mirror the way the GB hardware was wired up.  Read/write access to
the tile pattern table probably was directly wired to VRAM that was held up by
the PPU chip.

So this approach is fine.

** flycheck-rust errors when browsing cargo                  :flycheck:cargo:
When fixing bug #3390 in cargo, I noticed flycheck-rust had trouble finding
the right build targets in some files.  Should probably investigate.

Ah ok, first error was just a fluke caused by Spacemacs STILL NOT PICKING UP THE
FLYCHECK THAT'S IN MY LOAD PATH.  Jebus.  I have to find out how to do that.

After losing a couple hours (and losing WIP in flycheck due to a hard link even
though I had used ~ln -s~... sigh) and getting help on the Spacemacs Gitter,
turns out the only reliable way is to ~load-file~ my local packages.  In another
case of issue resonance, someone [[https://github.com/syl20bnr/spacemacs/issues/4979][faced the exact same problem]], and for flycheck
even!

And the other error was caused by a relative path as a target file name given by
~cargo read-manifest~.  I could workaround it in flycheck-rust, but it seemed
more appropriate that ~cargo read-manifest~ (and ~cargo metadata~) gave
canonical file names throughout, so I reported an issue.  Might even send a PR.

** Looking at how mooneye is structured                                 :gbs:
Because Rust definitely favors a single-owner for structs, since having multiple
mutable references to things is a nightmare.

In mooneye, the GPU and PPU (called GPU) are all in a Hardware struct that
handles read and writes to them.  Then the CPU contains the Hardware, through
the declared type is a Bus trait:

#+BEGIN_SRC rust
pub trait Bus {
  fn write(&mut self, u16, u8);
  fn read(&self, u16) -> u8;
  fn emulate(&mut self);
  fn ack_interrupt(&mut self) -> Option<Interrupt>;
  fn has_interrupt(&self) -> bool;
  fn trigger_emu_events(&mut self, EmuEvents);
}
#+END_SRC

So the CPU "owns" the hardware in the end, but at least it goes through an
interface.

The two are assembled in a Machine struct:

#+BEGIN_SRC rust
pub struct Machine {
  cpu: Cpu<Hardware>
}

impl Machine {
  pub fn new(config: HardwareConfig) -> Machine {
    Machine {
      cpu: Cpu::new(Hardware::new(config))
    }
}
#+END_SRC

Disappointing.

rustendo64 is sparse.  It does not appear to have anything other than the CPU at
the moment.

sprocketnes goes for CPU owning the memory as well.

Weeell.  Turns out, I may be overthinking this.  If the modules interact with
each other through a Trait, that should be good enough for reuse.  Just have to
make sure that when you assemble them, you put them in the proper order.

* [2016-12-20 mar.]
** Investigating template parameters for traits                        :rust:
I saw the solution, but not why it was necessary.  After starting from the bits
I understand and expanding, I think I'm getting more familiar with how the
pieces work together.

* [2016-12-23 ven.]
** Installing ArchLinux on a VAIO z13                                  :arch:
Because I'm tired of Ubuntu.

Installing ArchLinux on my desktop was mostly a breeze.  Configuring took time,
but no hardware hitches.

There are a few additional gotchas for laptops, as usual.

*** Hybrid graphics
First of all, the z13 is a hybrid graphics machine: one integrated (i915) GPU
and one discrete nvidia card.  That was a nightmare to get working every time I
installed Linux on it, but for a while now I have flashed the BIOS to open up
hidden options, and disabled the nvidia GPU at boot.

*** Raid0
Second, there are two 64Gb SSDs striped in raid0.  To partition the drives the
first time (years ago), the only thing that worked out of the box was a Gentoo
live USB.  Now the ArchLinux live USB flawlessly picked up the raid0
configuration, but not after the installation.

After GRUB selection, I got "unable to find root UUID".  This was caused by not
loading the raid modules in the kernel, so the partitions could not be found.
Adding:

: MODULES="md_mod raid0"

and adding the ~mdadm_udev~ hook:

: HOOKS="base udev autodetect modconf mdadm_udev block filesystems keyboard fsck"

to /etc/mkinitcpio.conf, and running:

: # mdadm --examine --scan > /etc/mdadm.conf

(after installing mdadm), did the trick.

At first I thought it was a fstab issue with the UUIDs, but no.  Also, I tried
to add the ~mdadm~ hook and it did not work.  Neither did adding the ~block~
module before ~autodetect~ as suggested by a SO answer.  From a cursory reading
of ArchWiki, it seems not a good idea to do that either, since autodetect will
help in lazy loading modules, so it should come early in the hooks list.

*** Backlight
Since I'm using i3 directly on top of Xorg and xinit, my brightness keys are not
directly recognized.  Well, they are by ~xev~, but they do not alter the
backlight.

Though there is a /sys/class/backlight/intel_video/brightness that can be
altered by writing to it.  And after installing xf86-video-intel, xbacklight
picks it up and I can bind ~xbacklight -inc 5~ to the brightness keys in my i3
config.

On a related note, it seems the z13 uses PWM (around 200Hz at the moment), but I
haven't noticed any flickering in years of use.

*** Optical disk
That one is probably my fault.  After opening the z13 a couple of times to clean
it up and re-apply thermal paste, the optical disk tray opens up by itself,
probably because some sensor was misplaced when putting it back together.

Disabling the sr_mod and cdrom kernel modules did not fix it.  Some forum thread
suggested disabling in the BIOS.  I don't know if I can.  BRB.

Nope, no option for that.  Probably will have to fix the hardware fault, or
disconnect it.

*** CapsLock as ctrl + escape
Can't live without it.

That's built-in in my desktop keyboards.  At least the Ctrl part.  Here I have
to tell Xorg to do it with:

: localectl --no-convert set-x11-keymap us colemak colemak ctrl:nocaps

the last part.

I could setxkbmap that in my xinitrc, but I'd rather have it as a system
default.  Even though it means not having it in a dotfile somewhere.  But that's
system specific anyway.

~xcape~ takes care of turning taps into escape.  Just have to install it, as
it's in AUR.

Installing cower, missing a PGP key.  Solution is [[https://bbs.archlinux.org/viewtopic.php?id=152337][here]]:

: gpg --list-keys
: echo 'keyring /etc/pacman.d/gnupg/pubring.gpg' >> ~/.gnupg/gpg.conf

Or, run makepkg with --skippgpcheck.

xcape doesn't work when run from .xinitrc.  Hmm.

Ah no, it does work, but only after hitting the Ctrl key once.  After that, it
treats CapsLock as a Ctrl as well.  If I never hit the Ctrl_L key, it won't pick
up the CapsLock as Ctrl.  Annoying.  Maybe if I remap CapsLock through xmodmap
it will pick it up.  Might come back later to it, but for now the workaround is
good enough.

*** Touchpad
At first, the touchpad had the surprising behavior of resetting the pointer
position to the middle of the screen every time I put my finger on it.  Simply
installing libinput fixed it.

But, that's not all.  I want taps to register as clicks.  man libinput, and add
the adequate options to /etc/X11/xorg.conf.d.

There are a couple of lingering warnings in dmesg.  From a quick search, they
seem harmless.

*** Xorg tips
I used to set the keyboard delay and rate through startx directly because xset
settings could be reset.  But I can also put it in
/etc/X11/xorg.conf.d/00-keyboard.conf to avoid repeating it.

Wait no, it doesn't work.  WTH.  Oh, that option was actually removed.  Well,
sticking to aliases it is then.

*** Fonts
Fira Sans is nice, but it hints pretty badly on small sizes (8px).  And without
hinting, antialiased fonts are blurry.  Noto scales well.

* [2016-12-25 dim.]
** ArchLinux with z13 cont.                                            :arch:
*** systemd-backlight service
This is supposed to save and restore the brightness on sleep/reboot.  I never
had this working before, and always had the screen brightness set to
burn-my-retina level.  No more!

The service is enabled by default.  But without the proper kernel setting, it
saved the wrong backlight under /sys/class/backlight (there were two of those,
one that was ineffectual).  So, toying with the acpi_backlight kernel option,
one managed to leave only one folder under /sys/class/backlight, and then it
worked.

In /etc/default/grub:

: GRUB_CMDLINE_LINUX_DEFAULT="acpi_backlight=native"

that's the ticket.

*** Xmodmap
The ctrl:nocaps option of X11 is not enough in combination with xcape, since
apparently xcape does not pick up caps lock as an additional control after I hit
the original Ctrl_L key.

Maybe by remapping the key in xmodmap directly?

And while I'm at it, I should move my volume/brightness keys bindings to Xmodmap
rather than i3.

Hmm remapping in xmodmap is picked up by xcape, but not as a modifier.  Maybe I
should leave the ctrl:nocaps option after all.

Ah!  It works.  Great.

For the other keys, well it appears I have to use xbindkeys to make the config
i3 agnostic.  But since I'm using i3 for the moment, might as well leave them
there.

*** Fontconfig nightmare
Well, I can get Noto as a default sans and serif font, but installing Fira Mono
*will* fuck up the italic for Dina in Emacs.  That's weird.

Well okay, I don't really care about Fira Mono in Firefox.  So... installing
ttf-dejavu gives me a mono font in Firefox, but *does not* fuck up Dina.  Sold.

*** Auto-connect to known wifi networks
That's [[https://wiki.archlinux.org/index.php/Netctl#Automatic_switching_of_profiles][documented]].  Just enable the netctl-auto service and good to go.  I think
there's a slight delay before I get an IP adress, but maybe there's a way to get
feedback on that in i3status.

One issue though is that disabling and re-enabling the wifi can fail if the wlan
interface was already up.  The workaround is to add the Force-Connect=yes option
to the netctl profile, though that's not the preferred course of action.

* [2016-12-30 ven.]
** (Puzzle) game ideas
Untangling a graph.  Generate a random planar graph and shuffle the node
positions.  Player can move nodes around.  Goal is to have no edges overlapping
each other.

Variant: instead of total freedom, nodes can only go in predefined positions.

* [2017-01-04 mer.]
** Prototype puzzle game
I built a prototype.  It works on tablet, but not as fast as I would like it
to.  50fps on tablet and lower on the phone.  On top of that, there are hiccups
caused by GC pauses.  For just displaying four nodes... sigh.

I tried to use pixijs, since it is supposedly fast, and will autodetect which
renderer to use (WebGL or Canvas).  Well, it's slower than using the canvas
directly.  On the phone, it picks up WebGL and gets 10fps.  Also, it even with
the antialias option to true, circles are not as nice as using canvas.

I might get more mileage out of not redrawing everything every frame.

** Related game
Looking around, there seems to be plenty of similar games:

http://johntantalo.com/wiki/Planarity/
https://www.jasondavies.com/planarity/planarity.js
https://github.com/JoyJing1/Planarity

Though they all perform poorly on mobile.

** Fixed failing tests in flycheck ERT suite                       :flycheck:
Making a note here: ERT failures are not very readable.  A simple diff would be
better.

** Writing an Emacs module in Rust                                    :emacs:
Should be possible with Emacs 25.

There is a [[https://github.com/janestreet/ecaml][library for OCaml]], and a [[http://diobla.info/blog-archive/modules-tut.html][tutorial for C]].

Problem is, native modules are disabled in the default Emacs build, so most
users would not benefit from them without going through the trouble of building
their own Emacs.

* [2017-01-05 jeu.]
** TODO Merge the flycheck/refactor-rust-cargo branch        :flycheck:cargo:
Or find out why it was blocked.

Even if the flycheck-rust branch is not ready to automatically fill the
variables, the checkers should work with benches and tests targets.

** Finding a way to test Flycheck with multiple versions of external tools :flycheck:
I've updated the tests to pass with the latest GHC (8.0) for instance, but what
about support for versions < 8.0?

If we don't test those regularly, we will break them.

Historically, flycheck makes no guarantees to keep supporting tool versions (you
can always not update flycheck).  But while that is convenient for maintaining
flycheck (keeps from accumulating cruft), it might not be convenient for users.

If there's a way to keep maintaining multiple versions of major languages, we
should try.  We would need a clear story on deprecating language versions
though.

Investigate how to test multiple versions: Docker image?  CI?

As a first step, I opened an issue to know if best-effort backward-compatibility
was something we wanted.

** Finding out why help window does not behave the same as in stock Emacs :emacs:
Help windows are always tiny and on the bottom in Spacemacs.  This is annoying
since I usually want to read all of more than a few lines if I'm calling the
help.  It does the same thing for explain error buffers for flycheck-rust as
well.

The always-helpful Emacs manual has many things to tell me.

Apparently, help buffers activate the ~temp-buffer-resize-mode~, which /should/
resize itself to match the output, but that does not appear to have any effect.

Haha!  Looking through the Spacemacs source, it seems that the ~popwin~ package
is the culprit.  It's a package to manage pop-up windows, like the Help buffer.
Disabling this package in Spacemacs gives me larger help windows.

But, it's still not the stock Emacs behavior.  In Emacs, if I call
explain-error, it will split the window horizontally since I have a wide screen.
If there frame is too small, it will split vertically.  In Spacemacs, it
always splits vertically.

Hmm, seems ~(split-window-sensibly)~ is the cause.  It can be tweaked using
variables as stated in its help.

Ahh okay, even though the threshold values are the same from stock, I'm not
using the same font, so I have more lines on the screen.  In stock, calling
~split-window-sensibly~ repeatedly never split vertically, since I never had 80
lines available.  With the bitmap font, I have 89 lines, and it splits
vertically first, then horizontally.

But wait, popwin in Spacemacs was configured only for ~*Help*~ buffers, not
error explanations...  Ahh, loading the flycheck layer actually adds the
incriminating line:

: ("^\\*Flycheck.+\\*$" :regexp t :dedicated t :position bottom :stick t :noselect t)

so any buffer matching the regexp will be managed by popwin.  If I remove only
that line, it behaves as expected.

Also, turning on ~temp-buffer-resize-mode~ will fit the window to the buffer
content, which is especially helpful for error explanations.

Except, curiously, for the error list.  Calling manually
~resize-temp-buffer-window~ inside works, but it pops up taking half the
screen.

Ah, that stems from that fact the error list is not temporary.  It is created
using ~pop-to-buffer~.  I think I can customize ~display-buffer-alist~ to
autofit it.

Nope, it seems the list is not populated in one go.  Ah, but there's a hook
after it has finished refreshing.

The hook does not run /inside/ the error list window, but I can select it in the
hook.

Ahah!  Victory!

Time to call it quits.  I should timestamp my paragraph as well.  These
inquiries take /hours/.

* [2017-01-06 ven.]
** Displaying errors from other files in Flycheck                  :flycheck:
Mentoring someone who volunteered to pick up the work from the previous PR.

After reading the discussion in that PR, it seems there were potential lingering
issues, especially concerning the :buffer and :filename fields of the
flycheck-error struct.  @lunaryorn was concerned that the changes would
highlight all errors in the current buffer, regardless of the file they come
from.  @chrisdone claimed it worked for Haskell mode.

Testing with Rust... ok, I get a new column in the error list with the filename
for the error.  But trying to go to the errors in the other file will jump in
the current buffer, and syntax highlight errors that are not there (since they
are in the other file).  That's indeed what @lunaryorn reported.

Let's test this in stock Emacs to make sure Spacemacs is not interfering
somehow.  Same thing.

Now let's see the code.

Okay.  I don't know how the Haskell checker creates its errors, but the Rust
checker will associate errors to the buffer that invoked it.  That means, errors
from the whole project will be associated with the current buffer.  Though they
will have the different filenames.

Now, in ~flycheck-jump-to-error~, we will jump to the buffer associated with the
error if it exists.  That's why we end up jumping in the same buffer, instead of
opening up the file associated with the error.

Hmm, the haskell checker uses the :error-patterns to build its flycheck-errors.
What is the buffer value in this case?  Apparently, the error is created in
~flycheck-try-parse-error-with-pattern~, and it doesn't give any value for
:buffer.  But it uses the constructor ~flycheck-error-new-at~, which uses
~(current-buffer)~ as the default value.  So it should work the same as for
Rust... weird.

Anyway, the :buffer property of a flycheck error is the buffer that ran the
checker, not necessarily the buffer associated with the file where the error
resides.  It seems wrong to reuse this value to find the location of the error.

The filename is the more sensible choice.  But recall that an Emacs buffer is
not necessarily associated with a filename, and the same file could have
multiple buffers open.  Regardless, if we have one buffer associated with the
file, switch to it.  If not, open the file in a new buffer and switch to it.

When this works, there is the question of maybe not calling cargo for every
buffer if we already have errors ... but it may be an optimization for another
PR.

That's an interesting feature, but it may not sit well with the way Flycheck
works.  I'm especially worried about the interaction with flycheck instances in
other buffers.
