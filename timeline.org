#+OPTIONS: ^:{}

* [2015-09-30 mer.]
:PROPERTIES:
:header-args: :results none
:END:
Trying to come up with a fast equivalent to ~which-func-mode~ under Spacemacs.
The most naive implementation would be to lookup backward for the first heading.

#+BEGIN_SRC elisp
(spacemacs|define-mode-line-segment which-org-headline-segment
  (fmdkdd/org-current-headline)
  :when (eq major-mode 'org-mode))

(add-to-list 'spacemacs|define-mode-line-segment 'which-org-headline-segment t)

(defun fmdkdd/org-current-headline ()
  (save-excursion
    (re-search-backward org-complex-heading-regexp nil t)
    (match-string-no-properties 4)))
#+END_SRC

That does not give you the full current hierarchy (bread crumbs).
Actually, there is an ~org-get-heading~.

#+BEGIN_SRC elisp
(spacemacs|define-mode-line-segment which-org-headline-segment
  (org-get-heading)
  :when (eq major-mode 'org-mode))
#+END_SRC

It even gives us the font-lock properties.

There is also a ~org-get-outline-path~ that gives the rest of the crumbs.

#+BEGIN_SRC elisp
(spacemacs|define-mode-line-segment which-org-headline-segment
  (fmdkdd/org-current-headline)
  :when (eq major-mode 'org-mode))

(defun fmdkdd/org-current-headline ()
  (let ((path (append (org-get-outline-path)
                      (cons (org-get-heading t t) nil))))
    (org-format-outline-path path 40)))
#+END_SRC

Removing the text properties can be achieved by calling
~substring-no-properties~.  Though I rather like the effect as is.

Another, longer (but more proper?) way of removing them is the following:

#+BEGIN_SRC elisp
(defun fmdkdd/org-current-headline ()
  (let* ((path (append (org-get-outline-path)
                      (cons (org-get-heading t t) nil)))
        (formatted (org-format-outline-path path 40)))
    (set-text-properties 0 (length formatted) nil formatted)
    formatted))
#+END_SRC

* [2015-10-01 jeu.]
I would like to be able to use Emacs for collaborative editing.  I have light
requirements:

- over local network would suffice.  I just want to be able to share a buffer
  with someone next to me, each with their own computer.  For pair programming
  or teaching.
- I don’t care much about security: I trust the other person since she is right
  next to me.  When we finish, the computer should not be left in a vulnerable
  state however.
- I prefer to stay with my own Spacemacs config, rather than having to use the
  config of the peer.
- it should be painless to setup, and stable.

This [[http://stackoverflow.com/questions/12546722/using-emacs-server-and-emacsclient-on-other-machines-as-other-users][SO thread]] is a good start.

The scenario is as follows.  Host is where the files to be modified reside.
Host has an Emacs session and buffer on file A.  Client wants to drop in Host
and take control of Emacs from his machine, and edit the same buffer.  He can
also split windows, switch buffers, etc.

** Using tramp and ssh
Client needs ssh access to Host.  Client can browse to file A from his Emacs.

However, Host will not see the changes until Client saves.  This is
insufficient.

** Using ~make-frame-on-display~
Emacs can spawn a frame on another X display.

The requirements:

- Allow X to listen to TCP connections.

  Under Ubuntu, X is spawned by lightdm, so, in =/etc/lightdm/lightdm.conf=
  : xserver-allow-tcp=true

  and restart lightdm.

- After that, allow the Host to access the X server with xhost
  : xhost +host

  In Ubuntu, my LAN machines can be accessed via =host.local=.

  One can also use ~xauth~ here, as described in the SO thread.  Deauthorize the
  Host with ~xhost -host~.

- Finally, the Host can spawn a frame from its Emacs on the client display
  server with
  : make-frame-on-display client.local:0

Now, Client can write in the buffer, and Host can see the changes.  Both can
even edit at the same time.

Seems stable.  There is the issue that if any of the peer starts a modal action
(helm lookup, M-x minibuffer spawn), the other cannot type anymore.  When the
modal action is over, the input will be sent to the frame however.

This is a distinct frame, so Client cannot control the Emacs frame on Host.
Splitting windows should be done on both machines.  Client can browse the Host
files.

Host only has one command to spawn.  But Client needs to restart X before
pairing, because tcplisten seems like a fun backdoor to leave open.

Alas, Client is stuck with the Emacs configuration from Host.  This cannot be
avoided, since there is only one Emacs process.

** Using tmux
As suggested [[http://www.emacswiki.org/emacs/tmux_for_collaborative_editing][there]].  However, I could not make the socket sharing work.

Rather, sharing the same tmux session is simple:

- Host does ~tmux new -s pp~ to create a new session named ‘pp’.
- Client does (connected on Host) ~tmux attach -t pp~ to join the session.

With tmux, Client can connect to Host using ssh, and join a tmux session.  Both
share the same cursor.

Since tmux is terminal-based, Emacs runs in tty mode.  Functionality is the
same, but can be unfamiliar for Host.  Using frames would be possible through
ssh X forwarding, but that would not give us more than the previous solution.

Client has to use Host Emacs config, again.  The setup is also slightly more
involved with Host.

But, sharing through tmux is useful beyond Emacs.  So there is that.  And this
solution should work well over the network (if you can ssh to Host).

There is even a wrapper around tmux called [[https://github.com/zolrath/wemux/][wemux]] which simplifies the setup and
provides relevant options for multiple peers.

** Using floobits
A proprietary web service.  Use a Github account, create a workspace (?) and
share files.  Other users you have authorized can then access the workspace, and
you can see the changes in realtime in your editor if you are viewing the same
file.

Rather nice is that every peer is using his own machine and editor.

However, it goes through the Floobits server, thus it’s pretty slow compared to
the previous solutions.

And there is the requirement of going through a workspace.  It might make sense
for collaborative realtime editing of a project, though I’d rather use Git then.
But it’s cumbersome to setup when playing on a throwaway file.

The nail in the coffin is of course having to go through a third-party.  If the
server software was at least available as open source, I could run a local
instance and that would be a pretty good solution.  Alas, that does not appear
to be the case.

** Using rudel
[[http://rudel.sourceforge.net/][Rudel]] is an Emacs package which share functionality with Floobits.

One Emacs must host a rudel session.  Others can join.  The host does not take
part in collaborative editing.  The host passes editing data from one peer to
another using an open protocol.  Other clients can join.

Users in a session can publish a buffer, and others can subscribe to it.  When
you subscribe to a buffer, Rudel opens a new window with the buffer text
inside.  You can then edit the text with your own cursor, and editor.  Changes
are highlighted with the color of each user (that can be disabled through the
menu option, thankfully).

Rudel is intended to work with menu-bar-mode on, it seems.

I don’t know what data Rudel sends, but from the project website, it seems it
can break the functionality of some modes like EShell.  This behavior can be
troubling.

I’m not sure what exactly is the buffer a client edits: does it have a local
copy?  Does it exist only temporarily?

Speed is alright, but slower than tmux and xhost.

Also, the setup is a bit more involved, and the package is in dire need of
maintenance.
* [2015-10-21 mer.]
** Explanations
In web apps, I find it would be useful to be able to ask why a value is 0, or
NaN.  E.g., why a DOM element has its ‘left’ property to ‘12px’.  I would like
to find the culprit code immediately.  Alas, there are no ‘conditional
breakpoints’ in Firefox or Chrome.

Wait, there are!  You can break on attribute modification by right-clicking a
DOM in the Elements panel in Chrome.  In FF, you can conditionally /stop/ a
breakpoint, but not break conditionally.

Anyway, jumping into the debugger when a value is modified is only one part of
the workflow.  That gives you the place where the value is set, but not how the
right-hand side was computed.  You have to backtrack through the call stack to
get this information.

Instead, if a value contains its history, the explanation is always available.
See [[file:javascript/explanation.js][explanation.js]] for a minimal proof of concept.

** Interactive value inspector in s3c
Trying to add interactive value inspectors into s3c.

*** Rationale
Instead of plain text, the editor should put an HTML element that represents the
full object, like in Firefox or Chrome consoles.  Each property can be
inspected.

- Why do you need that?  The current behavior of displaying serialized objects
  is good enough for small programs.  At least you have all the properties on
  display at once.  With an “interactive” object, you have to click to view
  further properties...

- The current behavior is nice and simple, true.  But for larger objects, it
  is unwieldy.  Also, an interactive value inspector opens the door for
  interactive “explanations” of values: backtrack through the code that created
  some value in order to understand why it’s a NaN, or 0, or ...

- Do you really need explanations?  I mean, in a full application it could be
  nice (provided a good signal-to-noise ratio), but s3c is for simple JavaScript
  code for beginners.  To find out why a value is NaN, just add more //: to
  track the flow.

*** Implementation
CodeMirror provides two functions: ~addWidget~ and ~addLineWidget~.  ~addWidget~
puts an HTML element on a line with absolute positioning.  So I can create HTML
to inspect an object, and put it after the delimiter.  It does not matter if the
element is larger than the line: with a positive z-index, it will appear as if
floating over the text.

To do that, in ~write~, instead of replacing, I can call:

: editor.addWidget({line: l}, p, false, "above")

The last argument is undocumented, but it puts the element /on/ the given line
rather than below (the default).

However, the element is absolutely positioned.  It does not move when the line
does, which breaks the illusion that it gives a view of the value to the left of
the delimiter.

To sync the widget, I would need to listen on changes on the document, and move
all markers that are potentially affected.  It is not sufficient to listen to
the ~change~ event of a line, as when a line is moved as a side-effect of
inserting a new line above, no change event is fired.

The ~addLineWidget~ is quite different, as it inserts the element below the line
and appears to be inset /in/ the text.  The lines it takes are not numbered, and
are skipped by the cursor.  It behaves correctly when inserting new lines.  Bit
of a space hog currently, as it eats vertical space rather than making use of
the usually empty space at the right of the screen.

Hacking the DOM created by CodeMirror sounds like a bad idea, if only for
forward compatibility.

* [2015-12-02 mer.]
** Free monad for interpreters
Reading up on free monads.  Again.  And discussing them with Ronan.

Beyond [[http://programmers.stackexchange.com/questions/242795/what-is-the-free-monad-interpreter-pattern][this blog post]], [[http://programmers.stackexchange.com/questions/242795/what-is-the-free-monad-interpreter-pattern][this SO answer]] is particularly helpful.

On a related note, even setting up a Free monad can be seen as boilerplate.
[[http://okmij.org/ftp/Computation/free-monad.html][Okmij shows]] how to eliminate the noise.

* [2015-12-09 mer.]
** GameBoy Sound player
The sound component of Boyo is a mess.  It sort-of works, but there are weird
artifacts coming out after a while.  And it’s eating at least 20% CPU.  And it
doesn’t even pass blargg’s tests!

I want to start from a clean state, and understand how the damn thing works.
Maybe writing a player for GBS files would be a more appropriate target?  I’m
curious as to what these files store anyway.  Can’t be samples, or they would
directly be in a sound format.  So they must be instructions directly from the
ROM, but probably only the instructions relevant to the audio?

Found a [[http://ocremix.org/info/GBS_Format_Specification][spec]] for GBS files.  At that point, eww does not seem capable of
downloading a sample GBS from Zophar.

Got some GBS.  They are indeed smaller than the ROM file from which they are
extracted.  Pokemon Red ROM is 376K while the GBS is 48K for instance.

Looking at the source for gbsplay, it seems indeed that playing the files means
emulating the CPU and the audio unit.

Maybe what would be nice is if we could compile the output from a GBS into audio
instructions only.  To get an output similar to what MOD file looks like for
trackers.  GBS to MOD converter.

* [2015-12-11 ven.]
** GameBoy Sound player
Will try to go with rust-lang.  Why not make it harder on myself?  At least if I
don’t complete the project, I’ll have learned the basics of a new language.

Someone already did a library for [[https://github.com/emu-rs/spc][reading spc]] in Rust!  This will help.

* [2015-12-12 sam.]
** Learning Rust
The proof of concept code I wrote yesterday worked, but some pieces went over my
head.  Today I went over the [[https://doc.rust-lang.org/stable/book/][Rust book]] to RTFM.

Now, I know how I should use result types to avoid deconstructing with match so
much.  And also how to put my utility functions in a module for better
organization in the long term.

* [2015-12-19 sam.]
** Filling instruction is boooring
Revamped the instructions macros a bit.  Leaner, and now matching the order of
[[https://code.google.com/p/game-music-emu/source/browse/trunk/gme/Gb_Cpu.cpp?r=40&spec=svn40][Blargg’s emulator]].  Though I don’t really know if there is a performance payoff
for that, since it could be optimized by the compiler as a jump table anyway.

Not sure what I want to do with flags tests after operations.  Seems like lot of
duplicate code.  Unless I use a ~test_flags~ macro...

* [2015-12-20 dim.]
** Overflow are safe in Rust
Which means ... that 0xFF + 1 triggers a panic!  But only in debug builds, since
these checks are removed on release builds.  However, the right way to go about
that is to use ~wrapping_add~ instead to /explicitly/ signal overflow is
intended.
* [2015-12-23 mer.]
** Improving s3c
Was looking to improve the error feedback of s3c.  But I realized that I could
fix the O(n^2) complexity of code evaluation.

Since we have only one worker when evaluating the whole file, and since the
worker evaluates all its code in the global context, we don’t need to
re-evaluate the previous blocks.  We can just send each block of code to the
worker by resetting the current code string.

So, evaluation is back O(n) with a one-line change.  D’oh.  And this also fixes
the multiple console.log calls!

But, it also changes the behavior of error output.  Previously, the first error
encountered in the evaluation would propagate as the result to all the following
evaluation markers.

: throw 1 //: 1
: 1 + 1   //: 1

Now, even a syntax error will affect only the next marker.

: throw 1 //: 1
: 1 + 1   //: 2

Is this ... better?  I’m not sure.  On the one hand, errors don’t propagate
anymore.  So you can go on with your code and still get feedback, even if a
previous definition triggers an error.

On the other hand, it’s now easy to miss an error up in the file and continue
working, and then wonder why something doesn’t work down the road.  Syntax
errors are signaled by the linter.  But other errors, like:

: fn f(a) { return a.b(a) }
: f(12) //: TypeError: a.b is not a function

are not.

For beginners, it might be a good idea to make runtime error more noticeable.

Okay, marked the lines in inverted red.  Can’t miss them now.

-----

Also added visual feedback for triggering evaluation.  Just erase the text after
//: at the time of sending the code to the worker is enough to /see/ that the
editor is doing something even when the results are the same.

-----

Made console calls to not trigger any linting error or warning, since they can
be used to step through a block.

-----

Maybe using a forEach on each /block/ rather than line would be faster than the
current way.  Another time.
* [2016-01-06 mer.]
** Decoding opcodes in GBS
The decoding opcode part of GBS is a bit redundant:

#+BEGIN_SRC
0x41 => ld!(b, c),
0x42 => ld!(b, d),
0x43 => ld!(b, e),
0x44 => ld!(b, h),
0x45 => ld!(b, l),
0x47 => ld!(b, a),
#+END_SRC

There is a way to factor that by just looking at how the opcode is composed.
For the ‘ld’ instruction, there is a pattern:

: ld r,q = 01rrrqqq
: ld r,n = 00rrr110 nnnnnnnn

With r and q being one of:

| Register | Code |
|----------+------|
| B        |  000 |
| C        |  001 |
| D        |  010 |
| E        |  011 |
| H        |  100 |
| L        |  101 |
| HL       |  110 |
| A        |  111 |

So, we already have the register information from the opcodes.  No need to spell
it out.  But this means additional work at runtime (decoding the opcode), and
decreased legibility of source code.  As of now, the code is very
straightforward, save for the organization of the opcodes.

We could decode the opcode at compile time using a macro, but I’m not sure we
would gain in legibility.

And unfortunately, the pattern breaks down for other opcodes:

: ld A,BC = 0000 1010

At least the tedious way to spell it out is homogeneous.

* [2016-01-07 jeu.]
** Dragging boxes around
For a prototype visualizer of the JS heap.  I need to move boxes, representing
objects, around, and link them with arcs.

Started with a simple div box absolutely positioned and a homebrew drag’n’drop.
Works.

** Cloning SVG in a template tag
But for arcs, I need to switch to SVG.  First suprise: using HTML templates to
clone SVG elements needs namespacing.  So I wrap the elements (like ~rect~) in a
~svg~ tag with explicit namespacing.  Works!

** Slow drag in Firefox
Chrome is perfectly happy using the CSS transform property for dragging the
SVG boxes around.  Firefox is choppy.

[[https://jakearchibald.com/2013/solving-rendering-perf-puzzles/][This post]] is helpful on the subject.  Changing the x and y attribute of the rect
is definitely worse.  Using the transform property of the SVG (rather than CSS
transform) seems okay.  Certainly not as fast as Chrome, but looking at the
numerous bug report on SVG performance on Bugzilla, I’m gonna assume that SVG
animations in Firefox are just slower.

Hmm, closing the DevTools /is/ a definite improvement however.  Good thing to
keep in mind.

* [2016-01-08 ven.]
** Switching to d3
Managing SVG and interactivity is tedious.  D3 seems a good fit for what I want
to do.  I get browser compatibility, selectors, the join model of handling data,
and even animations.

Drag and drop is built-in, and I might need things like force layouts.

Also, it’s one of the most-used JS library, which means it probably won’t
disappear for at least a few years.

** Heisendrag
I was curious as to why the drag and drop example of D3 in Firefox was fluid,
while mine was choppy.  Turns out, dragging the browser tab in the other window
fixed the slowness ಠ_ಠ

* [2016-01-12 mar.]
** Mastering D3 and event propagation
In order to better understand how event propagation works in the DOM, and to
experiment with D3 animations, I made a simple visualization based on [[http://www.quirksmode.org/js/events_order.html][this
helpful page]], and using [[http://bl.ocks.org/mbostock/3943967][this block]] as a model for chaining transitions, and [[http://bl.ocks.org/mbostock/9631744][this
block]] for the visual language.

* [2016-01-15 ven.]
** Mouseenter event fired only when going to the right in FF
At least I thought that, maybe it was a bug in Firefox.  The behavior puzzled me
and then I noticed that the SVG rect I was hovering my mouse onto was /not/ the
only element around: the temporary line I drew on top of everything was there
too!

So, #notabug.  Standard PEBKAC.  The line should not be interact with the cursor
in this case, and that is what the CSS property ~pointer-events: none~ is for.

And hey!  As a bonus, it fixed the behavior I was seeing in Chrome: since the
cursor was just above the line, whenever I clicked on it to validate, I was
clicking on the line, which had only the SVG container as a parent, and thus the
SVG registered the click while the node did not.  In Firefox, for some reason,
the cursor always clicked the node below the line.  Maybe the calculations were
off a pixel...

** The self-perpetuating task of explaining code with code
I want to visualize JS code to better understand it, and be able to explain it.
For that, I build a program.  I write more code, /different/ code, code that the
visualization might not suffice to explain.  The visualization if for heap
objects, but for that I’m writing an automaton, and we already have a good
visual language for those.  But!  If I want this automaton visualization to be
interactive, I again need to write more code.

Either at some point I have visualizations for the first kind of code, and also
for the code of the visualizations, etc.—I converge—or I just throw up my arms
in the air and leave some code unexplained, or self-evident.

Will only know if I try.

** Declarative automaton for linking nodes interactively
The linking nodes code is /clearly/ an automaton, and /clearly/ is spaghetti
code at the moment.  Dealing with listeners that should only exist on one state
is especially nasty, since we have to register them, then toggle them off, and
this is a repeating pattern that surely could be taken care of by a declarative
automaton.

As it stands, here is the description of the functionality needed to make the
linking:

#+BEGIN_EXAMPLE
Complete (functional description of) automaton

ready --click on circle--> select-dst
       |
       +- create temp line from circle to mouse

select-dst --move mouse-> select-dst
            |
            +- set end point of temp line to mouse position

select-dst --click on a free node-> ready
            |
            +- remove temp line
            +- add link between src and dst to model
            +- add link to view (update view)

select-dst --click elsewhere-> ready
            |
            + remove temp line

Animations and highlights:

ready --enter circle-> ready
       |
       +- grow circle

ready --leave circle-> ready
       |
       +- reduce circle to original size

select-dst --enter node-> select-dst
            |
            + stroke node in green

select-dst --leave node-> select-dst
            |
            + stroke node in default color (black)
#+END_EXAMPLE

I’m pretty sure there is a fluent API there that can take care of the
administrative details of entering a state, and setting up/destroying events
listeners as it goes through a transition.  Anything that need to be done on a
transition can be passed as a function.

Transitions, for my case, are always events happening on some element.  Then 4
things happen, in order:
1. We execute whatever needs to be done when leaving the state (cleaning up
   event listeners)
2. We execute the transition function
3. We change the state, internally
4. We execute whatever needs to be done when entering the new state (setting up
   new listeners)

If the transition is a loop (to and from the state), then only step 2 is needed.

That’s it!  Initially I don’t think any more control is needed for my use case.

Here is how I would sketch the API:

#+BEGIN_SRC js
var link_automaton = automaton()

var ready = link_automaton.state('ready')
      .on('circle.mouseenter', grow)
      .on('circle.mouseleave', shrink)
      .to('select-dst', 'circle.click', create_tmp_link)

link_automaton.init('ready')
#+END_SRC

Need to prototype that to know if it works in practice, and make sure it is
composable (can add states and transitions in multiple steps, not just one
monolithic call).

* [2016-01-19 mar.]
** Declarative automaton API choices
Nearly done.  The code is much clearer using the automaton.  For now I’m just
declaring state objects and adding callbacks to their transition/enter/leave
events, and not using a fluent interface at all.

However, the fluent interface can come on top of that, to alleviate two problems
with the lower-level interface:
1. All the states must be declared beforehand.  If A refers to state B (in a
   transition, say), then B must be declared.

   Using a fluent interface, we can just give the name of the state rather than
   a reference to it, and let the interface build the actual state objects for
   us.

2. Adding a callback to a transition is done with ‘on’, but a callback to an
   enter/leave event of a state uses ‘addListener’.  The fluent interface can
   merge the two calls based on the arguments.

There remains a problem with the automaton that I would like solved before
moving forward: how to deal with state that is local to the automaton.  The link
automaton needs to keep a reference to the first element selected, in the ready
state, for use in the select-dst state.

I elected to add an empty ‘data’ object to the automaton.  It’s basically the
same as closing over a variable, but at least it’s namespaced.  And in the
future, maybe I can provide a way to get a ref to the automaton from callback
calls.



An issue I encountered in this version is that I can’t add multiple callbacks to
one transition.  Or even add a callback after creating the transition without
any, at first.

To solve that, transitions should be first class, either through giving them a
name, or returning a new object.

As an added nicety, I think I know how to settle the dilemma of having to choose
whether transition callbacks happen after or before we leave the current state:
let the user choose.  Callbacks can be added either at the ‘debut’ of the
transition (before leaving the old state), or at the ‘end’ (after entering the
new state).  Maybe the ‘middle’ (after leaving the old state, but before
entering the new one) can also be useful.

* [2016-01-22 ven.]
** More design decisions
I’ve pondered whether using the automaton as a pure event emitter.  When
entering a state, when a transition is made (3 stages), just emit custom events
and define the behavior only in the listeners to these custom events.

This is better for decoupling the code.  But the cost is that you lose track of
the control flow.  Some animation bugs are subtle, and require you to know
precisely what happens and in which order.  Animation is part of the
interaction, and the code should not be declared separately.

* [2016-01-29 ven.]
** Event binding troubles
So, I was on the fence about binding listeners to elements themselves, rather
than on the containing SVG, fearing performance issues.  Since boxes can be
added/removed, and we add several listeners to different element of each box,
AND we add/remove listeners depending on the current state of the automaton.

The upside is code that is free of ~if~ statements, since the dispatching is
taken care of by the event dispatcher.

However, it has come to bite me back.  If I define the automaton only once (as I
should have from the start), then when a new box is created, no listeners are
bound to it.  Can’t be dragged.

Of course I could add the drag behavior to newly created boxes.  But, it might
not be correct if we are not in the ready state.  What we should do is add the
listeners for boxes (and sub-components, cell and circle) valid /in the current
state/.  That seems like it’s easy to forget, and it is.  Also, it seems
a bit wasteful, because I would select all boxes again, and reassign the
listeners for all.

Another solution is to catch all events at the container level, let them bubble
up and identify the original target.  But now the problem is that sometimes I
don’t want to just know the original target, but I need the path in the DOM that
the event took.  So now I need to walk up the tree, duplicating the bubbling
phase.

And, ultimately, the drag behavior from d3 need to be called on a selection, not
on the container.

The more pragmatic solution is just to call drag_box when a new box is created.
Since I know the user is in the ready state.  Even though it’s not correct, I
might find a better way to organize this stuff later down the road if need be.

* [2016-03-22 mar.]
** Comparing approaches to deal with state
Ronan has been using RxJS for an application that presents a GUI in the
browser.  I was wondering how the reactive programming approach would handle my
situation, for which I found that a state automaton was the best approximation.

But at the same time, it seems odd that I have to resort to an explicit state
automaton to handle my elementary interaction.  So, how do others deal with it?

Looking at RxJS docs, it seems that it is a complete algebra of events, meaning
I could use the basic operators to build richer ones, and eventually create
streams of predicates that would give me exactly the same information that a
state automaton gives.

But, would the complex operator be as clear, or clearer than the description of
an automaton?  And what about the performance of the thing, as this is always a
worrying concern when techniques from functional programming are naively ported
to JavaScript.

I need to find out:
1) the way ‘traditional’ GUI systems deal with this kind of interaction (Swing,
   GTK, Qt, Cocoa?)
2) if there is a ‘canonical’ way to handle this kind of interaction using RxJS
   (or in reactive programming)
3) if there is a standard, or well-known technique to bind listeners to DOM
   elements ‘lazily’, that is, whenever an element matches the given selector, it
   should trigger the listener.

For point 3, if I set up a single listener at the root of the document, I can
capture any click and match the given selector against event.target.  But what
if I want to match against a /parent/ of the target?  Knowing that clicks
bubble, I could walk up the DOM and test the selector against each element,
until I hit the root.

Except now I’m duplicating logic done by the browser, and it’s incompatible with
stuff like ~event.stopPropagation()~.



Okay, on 3, there is an [[https://developer.mozilla.org/en-US/docs/Web/API/element/matches][~Element.matches~]] predicate to know if the element would
have matched the given CSS selector.  Better than having to check the ~tagName~
and ~classList~.  But doesn’t solve the need to look up the parent.

The name of the technique is “event delegation”.  [[https://api.jquery.com/on/][Jquery]] has an argument for
that, but for some reason, it doesn’t work on SVG.  And indeed, it walks the
tree:

#+BEGIN_QUOTE
jQuery bubbles the event from the event target up to the element where the
handler is attached (i.e., innermost to outermost element) and runs the handler
for any elements along that path matching the selector.
#+END_QUOTE

On point 1, there are certainly a number of hits for “GUI state machine”, and
the pattern seems recognized.

* [2016-03-28 Mon]
** Trying out a Sparkets rust server
Since server is in need of a rewrite, to be faster, cleaner and more robust.

Since we already compiled Coffeescript, that does not change the compilation
time much.

** Choosing a library
I’ve got a fast and simple [[https://github.com/housleyjk/ws-rs][websocket library]].

Now, I know I will want to benchmark binary messages vs. text messages.  So I
should design around this choice by presenting a common interface.

** Testing input latency
I want to test how the game feels with a moderately high latency (~50 to 100ms
roundtrip).  I thought Chromium was able to do that, but it seems the throttling
option of the network panel only works for initiating the connection, and is not
applied to all subsequent frames when the websocket is established.

But, there is an option to add latency directly on the loopback interface
through [[https://daniel.haxx.se/blog/2010/12/14/add-latency-to-localhost/][netem]]:

: tc qdisc add dev lo root handle 1:0 netem delay 50ms

this sets 50ms of delay.  It does affect ~ping~, and it visibly affects
websocket frames on my machine.

To reset:
: tc qdisc del dev lo root

It seems you need to reset before applying a different delay.

** Multi-threaded server or asynchronous?
Building up a small prototype.  Not familiar at all with how to build a game
server in Rust.  And I have to deal with memory management explicitly.

The nodejs server was asynchronous, because nodejs.  One event loop where input
was collected, and one setTimeout to deal with game updates.

In Rust, I guess I could also do that, but I have to look up how.  Meanwhile, I
could also use a multi-threaded approach.  One thread per client might be
simpler to code, and since we are not expecting thousands of players, the
performance scaling of thread is not an issue.

In any case, I need to brush up on coding concurrency in Rust.

Been reading:

- [[http://fabiensanglard.net/quakeSource/quakeSourceNetWork.php][Network code review of Quake]]

  Yes, I know it uses UDP, and WebSocket is on TCP.  But I want to know how
  clients are handled.

  Well, it’s not clear from that article.

** What’s the ideal solution to input latency anyway?
I’ve always wondered if the treat input/update logic/render loop was optimal.
I’ve been doing that for ages.  I remember it bit me because updates were tied
to graphical frames, and lagging on frames made the game slow.

But this was an issue of handling time in the updates.  If the game updates by
doing ~player.x++~ each frame /and/ you assume the game runs at 60fps, then when
an old machine churns out 30fps, the game plays in slow motion.  Because what
you really wanted to say is ‘x increases by one each 16.66ms’; the simulation is
tied to continuous time.

A game is a simulation.  The simulation, to feel good, needs to be as responsive
as possible.  If I act on the real world, I expect an immediate feedback.  The
simulation, to feel real, must do the same.  It means that a player must be able
to react on input, and see his impact on the simulation in /realtime/.  Of
course, the computer cannot do realtime, only discrete.  But, the computer can
compute the simulation and redraw it much faster than the brain can notice.

25fps is good enough for our brain to believe that movies are real.  But when
you add interaction, you usually need to be a bit faster than that.  25fps means
40ms between two frames.

Let’s say it takes 10 ms to update the simulation, and another 10ms to draw the
scene and refresh the display.  Out of 40ms, the CPU is only busy for 20ms,
which is good.

#+BEGIN_EXAMPLE
   late input                            early input
   |                                     |
--UUUUUUUUUURRRRRRRRRR--------------------UUUUUUUUUURRRRRRRRRR----------------
  |  compute s        |                   |  compute s+1      |
 screen shows s-1     |  screen shows s                       | screen shows s+1
#+END_EXAMPLE

Already something is troubling.  The simulation should render things as they are
/right now/.  But as it /takes time/ doing so, the display is already outdated
as it is shown on the screen!

It’s like when I give you the time, by the time you hear it and process it, it’s
already false.  Now, luckily, the time is still useful to you because I only go
to the minutes.  Seconds are trickier.  Milliseconds are already hopeless.

Same thing for the simulation.  It’s in some state ‘s’, then at the scheduled
time (every 40ms), it starts updating to state s+1.  When the screen is
refreshed, we are already 20ms in.  What time does the simulation reflect?

If it reflects the time of the world at the /beginning/ of the update, then the
image on the screen is already 20ms outdated when it comes up.

That means that if a user action is made just before the update comes along, we
will see the result 20ms at the earliest.  Worst case, the input is made just
after the update component reads them, then we have to wait for the current
frame to draw, then the next: 60ms before our action impacts the world we see.

So, for any random button press, the screen might display the changed world
after a delay that is anywhere between 20ms and 60ms.

If that delay is long enough for the brain to have time to think “did I press
that button?”, for the brain to /notice/, then the simulation is not fluid, and
the illusion breaks.

The question is then, how long can this delay be before the brain starts to
notice?



Running some tests...

Typing a key (down key event) paints a square on the screen.  The square
alternate between pink and green colors to distinguish each key stroke.

Delays are chosen randomly, I just type to see if it feels responsive.
Delays are just lower bound on the actual perceived delay: the screen might take
some milliseconds longer to refresh.

I’ve noticed that typing just one key is vastly different than stringing a few
keys together.  If I type once, and wait to see if I notice the delay before the
square is painted, 100ms feels immediate.  But string 3 keys rapidly, and it
does not feel instant anymore.

400ms is definitely noticeable, and feels sluggish at all.

A delay of 200ms is noticeable, but can still feel responsive for one key.  Not
for 3 keys.

100ms feels immediate.  But I can feel the delay when stringing keys.

50ms feels immediate.  Stringing keys also.

10ms feels a bit faster than 50ms, but not really much.



Another test, on input speed this time.  Measuring time between key downs.

Double-tapping the same key: I can hit 87ms minimum reliably, but with effort.
Effortless is more 150ms.

Stringing two different keys: now there is an issue with measurement.  Tapping
two or more keys /at the same time/, I can never get below 8ms.

Since each key down is a separate call to the listener, I suspect that the time
is spent dispatching and cleaning up.  So, 8ms is the effective resolution of
the browser in this setup.  Sometimes I get a 3, or even 0.5, but quite
randomly.

Now, stringing two different keys: I can do 8ms (same time for the browser) and
16ms reliably (the earliest to distinguish between two key down), without
effort.

With 3 fingers, I can do <100ms for each successive tap, effortlessly.



What does this mean?  Well, if I am able to hit two keys with 20ms between them,
I can also hit them with a 60ms interval.  If I can feel the difference in
my fingers, the game should also reflect this difference.

But, if I sample the input every 40ms (by polling the keys at the beginning of
the update loop), keys hit with an interval <40ms are counted as being hit at
the same time.

It’s basic sampling.  The signal is 1 when the key is down, and 0 when the key
is up.

#+BEGIN_EXAMPLE
----------|----------|----------|----------
0000000111110000000000011111000000011110000
#+END_EXAMPLE

As long as the key is held down for longer than the polling interval, we are
sure to get every key.

And if we want to distinguish between two successive key pressed, we just have
to use a reasonably low polling.

On my browser, the lightest tap I can muster holds the key for 32ms.  Meaning
that if the polling was 40ms, I could miss that key down from time to time,
depending on how it falls with respect to the update.

In this case, 30ms would suffice.  Poll interval of 30ms, or you start losing
keys.



So I guess the morale of the story is: faster feedback is always better.  But
below 50ms of visual feedback, the gains are negligible.

Polling keys at the start of a monolithic update loop is okay, as long as the
polling interval is less than the time a key can be held down.  Should check on
target hardware how low the resolution can be (browser + keyboard is certainly
not the optimal setup).

** Carmack on movement prediction
To alleviate server latency in QuakeWorld, Carmack tried to use prediction.  The
player movement is duplicated on the client, starting from the last known good
state received from the server.

The server works by directly answering to received packets: update only the
world around the player, and send the state back.  There is no global time
anymore.  But the player does not have to wait for the fixed update.

Carmack notes that simulating 300ms of player movement on the client is
hopeless.  But, for <100ms delays, client prediction helps smooth out the
movements.  Because server updates may not always arrive on time, we can keep
the framerate constant on the client with prediction.

* [2016-04-02 Sat]
** Setting the MTU on Archlinux
I had issues connecting to wiki.archlinux.org, but other websites were fine.

Apparently, that was caused by a misconfigured MTU.  Under Windows, the MTU was
1480 for ipv6, and 1500 for ipv4, but in Linux it was 1500.

To find out the correct MTU, I used ping:

: $ ping -4 -l 1452 -M do www.dslreports.com

‘-M do’ tells ping to look for MTU discovery packets.  The host has to be
configured to send these packets back, which few of those I tested (8.8.8.8,
google.com, free.fr) did.

Setting the MTU temporarily:

: # ip link set eth0 mtu 1480

(replace ‘eth0’ by interface name)

Then wiki.archlinux.org loaded correctly.

To set the MTU permanently, the wiki advised to use an udev rule, but I could
not get it to match the interface name for some reason.  Too lazy to RTFM, turns
out there is an MTUBytes option for systemd-networkd.service.  In
/etc/systemd/network/my.network:

: [Link]
: MTUBytes=1480

Voilà.

** Mounting a WDTV Live Hub
Did not want to install/configure Samba.

But luckily, only ~cifs-utils~ is required:

: # mount -t cifs //SERVER_IP/WDTVLiveHub/ /mnt/wdtv -o uid=USER,gid=USER

To find what shares are up on the network:

: $ smbclient -L //SERVER_NAME

To find the IP of the server:

: $ nmblookup SERVER_NAME
